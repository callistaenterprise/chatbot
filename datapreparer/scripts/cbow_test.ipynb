{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-e1bb3fda23d9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-e1bb3fda23d9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    in_file_read = with open(in_file, 'r', encoding='utf-8', errors='ignore')\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def data_preparation_1(in_file):\n",
    "    in_file_read = open(in_file, 'r', encoding='utf-8', errors='ignore')\n",
    "    for line in in_file_read:\n",
    "        # remove the line/conversation codes at start of each line\n",
    "        yield re.sub('^L[0-9]+\\s[+$]+\\s[a-z0-9]+\\s[+$]+\\s[a-z0-9]+\\s[+$]+\\s[A-Z]+\\s[+$]+\\s', '', line)\n",
    "    finally:\n",
    "        in_file_read.close()\n",
    "\n",
    "def data_cleaner_new(in_file, out_file):\n",
    "    line_generator = data_preparation_1(in_file)\n",
    "        \n",
    "\n",
    "def data_cleaner(in_file, out_file, cap_dict = 'capitalized_words.txt'):\n",
    "    in_file_read = open(in_file, 'r', encoding='utf-8', errors='ignore')\n",
    "    out_file_write = open(out_file, 'w')\n",
    "    cap_set = set()\n",
    "    for line in in_file_read:\n",
    "        # remove the line/conversation codes at start of each line\n",
    "        new_line = re.sub('^L[0-9]+\\s[+$]+\\s[a-z0-9]+\\s[+$]+\\s[a-z0-9]+\\s[+$]+\\s[A-Z]+\\s[+$]+\\s', '', line)\n",
    "        \n",
    "        new_line = new_line.lower()\n",
    "        new_line = re.sub('\\'m', ' am', new_line)\n",
    "        new_line = re.sub('`m', ' am', new_line)\n",
    "        new_line = re.sub('let\\'s', 'let us', new_line)\n",
    "        new_line = re.sub('let`s', 'let us', new_line)\n",
    "        new_line = re.sub('\\'s', ' is', new_line)\n",
    "        new_line = re.sub('`s', ' is', new_line)\n",
    "        new_line = re.sub('\\'ll', ' will', new_line)\n",
    "        new_line = re.sub('`ll', ' will', new_line)\n",
    "        new_line = re.sub('\\'ve', ' have', new_line)\n",
    "        new_line = re.sub('`ve', ' have', new_line)\n",
    "        new_line = re.sub('\\'re', ' are', new_line)\n",
    "        new_line = re.sub('`re', ' are', new_line)\n",
    "        new_line = re.sub('\\'d', ' would', new_line)\n",
    "        new_line = re.sub('`d', ' would', new_line)\n",
    "        new_line = re.sub('won\\'t', 'will not', new_line)\n",
    "        new_line = re.sub('won`t', 'will not', new_line)\n",
    "        new_line = re.sub('wouldn\\'t', 'would not', new_line)\n",
    "        new_line = re.sub('wouldn`t', 'would not', new_line)\n",
    "        new_line = re.sub('don\\'t', 'do not', new_line)\n",
    "        new_line = re.sub('don`t', 'do not', new_line)\n",
    "        new_line = re.sub('can\\'t', 'cannot', new_line)\n",
    "        new_line = re.sub('can`t', 'cannot', new_line)\n",
    "        new_line = re.sub(' +', ' ', new_line)\n",
    "        new_line = re.sub('[-+]?[0-9]+[,0-9]*(\\.[0-9]+)?', 'number', new_line)\n",
    "        new_line = re.sub('[()\"#@/;:<>{}+=~ยง|.!?,\\[\\]]', '', new_line)\n",
    "        out_file_write.write(new_line)\n",
    "\n",
    "    in_file_read.close()\n",
    "    out_file_write.close()\n",
    "    \n",
    "\n",
    "def tokenize_file(in_file):\n",
    "    in_file_read = open(in_file, 'r')\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    all_tokens = set()\n",
    "    for line in in_file_read:\n",
    "        new_tokens = pattern.findall(line)\n",
    "        all_tokens.update(new_tokens)\n",
    "    \n",
    "    in_file_read.close()\n",
    "    return list(all_tokens)\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabetic character\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())    \n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner('movie_lines.txt', 'out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 308310)\n",
      "(1, 308310)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_file('out.txt')\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "vocab_size = len(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
