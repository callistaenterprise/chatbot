{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(in_file, out_file):\n",
    "    in_file_read = open(in_file, 'r', encoding='utf-8', errors='ignore')\n",
    "    out_file_write = open(out_file, 'w')\n",
    "    \n",
    "    for line in in_file_read:\n",
    "        # remove the line/conversation codes at start of each line\n",
    "        new_line = re.sub('^L[0-9]+\\s[+$]+\\s[a-z0-9]+\\s[+$]+\\s[a-z0-9]+\\s[+$]+\\s[A-Z]+\\s[+$]+\\s', '', line)\n",
    "        \n",
    "        new_line = new_line.lower()\n",
    "        new_line = re.sub('\\'m', ' am', new_line)\n",
    "        new_line = re.sub('`m', ' am', new_line)\n",
    "        new_line = re.sub('let\\'s', 'let us', new_line)\n",
    "        new_line = re.sub('let`s', 'let us', new_line)\n",
    "        new_line = re.sub('\\'s', ' is', new_line)\n",
    "        new_line = re.sub('`s', ' is', new_line)\n",
    "        new_line = re.sub('\\'ll', ' will', new_line)\n",
    "        new_line = re.sub('`ll', ' will', new_line)\n",
    "        new_line = re.sub('\\'ve', ' have', new_line)\n",
    "        new_line = re.sub('`ve', ' have', new_line)\n",
    "        new_line = re.sub('\\'re', ' are', new_line)\n",
    "        new_line = re.sub('`re', ' are', new_line)\n",
    "        new_line = re.sub('\\'d', ' would', new_line)\n",
    "        new_line = re.sub('`d', ' would', new_line)\n",
    "        new_line = re.sub('won\\'t', 'will not', new_line)\n",
    "        new_line = re.sub('won`t', 'will not', new_line)\n",
    "        new_line = re.sub('wouldn\\'t', 'would not', new_line)\n",
    "        new_line = re.sub('wouldn`t', 'would not', new_line)\n",
    "        new_line = re.sub('don\\'t', 'do not', new_line)\n",
    "        new_line = re.sub('don`t', 'do not', new_line)\n",
    "        new_line = re.sub('can\\'t', 'cannot', new_line)\n",
    "        new_line = re.sub('can`t', 'cannot', new_line)\n",
    "        new_line = re.sub(' +', ' ', new_line)\n",
    "        new_line = re.sub('[-+]?[0-9]+[,0-9]*(\\.[0-9]+)?', 'number', new_line)\n",
    "        new_line = re.sub('[()\"#@/;:<>{}+=~ยง|.!?,\\[\\]]', '', new_line)\n",
    "        out_file_write.write(new_line)\n",
    "\n",
    "    in_file_read.close()\n",
    "    out_file_write.close()\n",
    "    \n",
    "\n",
    "def tokenize_file(in_file):\n",
    "    in_file_read = open(in_file, 'r')\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    all_tokens = set()\n",
    "    for line in in_file_read:\n",
    "        new_tokens = pattern.findall(line)\n",
    "        all_tokens.update(new_tokens)\n",
    "    \n",
    "    in_file_read.close()\n",
    "    return list(all_tokens)\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabetic character\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())    \n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner('testfile.txt', 'out.txt')\n",
    "#data_cleaner('movie_lines.txt', 'out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = \"After the deduction of the costs of investing, \" \\\n",
    "#      \"beating the stock market is a loser's game.\"\n",
    "#tokens = tokenize(doc)\n",
    "tokens = tokenize_file('out.txt')\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "#print(X)\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):    \n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will's neighbor words: ['with', 'are', 'yo', 'number']\n",
      "yo's neighbor words: ['too', 'with', 'will', 'are']\n",
      "are's neighbor words: ['he', 'too', 'with', 'yo']\n",
      "with's neighbor words: ['number', 'he', 'are', 'too']\n",
      "too's neighbor words: ['again', 'number', 'he', 'with']\n",
      "he's neighbor words: ['she', 'again', 'too', 'number']\n",
      "number's neighbor words: ['thousand', 'she', 'he', 'again']\n",
      "again's neighbor words: ['line', 'thousand', 'she', 'number']\n",
      "she's neighbor words: ['testing', 'line', 'again', 'thousand']\n",
      "thousand's neighbor words: ['am', 'testing', 'line', 'she']\n",
      "line's neighbor words: ['and', 'am', 'testing', 'again']\n",
      "testing's neighbor words: ['negative', 'and', 'am', 'line']\n",
      "am's neighbor words: ['normal', 'negative', 'thousand', 'testing']\n",
      "and's neighbor words: ['would', 'normal', 'negative', 'line']\n",
      "negative's neighbor words: ['they', 'would', 'normal', 'and']\n",
      "normal's neighbor words: ['one', 'they', 'would', 'negative']\n",
      "would's neighbor words: ['first', 'and', 'one', 'negative']\n",
      "they's neighbor words: ['also', 'would', 'one', 'first']\n",
      "one's neighbor words: ['not', 'they', 'first', 'would']\n",
      "first's neighbor words: ['decimals', 'not', 'also', 'one']\n",
      "also's neighbor words: ['million', 'decimals', 'not', 'first']\n",
      "not's neighbor words: ['or', 'million', 'one', 'decimals']\n",
      "decimals's neighbor words: ['no', 'also', 'first', 'or']\n",
      "million's neighbor words: ['many', 'no', 'or', 'not']\n",
      "or's neighbor words: ['second', 'many', 'no', 'decimals']\n",
      "no's neighbor words: ['blanks', 'second', 'many', 'decimals']\n",
      "many's neighbor words: ['is', 'blanks', 'second', 'no']\n",
      "second's neighbor words: ['i', 'blanks', 'many', 'or']\n",
      "blanks's neighbor words: ['i', 'ars', 'is', 'no']\n",
      "is's neighbor words: ['i', 'ars', 'twice', 'many']\n",
      "i's neighbor words: ['aint', 'is', 'twice', 'blanks']\n",
      "ars's neighbor words: ['aint', 'is', 'twice', 'blanks']\n",
      "twice's neighbor words: ['aint', 'is', 'twice', 'blanks']\n",
      "aint's neighbor words: ['i', 'ars', 'twice', 'many']\n"
     ]
    }
   ],
   "source": [
    "#for input_ind in range(vocab_size):\n",
    "#    input_word = id_to_word[input_ind]\n",
    "#    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "#    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
