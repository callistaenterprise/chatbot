 Going back to the roots of OOD has been commonly advocated since Eric Evans presented his book [Domain-Driven Design: Tackling Complexity in the Heart of Software].  In database systems, DDD often takes its representation in a class model of the persistent information to be managed by a service or a system. If Java is used to implement the entity classes describing persistent objects, JPA may be the persistence technology  used to add data base persistence behavior to the Entity POJOs.  Within DDD, the idea is to allocate all logic .   A challenge with DDD is to know what should go into the object model and what shouldn't. There are many types of behavior that are specific to process or external context that - when pushed into the entity classes - makes them cluttered and potentially fragile. They become fragile, when the the forces that generate change are related to external parties rather than the core business for which the model was initially creates. Change management and dependency management becomes complex.   The Java language provides little support for dealing with this problem. [qi4j] is an interesting approach to deal with many aspects of POJO cluttering, but not the dependency problem. With qi4j, model objects can only be extended by making changes to the definition of Java interface of a model class. This creates dependencies between change management processes and clutters the dependencies of the entities. I think qi4j carries a lot of promise, but its evolution is bound to the limitations of the Java language, which is sometimes a problem.  In Mid 90's, I worked with Objective-C on the [NextSTEP platform].  I've missed this ability in Java since I first learned it. It is how ever available in [Groovy]. The remaining part of this blog entry illustrates how Groovy categories can be used to extend the use of DDD without cluttering your domain model.   I see Groovy typically being used in the following contexts:  - As the core development language of web application development based on the [Grails framework] - As integration language complementing Java projects due to it's phenomenal XML processing capabilities  - As scripting language  My example for this blog is of the second category. This is the problem:  Create an RSS feed on top of the AccuWeather RESTFul web service .  Prerequisites:  - Use JAXB-generated POJOs available in an existing jar file to parse weather data from the AccuWeather service.  The task:  - Use Groovy to create a web app that transforms the response from the AccuWeather service to an RSS 2.0 feed. - The result should look like this   _Bild saknas_  Design constraints:  - DDD should be applied such that entity logic related to producing RSS output should be added to the domain objects  - All RSS-related logic should be in the same project as the logic that uses/depends on that logic. The logic should be added to the JAXB POJO:s in runtime, when needed by the Groovy RSS servlet.   We need to create a Groovy servlet that requests XML weather data from the AccuWeather weather service, parses the response into JAXB-objects produced from the XML Schema of the service and finally writes RSS XML to the servlet response by accessing the JAXB POJOs. Here's the Servlet:   The interesting part is the -keyword that in runtime applies a category that adds the method  that is later invoked as if it was part of the domain class DayType `.  What is then exactly a Groovy Category? It is actually just a plan Groovy class that follows a set of conventions, that allows it to be applied as a Category using the use-keyword. Here's the category class that defines the  method for the  class:    I do think the type of construct represented by the Groovy Categories is a great way to make DDD scale for real-world-scenarios. In real-world scenarios for DDD require some way of separating concerns so that a core domain object can add value .  Traditionally the solutions would involve various types of advanced patterns or frameworks. The sample used in this blog, would require the Visitor Pattern in order to keep the same level of separation of concerns and dependencies. Using qi4j is a Java-only framework solution that would solve the cluttering-problem, but still make the POJO library depend on the fact that it needs to support the needs of an RSS feed. In this specific case, when we are not in control  of the domain classes, qi4j would - to my understanding - not be useful.  With category support in the language , DDD becomes much more intuitive to implement.   I've attached a [zip] with the a maven multi project for running the sample. The zip contains one project for the feed web-app and one project that builds the jaxb model classes from the xml schema of the accu weather data response payload.  If you want to use eclipse, then issue...   ...which generates an eclipse project for each of the maven projects. Make sure you are located in the  folder when running the command. To run/debug in Eclipse with WTP you will also need the Groovy plug-in, available here: [http://dist.codehaus.org/groovy/distributions/update/] Have fun!  **Improved syntax with AST transformations**  With Groovy 1.6, something called [AST Transformations]. Using this way of defining the category, it would look like: 
 How do you get your application to the cloud? This is a complex question to answer shortly but in the following text there are some suggestions that can serve as a starting point.  -[readmore]-  At SpringOne2GX a lot of talks were centred around aspects of cloud-centric applications. This also included how to make your applications ”cloud-safe”. One thing that constantly was mentioned was [The Twelve-Factor App].  The twelve-factor app is a collection of patterns  for cloud applications. Not so surprising there are 12 patterns ☺  To give you a quick overview these will be listed shortly below to give you a hint what this is all about:  1. Codebase; One codebase tracked in revision control, many deploys. 2. Dependencies; Explicitly declare and isolate dependencies 3. Config; Store config in the environment 4. Backing Services; Treat backing services as attached resources 5. Build, release, run; Strictly separate build and run stages 6. Processes; Execute the app as one or more stateless processes 7. Port binding; Export services via port binding 8. Concurrency; Scale out via the process model 9. Disposability, maximize robustness with fast startup and graceful shutdown 10. Dev/prod parity; Keep development, staging, and production as similar as possible 11. Logs; Treat logs as event streams 12. Admin processes; Run admin/management tasks as one-off processes  Many of these patterns can also serve as best practices for a **normal application!** Take for example pattern 1; to have one codebase tracked in revision control.  A way to start to cloud-enable your applications is to go through these patterns with your application in mind. Even if you find a pattern that doesn’t make sense right away, it could still be a mind-opening experience.  Also bear in mind that an application in the context of twelve-factor app refers to a single deployable unit. Your application could consist of multiple collaborating deployed components, in the twelve-factor context this is referred to as a distributed system.  Another good source to look into are a book by Matt Stine, [Migration To Cloud-Native Application Architectures].  It is short  and makes a good introduction into why and how to migrate to the cloud.  I hope these short notes can help you start your way to the cloud if you still are firmly on the ground but would like to try flapping your wings a bit.
 This blog series cover various aspects of building microservices using Java and Go with supporting services from [Spring Cloud] containers, e.g. container orchestration tools.  -[readmore]-    [An operations model for Microservices] - An introduction blog post that motivates why supporting services from *Spring Cloud* and *Netflix OSS* are required in a microservices based system landscape.   | Java | Go | |---|---| | [Part 1: Using Netflix Eureka, Ribbon and Zuul] | | [Part 2: Trying out the circuit breaker, Netflix Hystrix ] | | [Part 3: Secure API's with OAuth 2.0] | | [Part 4: Dockerize your Microservices] | | [Part 5: Upgrade to Spring Cloud 1.1 & Docker for Mac] | | [Part 6: Adding a Configuration Server] | | [Part 7: Distributed tracing with Zipkin and Spring Cloud Sleuth] | [Part 8: Centralized logging with the ELK stack] | | | [Part 9: Messaging with AMQP] | | | [Part 10: Logging to a LaaS with Logrus and Docker's log drivers] | | | [Part 11: Circuit Breakers and resilience with Hystrix] | | | [Part 12: Distributed tracing with Zipkin] | | | [Part 13: Distributed persistence with CockroachDB and GORM] | | | [Part 14: GraphQL with Go] | | | [Part 15: Monitoring with Prometheus and Grafana] | | | [Part 16: It's 2019, time for a rewrite!] |  Stay tuned for more posts!  If you want to learn about new features in Docker, take a look at the blog series - [Trying out new features in Docker].
 In this blog post I will describe how to setup a local Docker Swarm cluster for test purposes with a minimal memory usage and fast startup times, using [Docker in Docker] instead of traditional local virtual machines.  -[readmore]-  This blog post is part of the blog series - [Trying out new features in Docker].   Before deploying your Docker containers to production you most probably need a *Container Orchestrator*, such as [Docker in Swarm mode].  Before deploying your Docker containers to a container orchestrator, you need to understand how your container orchestrator works. Even though you can setup a test instance in the cloud to get used to it, it is typically much more convenient and efficient to learn a complex software, such as a container orchestrator, if you initially can try it out locally.  Traditionally, setting up a local multi - node container orchestrator means spinning up a number of virtual machines locally and then setting up the container orchestrator on these nodes. It is known to both consume a lot of memory  and is also time consuming to setup.  ![docker-swarm] **Source:** [http://info.crunchydata.com/blog/easy-postgresql-cluster-recipe-using-docker-1.12]   But, what if we can use a number of containers to act as our cluster nodes instead of fully fledged virtual machines?  We know for sure that they start much faster and have very little overhead in terms of memory!  The *only* tricky thing is that these containers needs to run Docker in themselves, i.e. run Docker in Docker :-)  Docker provides a official Docker image for this specific purpose: [Docker in Docker]  > **Words of warning:** You should only use Docker in Docker for test purposes. See, for example, this [blog post] for details.  Let's try it out using [Docker in Swarm mode]!  We will use [Docker for Mac].   First, you need to have [Docker for Mac] installed, I'm on version .  Next, you also need to have  and  installed to be able to follow my instructions below. If you use [Homebrew] they can be installed with:  brew install jq brew install ctop  >  is like , but for containers.   To be able to follow the startup sequence, launch the  tool in one terminal window:  # Monitor Master and Worker nodes ctop  Next, in another terminal window, run commands to:  1. Ensure that Docker for Mac runs in Swarm mode, i.e. acts as a Swarm Master 1. Setup environment variables for the token to be used to join worker nodes and the IP address of the master node  Run the following commands:  # Init Swarm master docker swarm init  # Get join token: SWARM_TOKEN=$ echo $SWARM_TOKEN  # Get Swarm master IP  SWARM_MASTER_IP=$ echo $SWARM_MASTER_IP  Next, setup environment variables for what Docker version to use and how many worker nodes you want:  # Docker version DOCKER_VERSION=17.09.1-ce-dind  # Number of workers NUM_WORKERS=3  Now, we can launch the worker nodes as containers and ask them to join our Docker Swarm cluster:  # Run NUM_WORKERS workers with SWARM_TOKEN for i in $; do docker run -d --privileged --name worker-$ docker --host=localhost:$:2377 done   The first `docker run` command might take some time due to the required Docker image needs to be downloaded, otherwise it should only take a few seconds to setup the Swarm cluster!  Verify that you can see the worker nodes using  as ordinary containers in Docker for Mac:  ![ctop-3-workers]  > **Note:** The memory used per worker node is only some 30 MB! > To be compared to  1 GB required for an empty worker node that runs as a virtual machine...  To be able to monitor the state of the cluster you can start a Visualizer service as:  # Setup the visualizer docker service create \ --detach=true \ --name=viz \ --publish=8000:8080/tcp \ --constraint=node.role==manager \ --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \ dockersamples/visualizer   Ok, so let's see if we actually have a Docker Swarm cluster up and running:  docker node ls  It should result in a response like:  ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS p04lc6eikqz76rdzp9ehpejdx * moby Ready Active Leader owpvm6mt8xpoxqk28mph81n0g worker-1 Ready Active blf4o51phy1k7ds7pn2sok1h9 worker-2 Ready Active wtia3kscr694577je4v5ryms4 worker-3 Ready Active  Also, let's see if we have the Visualizer service up and running:  docker service ls  Expect output as follows:  ID NAME MODE REPLICAS IMAGE PORTS nv7uhth4k3vw viz replicated 1/1 dockersamples/visualizer:latest *:8000->8080/tcp  > **Note:**  set to  means that the service is up and running!  Try out the Visualizer: [localhost:8000]  Expect only a single Visualizer container to show up at this point:  ![visualizer-1]   Now, let's deploy a service and try it out!  I have a very simple   that creates some random quotes about successful programming languages. It can be deployed with the following command:  docker service create --name quotes-service --detach=false -p 8080:8080 magnuslarsson/quotes:go-22  > **Note**: This time we set the  - flag to  meaning that the docker service create command waits to complete until the service is actually accessible, i.e. at least one container for the service reports to Docker that it is up and running.  Expect an output from the command like:  xvcvdc28kt1or1tslj0dv5ptn overall progress: 1 out of 1 tasks 1/1: running [==================================================>] verify: Service converged  The Visualizer should report:  ![visualizer-2]  We can now try it out using :  curl localhost:8080/api/quote -s -w "\n" | jq .  Output should look like:  { "ipAddress": "fc9193f817ef/", "quote": "If I had to describe Go with one word it’d be ‘sensible’.", "language": "EN" }  The most interesting part of the response from the service is actually the field , that contains the hostname of the container that served the request,  in the sample response above.   This can be used to verify that scaling of a service actually works. In the output from a scaled service we expect different values in the  - field from subsequent requests, indicating that the request is load balanced over the available containers.  Let's try it out, shall we?  First, start a loop that use  to sends one request per second to the  and prints out the  - field from the response:  while true; do curl localhost:8080/api/quote -s -w "\n" | jq -r .ipAddress; sleep 1; done  Initially the output should return one and the same hostname, since we only have one container running in the service:  5fcdd3974d5c/ 5fcdd3974d5c/ 5fcdd3974d5c/ 5fcdd3974d5c/ 5fcdd3974d5c/ 5fcdd3974d5c/  Now, scale the  by adding 10 new containers to it :  docker service scale quotes-service=11 --detach=true  Verify that you can see all 11  containers in the Visualizer:  ![visualizer-3]  Now, the output from the curl - loop should report different hostnames as the requests are load balanced over the 11 containers:  5fcdd3974d5c/ 5aa06cf9a0df/ 8e02323f6b67/ 5fcdd3974d5c/ b9f8da53ec30/ be9489f011c3/ 559489f857d6/ 494d5b0a5b5a/ 4541532bc9bb/ 709a6b244149/  Great, isn't it?   Now, let's expose the container orchestrator, i.e. Docker in Swarm mode, to some problems and see if it handles them as expected!   First, let's shut down some arbitrary containers and see if the orchestrator detects it and start new ones!  To keep things relatively simple, let's kill all  running on the master node:  docker rm -f $  The command should respond with the ids of the killed containers:  25ca7c327cb4 6d28d64ff831  Take a quick look  in the Visualizer and you will see that there are two missing  containers in the master node:  ![visualizer-4]  But after a few seconds you will find the new containers started by the orchestrator to fulfill the desired state, i.e. 11 containers for the !   Now, let's make it even worse by killing a worker node, i.e. simulating that the node crash unexpected. Since we use Docker in Docker, this can be done by killing the worker nodes container. Let's shutdown :  docker rm -f worker-2  You will soon see how  disappears from the  display:  ![ctop-worker-2-down]  ...and the Visualizer also reports that  is down:  ![visualizer-5]  ...and soon after, the containers that was running on the failing node are rescheduled on the other nodes:  ![visualizer-6]  Let's wrap up with restarting the node again:  i=2 docker node rm worker-$ docker run -d --privileged --name worker-$ docker --host=localhost:$:2377  As you can see in the Visualizer, the new  node is back online but the containers are not rebalanced back to the new worker node automatically:  > **Note:** You might need to refresh the Visualizer window in the browser to see that the  node is back online.  ![visualizer-7]  You can, however, manually rebalance your containers with the commands:  docker service scale quotes-service=8 --detach=false docker service scale quotes-service=11 --detach=false  ![visualizer-8]   That's it, let's remove the Swarm cluster:  1. Remove the services 2. Unregister the worker nodes from the Swarm cluster 3. Shutdown the worker nodes  4. Remove the "Docker for Mac" Docker engine from the Swarm cluster  Run the following commands:  # Remove services docker service rm quotes-service viz  # Unregister worker nodes for i in $; do docker --host=localhost:$2375 swarm leave done  # Remove worker nodes docker rm -f $  # Leave Swarm mode docker swarm leave --force   For more blog posts on new features in Docker, see the blog series - [Trying out new features in Docker].
 This is a guide to setting up a lightweight messaging development environment with [Active MQ] as the application server. This tips might be of help to you regardless you are using another platform such as Websphere MQ or just want to try out JMS in your web or Java application.  When integrating enterprise applications and services, Websphere MQ as messaging backbone is a common choice. While Websphere MQ is a valid, stable platform for production environments, the same platform makes a large, clumsy footprint in the development environment, making testing of services almost impossible in a sensible way. This is totally the opposite way of agile development methods where unit testing and test automation plays an important role. A quite easy way to help you out is to replace WMQ in your development environment with a more lightweight alternative /platform such as Active MQ /Spring / Tomcat.  Design wise there are three main different scenarios to think about.  1. You are the service 2. You are the caller making an asynchronous call 3. You are the caller making a synchronous call   We begin with a look at the first scenario, where we need to be able to accept and read messages. Then we move on to act as the caller, the sender of these messages. I will not cover the third scenario in this post, why I will explain later.  First of all - download Tomcat, ActiveMQ, and Spring, create a web project and place the jars in the lib folder of your web app. Or you can just download the zip file containing the example code at the end of this page   There are several ways of implementing a Jms Listener and I will show you one way. There are four main artifacts needed to make this possible.  - A Jms listener implementation class that receives the call - A Spring configuration declaring the listener bean, the factory and destinations - We need to declare resources in the Tomcat context - We also have to bind the resources in the web.xml  Jms Listener:   In the spring context we need to create the queue connection factory and a destination that maps to the physical queue:   as well as a listener container in which we inject the listener   Then we need to let Tomcat be aware of the messaging server. It is possible to embed the ActiveMQ instance in the same JVM as Tomcat, but I have chosen to let ActiveMQ startup as a standalone server to make this environment resemble a normal setup as much as possible. If you are running Windows, ActiveMQ comes with a handy possibility to create a Windows service. There is also a practical reason for keeping ActiveMQ separate - it takes about ten seconds for ActiveMQ to start which makes it quite cumbersome when you need to restart yor Tomcat now and then.  In the Tomcat context of your application, make a reference to a connection factory as well as to the specific queue:   One thing that is nice about Tomcat is that you can place this context information in a file named context.xml in the META-INF folder of your web application and Tomcat just reads it from there. No need to fiddle in the config folders.  The last thing we need is to map the resources in web.xml and kickstart the Spring context initialization:    This is all to it, just deploy and run ActiveMQ and Tomcat. Since ActiveMQ creates the queue on the fly we don't have to care about doing that by hand. There is an admin gui in ActiveMQ that lets you send simple text messages. Just go to [http://localhost:8161/admin] and try your new service.   The second scenario is to call our remote service asynchronously through the messaging platform. I am using the Spring JmsTemplate to send messages so we need to add this to our config file as well as the bean that uses it:   The Java code of the caller bean:   In order to test this, create a servlet which could contain code like this:   And there you go!  The third scenario - the Request-Reply pattern - is something I may address in a later post. The Spring JmsTemplate currently lacks  that is extracted from the sent message. This pattern, though, is supposed to be addressed in the next version.   - [http://activemq.apache.org/] - [http://www.springframework.org/] - [http://tomcat.apache.org/]
 Have you ever tried to setup a secure communication using HTTPS?  With mutual authentication, a service consumer, an ESB in the middle, a service producer and a number of certificates and truststores this can be quite challenging and time consuming. This is however a very typical integration scenario that we help our customers to implement over and over again. Getting all the security configuration in place at the same time always seem to be a challenge...  Based on experiences from a number of projects we have added support in the latest version of [soi-toolkit], v0.6.0, to automate the initial setup of secure communication in Mule ESB using HTTPS with mutual authentication.  -[readmore]-  Soi-toolkit is integrated with Mule Studio as a Eclipse plugin and can after a few clicks create a proper setup for you including mule flow, unit and integration tests, test consumer, teststub for the service producer, sample certificates and truststore and a property file that you can use to simply replace the sample certificates to your own once you are ready for that.  A sample secure mule flow created by soi-toolkit looks like:  ![Mule Flow]  ...and execution of the generated unit and integration tests looks like:  ![jUnit tests]  A [tutorial].  In this tutorial you can as well get help on testing the secure service using soapUI:  ![soapUI]  ...and see a proof on that the secure setup actually works:  ![tcpmon]  Give it a try and run through the [tutorial]!
 Sometimes, you just need to leave all that microservice and enterprise stuff behind and do some old-fashioned coding just for fun. This blog post describes how I - just for the fun of it - wrote a [Golang] program that can control IKEA Trådfri home automation using CoAP over DTLS.  _Important note: I am in no way whatsoever affiliated with IKEA or take any responsibility if this stuff breaks your IKEA stuff... ;)_  1. Overview 2. DTLS 1.2 3. The CoAP protocol 4. The Trådfri API 5. Running 6. Summary  The objective of this blog-post is to write a native Go application that can talk to the IKEA Trådfri gateway in order to query/control lights and other devices. There already exists quite a few 3rd party applications that performs exactly this feat such as [coap-client]. I've drawn inspiration from several of these libraries, but wanted to see if I could do something similar using a pure Go implementation.  This blog post does not aim to be an advertising campaign for IKEA produts, but in order to put this venture into leisure-coding into some kind of context, I'll describe the home-automation setup briefly:  The trådfri series provides a number of products for home automation including the following:  * Light bulbs of various capabilties  * Light panels * Power plugs  * Motion sensors * Electric blinders * Accessories such as remote control, dimming controls * The gateway  My personal setup currently consists of three light bulbs, one power plug, a remote and  their Gateway.  ![overview]  The Raspberry Pi 3 is the unit running the Go program this blog post is about. It works just as well on my dev Mac so it should be possible to run it on any OS/arch Golang supports.  Most  mesh network. Using a phone with Trådfri isn't strictly necessary as the remote control can be used without WiFi, but I guess most users will use either the iOS or Android app to set things up.  Trådfri also supports integration with Apple HomeKit and Amazon Alexa, but that integration is not in scope for this blog post.  The objective of this blog post is to develop a standalone program using Go that can talk to the Gateway and both query the state of bulbs etc as well as controlling groups or individual devices. Communication from our Go program to the Gateway is based on the CoAP protocol running over DTLS 1.2, with CoAP payloads and identifiers based on [LWM2M]. I'll get back to CoAP and DTLS a few sections down.  When starting this little venture, I set up the following objectives for the program:  * Use Go-native implementations of DTLS and CoAP, e.g. no CGo or proxying through OpenSSL etc. * Provide a simple RESTful API to query the state of the devices * Provde a simple RESTful API to control on/off, dimming, color etc * Stretch goal 1: Create a simple web-based GUI that uses the RESTful endpoints. * Stretch goal 2: Continuously query and store the state of the bulbs etc in a time-series database for future use as training data for a neural network, potentially providing autonomous operation of my bulbs etc in a sensible manner. * Deployment on a Raspberry Pi 3  The source code for the program can be found here:  [https://github.com/eriklupander/tradfri-go]  [DTLS] is short for "Datagram Transport Layer Security", i.e. TLS over UDP. 1.0 and 1.2 of DTLS respectively maps closely to normal TLS 1.0 and 1.2, with a few differences to accomondate the differences between TCP and UDP transports.  I've based my tradfri-go application on the [DTLS] for now.  DTLS supports a number of authentication schemes including certificate-based solutions as well as "Pre-shared keys" , which is what the Trådfri series uses. The PSK of your Gateway is printed on the sticker on the underside of the gateway.  The PSK on the gateway is however only used for obtaining _another_ key, an important piece of information I found [here]. The printed PSK is used to do an initial authentication with the gateway, which returns the key used for all subsequently interactions with the Gateway:  ![PSK exchange]  The handshake in DTLS  has the purpose of exchanging keys, specifying cipher to be used etc between client and server in order to establish a securely encrypted connection. The details of this is typically handled by your DTLS library of choice, but I guess a simple overview of a DTLS handshake with PSK authentication can be fun to include for educational purposes:  ![DTLS handshake]  For more details, I suggest reading [RFC6347].  Anyway - the [dtls] library handles all of this for us as long as we can supply the correct Client_id, PSK and IP to the gateway.   Here's the tradfri-go code where the handshake is performed behind the scenes. I've defined a struct _DtlsClient_ that encapsulates the dtls.Peer and the message counter:  // DtlsClient provides an domain-agnostic CoAP-client with DTLS transport. type DtlsClient struct { peer *dtls.Peer msgID uint16 gatewayAddress string clientID string psk string }  // NewDtlsClient acts as factory function, returns a pointer to a connected  DtlsClient. func NewDtlsClient *DtlsClient { client := &DtlsClient{ gatewayAddress: gatewayAddress, clientID: clientID, psk: psk, } client.connect return client }  Here's the handshake/connect code:  func  { dc.setupKeystore  listener, err := dtls.NewUdpListener if err != nil { panic }  peerParams := &dtls.PeerParams{ Addr: dc.gatewayAddress, Identity: dc.clientID, HandshakeTimeout: time.Second * 15} fmt.Printf  dc.peer, err = listener.AddPeerWithParams if err != nil { fmt.Printf os.Exit } dc.peer.UseQueue fmt.Printf }  As one can see, the complexities of the DTLS handshake is fully handled behind the scenes. The returned [dtls.Peer] can then be used to write and read arbitrary _[]byte_ just like any socket, i.e:  // Write data over the socket err = dc.peer.Write if err != nil { return coap.Message, err }  // Wait for response respData, err := dc.peer.Read if err != nil { return coap.Message, err } // do something with the response...  Defined in [RFC7252]:  "specialized web transfer protocol for use with constrained nodes and constrained  networks."  It re-uses much of the well-known RESTful paradigm with verb-based methods  and servers providing resources under a URL.  Message payloads can be anything you want, e.g. JSON, XML or some arbitrary binary format, and the content-type of payloads can be specified with headers just like in HTTP. It also borrows [response codes] very similar to HTTP, such as "_4.00 Bad Request_" and "_4.04 Not Found_". The 2.XX response codes indicating a successful request has slightly different semantics compared to HTTP. For example, no "_2.00 OK_" exists.  The actual messages are encoded into a compact format to conserve resources such as bandwidth and CPU-cycles. Again - the gritty details of which info that goes into which bit is out-of-scope for this blog post, but as reference the format looks like this:  ![coap header] _Source: Wikimedia Commons_  Writing a CoAP message serializer/deserializer may sound like fun, but perhaps not fun enough for me to do it given that there already exists a nice CoAP library for Go: [go-coap].  Dustin's library makes creating/parsing CoAP-messages a breeze. Here's an example where I build a GET message:  func  coap.Message { dc.msgID++ req := coap.Message{ Type: coap.Confirmable, Code: coap.GET, MessageID: dc.msgID, } req.SetPathString return req }  The _coap.Message_ can then be serialized into a byte-array and written to the dtls.Peer and the response is as easily read and deserialized into a coap.Message.  I've wrapped the CoAP / Trådfri stuff into a struct _TradfriClient_ that encapsulates the _DtlsClient_:  type TradfriClient struct { dtlsclient *dtlscoap.DtlsClient }  func NewTradfriClient *TradfriClient { client := &TradfriClient client.dtlsclient = dtlscoap.NewDtlsClient return client }  Here's the code that performs a GET for a given resource, for example the state of a bulb:  func  { device := &model.Device  resp, err := tc.Call if err != nil { return *device, err } err = json.Unmarshal if err != nil { return *device, err } return *device, nil }  The _tc.Call_ proxies to the _Call_ method of the DtlsClient which writes and reads plain bytes to/from the peer:  func  { // Serialize msg struct into raw CoAP payload data, err := req.MarshalBinary if err != nil { return coap.Message, err }  // Write the payload into the peer  err = dc.peer.Write if err != nil { return coap.Message, err }  // Wait for the response respData, err := dc.peer.Read if err != nil { return coap.Message, err }  // Deserialize the CoAP response into a coap.Message struct and return msg, err := coap.ParseMessage if err != nil { return coap.Message, err } return msg, nil }  Now that we can write and read CoAP messages over DTLS to the IKEA Gateway, it's time to explore the capabilties of the CoAP API of the IKEA trådfri gateway.  The CoAP endpoints on the Trådfri Gateway are _not_ an official API, though IKEA has stated an intent to someday make an API available for official use.  There are a number of unofficial resources describing the various CoAP endpoints and data structures that I've used to create this client:  * https://gist.github.com/hardillb/4ce9fc493b792806e39f7fae4b7c28a7 * https://learn.pimoroni.com/tutorial/sandyj/controlling-ikea-tradfri-lights-from-your-pi * https://bitsex.net/software/2017/coap-endpoints-on-ikea-tradfri/  The kind people in the links above have deducted a few basic guidelines that the Trådfri API seems to be built upon:  - /15004 returns an array of identifiers for groups configured in your setup.   [131073]  - /15004/131073 returns that group     - /15001/65536 is the remote control - /15001/65537 is the power outlet - /15001/65538 is the first light bulb in my setup, here's a sample response:     Let's try to make some sense of the payload examples above. The CoAP messages largely follows OMA LWM2M, i.e. the "Open Mobile Alliance Lightweight Machine 2 Machine" standard. Their [registry] provides some descriptions on various codes. For example, we can see that 5850 is _"an on/off actuator, which can be controlled, the setting of which is a Boolean value where True is On and False is Off."_.  However, a lot of those codes - especially those in the 9xxx range - doesn't seem to be in a public registry, so some guessing, reading resources such as the links above and reverse-engineering the messages is required. Let's break the device message down:  { "9019": 1, // No idea "9001": "Färgglad", // The name I gave this bulb in the Tradfri app "9002": 1550336061, // Some unix timestamp "9020": 1551635481, // Some unix timestamp "9003": 65538, // Object id "9054": 0, // No idea "5750": 2, // Application Type "3": { // Device type metadata? "0": "IKEA of Sweden", // Vendor name "1": "TRADFRI bulb E27 CWS opal 600lm", // Device type name "2": "", // No idea "3": "1.3.009", // Device type id? "6": 1 // No idea }, "3311": [ // Device values { "5708": 42596, // Something with color... ? "5850": 1, // Device power on/off "5851": 110, // Dimmer  "5707": 5427, // Something with color... ? "5709": 30015, // X color  "5710": 26870, // Y color  "5706": "f1e0b5", // Hex color "9003": 0 } ] }  If one were to build a user interface  for viewing your IKEA Trådfri setup, I wouldn't want the payload above exposed as-is for the client to use. I'd map the relevant stuff into a new JSON struct and pass that. In Go terms, a few structs like these could represent bulbs and power plugs:  type DeviceMetadata struct { Id int  Name string  Vendor string  Type string  }  type PowerPlugResponse struct { DeviceMetadata DeviceMetadata  Powered bool  }  type BulbResponse struct { DeviceMetadata DeviceMetadata  Dimmer int  CIE_1931_X int  CIE_1931_Y int  RGB string  Powered bool  }  For other device types such as the Power plug or the remote, other fields may be relevant so I'm doing a bit of composition so common stuff can go into the DeviceMetadata struct.  Let's get practical. The source code for this little program is on my [github page].  Clone the source code and build using:  > go build -o tradfri-go  or produce binaries for different platforms:  > make release mkdir -p dist GO111MODULE=on go build -o dist/tradfri-go-darwin-amd64 GO111MODULE=on;GOOS=linux;go build -o dist/tradfri-go-linux-amd64 GO111MODULE=on;GOOS=windows;go build -o dist/tradfri-go-windows-amd64 GO111MODULE=on;GOOS=linux GOARCH=arm GOARM=5;go build -o dist/tradfri-go-linux-arm5  Start by finding out the IP-address to your Gateway. It's probably possible to do a quick port-scan or multicast to find it, but I chose to simply go into the admin GUI of my NetGear router and find the gateway there.  The deviceId looks like this: GW-A1D4A0D1FF45  Then, I recommend setting an environment variable with this IP, for example:  export GATEWAY_IP=192.168.1.19  It's also possible to pass the IP to the gateway to the "tradfri-go" executable using the --gateway_ip flag.  This is a 1-time step required before you can run in server mode or play around in the client mode. It will exchange the pre-shared key printed underside your Gateway for a new one bound to the _client_id_ you specify. All subsequent calls to the Gateway from _tradfri-go_ will then use these credentials for the DTLS handshake.  Running the command below will perform the token exchange and store your settings to a _config.json_ file.  ./tradfri-go --authenticate --client_id=MyCoolID --psk=TheKeyAtTheBottomOfYourGateway --gateway_ip=<ip to your gateway>  The new token is stored in the current directory in the file "config.json", which contains your clientId, the new PSK and the Gateway IP you specified, e.g:  > cat config.json { "client_id": "MyCoolID", "gateway_address": "192.168.1.19:5684", "gateway_ip": "192.168.1.19", "pre_shared_key": "the generated psk goes here", "psk": "the generated psk goes here" }  The program will try to read your gateway_ip, clientId and PSK from the _config.json_ file for both client and server modes.  If you don't feel like using _config.json_, you can either specify the configuration as command-line flags or using environment variables:  ./tradfri-go --server --client_id MyCoolID122 --psk mynewkey --gateway_ip=192.168.1.19  or  > export CLIENT_ID=MyCoolID1122 > export PRE_SHARED_KEY=mynewkey > export GATEWAY_IP=192.168.1.19 > ./tradfri-go --server  Configuration is resolved in the following order of precedence:  config.json -> command-line arguments -> environment variables  While my primary intent for tradfri-go is to run in its "server" mode, it also supports basic GET and PUT ops directly from the command-line that returns the raw JSON payload from the CoAP messages.  A few examples:  GET my bulb at /15001/65538:  ./tradfri-go --get /15001/65538   PUT that turns off the bulb at /15001/65538:  ./tradfri-go --put /15001/65538 --payload ''  PUT that turns on the bulb at /15001/65538 and sets dimmer to 200:  ./tradfri-go --put /15001/65538 --payload ''  PUT that sets color of the bulb at /15001/65538 to purple and the dimmer to 100:  ./tradfri-go --put /15001/65538 --payload ''  ![it's purple]  The colors possible to set on the bulbs varies. The colors are in the CIE 1931 color space whose x/y values _in theory_ can be set using the 5709 and 5710 codes to values between 0 and 65535. You can't set arbitrary values due to how the CIE 1931  works. Play around with the values, I havn't broken my full-color "TRADFRI bulb E27 CWS opal 600lm" yet...   To start in the server mode, which provides the [chi]-based HTTP REST API, just add the --server flag:  ./tradfri-go --server Running in server mode on :8080 Connecting to peer at 192.168.1.19:5684 DTLS connection established to 192.168.1.19:5684  Now, you can use the simple RESTful API instead which returns more human-readable responses. Get a device:  > curl http://localhost:8080/api/device/65538 | jq . { "deviceMetadata": { "id": 65538, "name": "Färgglad", "vendor": "IKEA of Sweden", "type": "TRADFRI bulb E27 CWS opal 600lm" }, "dimmer": 100, "xcolor": 30015, "ycolor": 26870, "rgbcolor": "f1e0b5", "powered": true }  Get a group:  > curl http://localhost:8080/api/groups/131073 | jq . { "id": 131073, "power": 0, "created": "2019-02-16T17:44:55+01:00", "deviceList": [ 65536, 65537, 65538, 65539, 65540 ] }  We can PUT to the _/api/device/_ endpoint to mutate the state of the bulb using three pre-defined settings:  > curl -X PUT --data '' http://localhost:8080/api/device/65538  Just like the client mode, the application will try to use clientId/PSK from _config.json_ or using env vars.  I havn't built a "complete" API, just a few ones as a proof of concept. See [router.go].   In its current state, the tradfri-go program isn't _that_ usable, it's mainly been an exercise up until now trying to get my head around CoAP, DTLS and how to interact with the Gateway.  I'd say that it could be a good foundation for building something more advanced such as custom GUI or that idea of continuously collecting the powered/dimmming/colors state of your bulbs and eventually combining that data with time-of-day, weekday, weather data and whatnot with the intent of training a neural network that could automate your lights, blinders etc. given historical data and various environmental circumstances.  It should also be possible to use the _github.com/eriklupander/tradfri-go/dtlscoap_ package as an external dependency to build something different around it.  Anyway - my primary intent was to have a good time writing some "non-enterprisy" code while learning something new, so I'm quite happy with this excerise! And not to forget - making lights blink is always fun!  Please help spread the word! Feel free to share this blog post using your favorite social media platform, there's some icons below to get you started.  Until next time!  // Erik
   In this blog we’ll do something quite different compared to the usual heavyweight stuff here on the blog. This blog post is 100 % guaranteed free from Microservices, Integration Patterns and ESB’s. We’ll build a “Golf Distance” smartwatch app for Garmin Connect IQ compatible devices.  ![frilagd-liten.jpg]  First of all, what is a “Golf Distance” application and why do I want one on my watch? Basically, as a golfer, you always want to know the distance to the green  from any given point on the hole you’re currently playing. Either you buy a laser rangefinder, use some smartphone app, buy an expensive purpose-built device or you hire a kick-ass caddie. Or you decide your brand new Garmin Fenix 3 multisport watch should be able to do the job too. Only that it doesn’t. Quite natural as Garmin has a separate product line for those club-swinging wanna-be Tigers/Jordans/Stensons. However - to my great enjoyment many new Garmin devices supports custom app development and deployment through their Garmin Connect IQ SDK. So instead of spending $300 or so for a another device, I decided to go down the DIY route and brew my very own distance measurement application.  Conceptually, it's extremely simple. At 1-second intervals let the [Positioning service] property or plain hardcoded. For the selected course and current hole, just perform a distance calculation between current watch geoposition and centre green geoposition and render this as text to the screen together with some hole info.  Example hard-coded course data, e.g. array of objects. Quite similar to javascript except the => instead of :  var testCourseHoles = [ , , ... ... ]  This is the programming language you use when writing code for Connect IQ applications. It’s a nice little language having a lot more in common with Java, PHP or Python than C in my humble opinion. It’s object-oriented, uses reference count based memory management and is Duck Typed. I love this little quote from the official Monkey C docs about duck typing:  _“Duck typing is the concept that if it walks like a duck, and quacks like a duck, then it must be a duck”_  The code is very readable and easy to follow if you’re used to some of the languages mentioned above. Here’s a little snippet from the GolfDistance application where we declare a onHold function that the underlying framework will execute when a given button has been pressed for more than N seconds:  class KeyDelegate extends Ui.InputDelegate {  function onHold { Sys.println; Ui.pushView( new Rez.Menus.MainMenu, new GolfDistanceMenuDelegate, Ui.SLIDE_UP ); return true; } }  Here’s another snippet with “import”-statements from our main “view” class:  using Toybox.WatchUi as Ui; using Toybox.Graphics as Gfx; using Toybox.System as Sys; using Toybox.Lang as Lang; using Toybox.Position as Position; using Toybox.Math as Math;  Very similar to imports in java, with each imported module given an alias. Yes, _module_. Those aliased imports aren’t classes, they’re modules now available to use in the namespace of the given .mc file we’re working in. Scoping is somewhat loose in Monkey C. A .mc file can contain any number of classes, functions and variable declarations. Variables and methods declared outside of the class scope becomes global and can be accessed throughout the application. Imported modules are made available to all classes and functions declared in the file.  Finally, let’s have a quick look at a function to calculate the equirectangular distance between two geopositions. It shows use of Math functions and built in number formatting on the duck-typed distance variable . No primitives in Monkey C btw.  function distance { var x = deg2rad; var y = deg2rad; var distance = Math.sqrt * R; return distance.format; }   Connect IQ applications runs in a virtual machine called Monkey Brains which exposes various APIs for accessing functionality from the underlying OS such a graphics, location services, storage and communication. The APIs are well [documented] and the limited scope for Connect IQ applications makes the APIs easy to comprehend and find one’s way around.  One of the quirkier things to consider is probably familiar to mobile developers - how does the underlying framework help me differ different devices from another? Some Garmin units have round screens, other rectangular. Some might have color displays, other b&w. Some units have certain sensors or features not available on others. Some uses touch screens, others buttons. Here is a list of [current compatible devices] directly from the manufacturer.  The Connect IQ SDK lets developers sort this out using a number of mechanisms. First of all, [manifest XML] files declaring app permissions, features required and similar stuff let’s the Garmin Connect Store show/hide your application from being installable on devices that’s never going to be able to run your app. Sounds rather similar to what Android does, right?  Furthermore, application resources such as layouts, images and fonts can be either defined in a device-specific manner or be overridden for certain types of devices. User input  that provides an abstraction letting you be key or touchscreen agnostic to a certain degree. Another mechanism is the ‘has’ keyword that during runtime can query the current device for support for a given API. This device doesn’t support barometric pressure? Ok, let’s hide that data field.  There’s a lot more to the Connect IQ SDK, but it’s time we got started with some actual coding.  Stop right there. Coding? Where do I code this stuff? Emacs? Notepad? Vi? Is there an IDE? Yes, there is an IDE - or at least a nicely working Eclipse Plug-in for Eclipse Luna. After installing the plug-in and SDK, building applications and running them in the simulator or on your device is straightforward and trouble-free. [Get started here].  ![filetree.png]  The heading sounds more heavyweight than it really is. The “architecture” of our application is more or less forced by Connect IQ and Monkey C conventions, but can be summarized in the sketch below: ![ConnectIQ.jpg]  "[Resources]" are the images, layouts, fonts, translatable texts and similar resources your application uses.  The “[App]”-method lives there even though there is no main method. But so you Javaheads get it. The App class also has some lifecycle methods in it. Most importantly, it declares the initial view of the system and what InputDelegate subclass to use for handling user input.  class GolfDistanceApp extends App.AppBase {  //! onStart is called on application start up function onStart   //! onStop is called when your application is exiting function onStop   //! Return the initial view of your application here function getInitialView { return [ new GolfDistanceView ]; } }  The two input delegates takes care of key presses , an example of such code was displayed earlier in this blog post.  The [Ui.View] takes care of actual rendering and also a bit of application logic.  class GolfDistanceView extends Ui.View {  // Load your resources here function onLayout { setLayout; }  // Called when this View is brought to the foreground. Restore // the state of this View and prepare it to be shown. This includes // loading resources into memory. function onShow { Position.enableLocationEvents; }  // Update the view function onUpdate { ...... // We'll return to this!  It has a few lifecycle methods that let’s us specify the  callback invoked when the View beomes visible.  That onShow method invokes Position.enableLocationEvents that in our case tells the OS to give us continuous positional updates and to hand them as they become available to the _onPosition_ method. Note the method reference using _:[methodName]_. Pretty nice!  function onPosition { posnInfo = info; Ui.requestUpdate; }  Above, we see us assigning the _geoPositionInfo_ to the globally declared variable _posnInfo_ and then requesting a Ui re-render which kicks off the _onUpdate_ method.  The onUpdate method, we request an UI update. I also believe UI updates are performed when we let an Input event propagate from the event handler after our own stuff is done.  In the _function onUpdate. In short, it's quite standard canvas-drawing stuff such as drawText, drawBitmap, fillPolygon as well as useful helpers for getting screen dimensions and measuring text size given a certain font and string.  In the GolfDistance app, we just draw text in various sizes, though it would be cool to do some graphical representation of distance left to hole. The current drawing code looks like this:  dc.setColor; dc.clear; dc.setColor; if {  var lat = posnInfo.position.toDegrees[0]; var lon = posnInfo.position.toDegrees[1];  var distanceStr = "" + distance, courses[currentCourseIdx]["holes"][currentHoleIdx]["lat"], courses[currentCourseIdx]["holes"][currentHoleIdx]["lon"]);  dc.drawText; } else { dc.drawText; }  Simple enough - set current rendering color and clear screen using that color. Then switch to our preferred text rendering color and check whether we have any positionInfo available. If so, extract the latitude and longitude, pass those to the distance we render an alternate text. That's it!  The full rendering code also renders a bit more text, it looks like this while waiting for position in the Simulator:  ![simulator.png]  This was another one of those "should this really be so simple" moments working with the Connect IQ SDK and its companion Eclipse plug-in.  ![filetree.png]  Yes, the file system of the watch is mounted on the local computer and when the ".prg" file is placed in the APPS directory, the GolfDistance app will automatically appear in the list of applications.   Garmin has an "[app store] are adhered to. I assume Garmin will not approve apps that doesn't function on devices declared as compatible in the manifest or crashes if a given feature doesn't exist and isn't gracefully handled etc.  Garmin allows [monetization] on apps published through the app store, though it seems that involves having companion smartphone applications for money, ANT+ hardware or some kind of paid subscription service through Garmin Connect.  Well, there's not actually that much more to this application. There's a nice-to-have function that starts recording distance traveled on a keypress and shows it until one presses the same button again. Useful when you want to measure the actual length of a given shot or when you just want to practice your distance measuring skills. Also, by long-pressing the up button the app goes into its context menu where one can pick which of the  golf courses you're currently playing.  Does it work? Absolutely! Used it quite a bit during the summer. Not that it helped my remedy my horrendous on-course performance, but that can't be blamed on the watch.  The [full source code] can be found on my personal github page.
 For more than a year, since the end of 2015, I’ve been working on a project based on [Amazon web services] was the only option we had.  All though my lessons learned might be perceived as being sligthly negative that is by no means representative for my overall experience which in contrary has been overwhelmingly positive. Using Lambda, for instance, has been most rewarding and has just worked with very little hassle. The support for [Node.js], that we use, has been first rate with really no breaking changes.  Unfortunately we had to build our own build and deploy tool due to the lack of options back in 2015 and now we have to live with all of its warts. We would, if we could, move out of it in an instant. Today when we create new subprojects we use the [Serverless framework] that cater for all kinds of specific needs that aren’t covered by the core plugins.  We provision all AWS resources we need in a single cloudformation stack, thus we have no modularization to speak of. I guess this monolithic approach is how many projects start out but in hindsight it would have been wise to break out resources that seldom change into their own stacks, e.g. [Elasticsearch] clusters. A modular approach would also allow for easier additions of new functionality and reuse of existing cloudformation templates. The cloudformation template we are sitting on today is a monster in size.  [AWS Kinesis] you can’t really use kinesis as it will add a considerable amount of latency to your flow. Used in conjunction with Lambda we have, at best, been able to get down to 1000 ms added latency. If your application doesn’t have the need for NRT events, then there is nothing better to use than Kinesis.  We use [MQTT] for our NRT purposes. Using MQTT is also, at the time of writing, the only way of setting up websockets in a truly serverless fashion, i.e. without provisioning an EC2 instance to handle it.  Our API is based on a client directly invoking a lambda and not going through [AWS API Gateway] at all. I would have preferred a REST API on top of the lambdas. I think the purpose of API Gateway is well put in the following quote:  > Amazon API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, > including traffic management, authorization and access control, monitoring, and API version management.  Thus, API Gateway is about handling the http side of things and lambda is about doing your backend chores. That we use lambda is really an implementation detail and nothing you want the user of your api to adhere to.  It has happened to us on several occasions that a kinesis stream has filled up without any records being processed due to a payload in one record causing the lambda to throw an error. Not taking care of the error inside of the Lambda means that the same kinesis batch will be processed over and over, until it is removed automatically when it falls out of kinesis 24 hour gliding window.  I think that non returning lambdas that aren’t part of a traditional request/response flow nearly always need to handle all errors inside of the lambda to avoid getting stuck and use a [dead letter queue] for those requests that can not be processed by the lambda. Typically an event is, after a certain number of retries, automatically moved to the DLQ.  In one subproject we used Lambda in conjunction with [AWS RDS], i.e. a relational database service. There is a definitive upside to this in that it enables local testing by allowing you to run an in memory db in place of the real db. The biggest issue we have had with this, and this is a big one in my opinion, is that your Lambda have to run in the same virtual private cloud  has ceased to work all of a sudden.  The statefulness of database connection pooling is another thing that goes against the grain of the Lambda stateless nature. For the time being I consider [AWS DynamoDB] to be the best fit for usage with Lambdas if you truly want to stay as close to a serverless vision on AWS. DynamoDB of course it has its own quirks and it is obviously not a relational db either.  The are a big list of [limits] for AWS Accounts. Some of these limits are hard ones, meaning there is no way you can change them and then there are soft limits that you can change, like how many concurrent lambdas you can have. Unfortunately it is not always possible to know, by the list itself, wether or not a particular limit is hard or soft. Only by asking a support contact at Amazon will, hopefully, give you the answer.  The hard lesson we’ve learnt, regarding limits in AWS, is that you will run in to limits and it will be the ones you least expect. So take a good look at the limits and have a plan from the beginning.
 This is a blog series about new features in Docker that I learned from visiting the DockerCon EU conference in Copenhagen, 16-19 October 2017, and in my daily work.  -[readmore]-  In this blog series, I will focus on news in the Docker Engine and surrounding tools, i.e. not go into details on the inner workings of container orchestrators, such as [Docker in Swarm mode]  However, I will use container orchestrators to demonstrate other features in Docker. For example, demonstrate how we can use [Docker in Docker] to quickly and easily setup a local container cluster on our development machines without blowing away the memory...  1. [Setting up a Docker Swarm cluster using Docker in Docker]  1. [Setting up a Kubernetes cluster using Docker in Docker]  1. [Create multi-platform Docker images]  If you want to learn about how to develop microservices in Java or Go that can be deployed as Docker containers, take a look at the [blog series - Building Microservices].
 In my current project we are using the [JBoss Application Server]. Since the project in based on EJB3 and is rather extensive we have been experiencing problems with long "deployment to test"-cycles during development. We write lots of unit tests and use a test driven approach, but we also need to do lots of integration testing in the target environment since the application is heavily AJAX based. Sitting and waiting for application deployment and application server restarts is really not only wasting valuable time, but also interruptive for the creative flow that you build up during software development. But worst of all, it takes the joy out of programming.  What we have done to minimize this problem is to introduce [JavaRebel] without a restart or application redeploy.  JavaRebel is enabled using a [Maven plugin] that places a rebel.xml file into all jar and war files in our application. The rebel.xml file specifies where JavaRebel should go and look for the actual class files. In our case it is in the target directories that Eclipse uses as it's output folders. The second thing that is required is to enable a JavaRebel agent when starting the application server JVM. When the JVM is started the JavaRebel agent will start monitoring changes to classes and reload them when they are needed. It will even through plugins reload and reconfigure Spring application contexts when the xml configuration files are edited.  We use [Maven] as our build system, so the development cycle used to look something like this:  1. Build with maven 2. Deploy to application server 3.  4. Test 5. Edit - and back to 1...  Now it looks like this:  1. Build ONCE with Maven 2. Deploy ONCE to application server 3. Edit/Compile/Test  The [FileSync plugin] is used to keep files in the exploded war archives in synch with the files in the Eclipse source folders. This enables editing in Eclipse and testing in the browser of JSP and Javascript files just by refreshing.  Keep in mind that JavaRebel does not handle all kind of file changes. If you change something that has to do with application configuration, e.g. annotations it is not picked up, but that would require a hook into the application server. You do not have to rebuild from Maven though. The only thing needed is a restart of the application server.  I definitely think that JavaRebel is a valuable, easy to setup tool that bring joy back to programming!
[Transactions] play an important role in most software systems, as well as in many everyday situations. In recent days however, and especially in the context of highly distributed, internet scale solutions, transactions have gained quite a bad reputation . So what's the deal? -[readmore]-  [comment]: #  [TCC Pattern]: https://www.atomikos.com/Blog/TransactionsForTheRESTOfUs [ACID]: https://en.wikipedia.org/wiki/ACID [BASE]: https://www.dataversity.net/acid-vs-base-the-shifting-ph-of-database-transaction-processing/ [CAP Theorem]: https://en.wikipedia.org/wiki/CAP_theorem [Atomicity]: https://en.wikipedia.org/wiki/Atomicity_ [Transactions]: https://en.wikipedia.org/wiki/Transaction [Database Transaction]: https://en.wikipedia.org/wiki/Database_transaction [Two Phase Commit]: https://en.wikipedia.org/wiki/Two-phase_commit_protocol [XA]: https://en.wikipedia.org/wiki/X/Open_XA [Bounded Context]: https://martinfowler.com/bliki/BoundedContext.html [Compensating Transaction]: https://en.wikipedia.org/wiki/Compensating_transaction  [comment]: #  [Transaction-Sign]: https://www.picpedia.org/highway-signs/images/transaction.jpg [2pc]: /assets/blogg/transactions-revisited/2-phase-commit.png   There is clearly an ambiguity in place here, which may be fooling us. "Transaction" as in [Database Transaction] seems tho have a highly specialized meaning, which contains elements that may not be so important from the business domain perspective. Let's try to clarify things:  ![Transaction-Sign][Transaction-Sign] *Image from www.picpedia.org/highway-signs/images/transaction.jpg, courtacy of Nick Youngson, www.nyphotographic.com*   Within the area of Databases, the notion of a Transaction has traditionally been defined in terms of a set of properties intended to guarantee validity even in the event of errors, refered to as the *ACID* properties:  * **Atomicity** guarantees that all operations within a "unit of work" succeeds completely, or fails completely. * **Consistency** ensures that a transaction can only bring the database from one valid state to another, maintaining all invariants at all times. * **Isolation** determines how the changes done in one transaction is visible to other concurrent transations. * **Durability** guarantees that a transaction which has committed will survive permanently.  Within a single database instance, the ACID properties provides no challenge for most databases. Such transactions are often called *local* transactions. But when a single database isn't sufficient , things get trickier. Such transactions are often called *global* transactions. We now need a more sophisticated mechanism to synchronize the transaction between the multiple resources involved, a mechanism which is usually referred to as [Two Phase Commit] and standardised by the [XA] protocol. During the early JavaEE days, support for global transactions was a key selling point for high-end application servers, databases and message queue providers.  ![Two Phase Commit][2pc]  What makes things tricky is the asynchronous and unreliable nature of the networks that plague all distributed solutions. As noted by Eric Brewer in 1999, it is theoretically impossible to achieve both high availability, consistence and data partitioning as the same time. His [CAP Theorem] states that Web Services cannot ensure all three of the following properties at once:  * **Consistency**: The client perceives that a set of operations has occurred all at once. * **Availability**: Every operation must terminate in an intended response. * **Partition tolerance**: Operations will complete, even if individual components are unavailable.  Since we started out with a need for data partitioning, we are left with two options to choose between: consistency **or** availability. In the Internet era, not many people are willing to sacrifice availability. Hence we give up consistency.   So we give up the Consistency guarantee, and must accept the fact that data will inevitably be inconsistent at times . In the best of worlds, the inconsistencies are short-lived, and the data will self-heal over time and become consistent again. This alternative to ACID is sometimes referered to as [BASE]:  * **B**asically **A**vailable * **S**oft state * **E**ventually consistent   So we are forced to give up ACID in order to have high availabilty. Hence transactions suck, and should be avoided, right?  Well, not really. The *Consistency* and *Isolation* aspects of ACID came from the specialized Database Transaction notion. But the most important aspect of a transaction in most Business scenarios as well as in everyday life is *Atomicity*. If two or more operations or steps in a business process forms a logical atomic *Unit of Work*, we must make sure that all those operations are successfully applied, or else that none of them are. If we cannot guarantee this atomicity, there is no hope for any eventual consistency either: If some of the operations are applied and some aren't, the resulting inconsistency will remain forever .   So it seems we still need to care about the transactional needs and boundaries of our business domain. Ideally, the transactional boundaries are kept within a [Bounded Context] of the domain . If so, a wise choice of database technology might be sufficient to fulfil the needs for atomicity. If however the logical Unit of Work spans multiple Microservices, Database Transactions will not be of much help. In such cases, we need dedicated mechanisms or patterns to achieve the transactional needs of the domain. In a forthcoming blog, I'll discuss two such patterns; Compensating Transactions and Reservations. Stay tuned!
 In this article, we describe an open source tool that makes it possible to visualize access control lists in Kafka to help you get an overview of how access in a Kafka cluster is configured.  Using access control lists  so that we can work with Kafka in the same declarative manner that we are used to with Kubernetes.  ![kafka-acl-viewer example]  In order to get started you will need a Kubernetes cluster to run Strimzi and kafka-acl-viewer. During development I usually use [Minikube] but any Kubernetes cluster will do just fine. One thing to note if you do go with Minikube is that Strimzi requires a bit more RAM in your cluster than the default 2GB. To avoid issues make sure you configure Minikube with at least 4GB of RAM when you start your cluster,  When your cluster is up and running go ahead and create yourself a namespace to run things in, The rest of the post and its examples will assume you are using a namespace called .  Now you're ready to apply the Strimzi installation from Github, note the piping through  to create everything in the correct namespace you created above, This will create the Strimzi deployments in charge of managing your Kafka installation, Cluster Roles, Cluster Role Bindings, and the CRDs  you use to configure and manage your cluster.  The next step is to actually create our Kafka cluster, this is done through one of these CRDs, the Kafka custom resource. In order to enable the use of ACLs we need some kind of authentication so that the clients have an identity to tie the access to. Luckily Strimzi makes it really easy to set up TLS authentication. Strimzi will automatically issue certificates for your clients and store them as Kubernetes Secrets for easy access in your application deployments. More on that shortly, first, let us bring up the Kafka cluster.  The following [Kafka] resource will create a persistent Kafka cluster with one node and TLS authentication enabled.  When you apply the Kafka resource the Strimzi Cluster Operator will start up a Kafka cluster according to the configuration. This will take a few minutes depending on how fast your cluster is. The following command can be used to wait for everything to be ready,  Your Kafka cluster should now be up and running!  On to the fun part, creating topics and ACLs! Normally it is quite the headache with longwinded kafka-cli commands to manage your Kafka topics and ACLs. Luckily Strimzi makes this into a much more user-friendly experience by leveraging Kubernetes CRDs, just like the Kafka resource we used in the previous section to create the cluster.  Here is an example of a [KafkaTopic] resource describing the configuration for a topic in the cluster. Applying this in your Kubernetes cluster will cause the Strimzi Topic Operator to pick it up and create the topic in your Kafka cluster. The topic will be given the same name as the Kubernetes resource. The  label specifies which Kafka cluster it should be created in, if you have more than one Kafka cluster in the namespace. The  section defines topic specific configuration such as the number of partitions and how it should be replicated. Since we only run a single node in the Kafka cluster we are limited to one replica.  Kafka users and their access is described in a similar manner with a [KafkaUser] resource, here is an example of such a resource, This resource will be picked up by the Strimzi User Operator which will issue a client certificate signed by the certificate authority trusted by the Kafka cluster nodes. The certificate along with the private key will be stored in a Kubernetes Secret with the same name as the user, the secret also contains the public key of the certificate authority issuing the certificates for the cluster nodes. This allows a client to connect and authenticate with the Kafka cluster using mutual TLS. The client is then identified by its certificate and the cluster can authorise it to access resources in the cluster according to the ACLs.  The ACLs listed in the  section are applied on the Kafka cluster by the User Operator as well. This specific example gives the user access to perform API calls grouped under the Write operation on the *shipments* topic. More information about the specifics of the Kafka access model can be found here, [Authorization using ACLs — Confluent Platform].  These Kubernetes resources already give a nice and searchable definition of your cluster access model but it is somewhat difficult to get an overview. Let's fix that!  kafka-acl-viewer is a small open-source application written in [Go].  Before we can deploy the application we need to set up a KafkaUser with the appropriate access to the Kafka cluster. The kafka-acl-viewer application will contact the cluster directly to list active ACLs and Topics in the cluster. The *Describe* operation on the *cluster* resource is needed to list the ACLs. The second section allows the application to do *Describe* on all topics in the cluster to read metadata and offsets for topics among other things. It does not give the application access to read the data on the topic, that would require the *Read* operation. You can refer to [Authorization using ACLs — Confluent Platform] in order to see exactly which API calls the different operations allow.  Once the ACL is applied kafka-acl-viewer can be deployed, this is a fairly standard Kubernetes deployment, As you can see the certificates and the private key are regular Kubernetes secrets which we mount as files in our pod where the application can access them.  When the application is up and running the easiest way to access it is to use `kubectl port-forward -n kafka deploy/kafka-acl-viewer 8080`. If you are going to use it on a more permanent basis you probably want to set up some kind of ingress. If you now open [localhost:8080] you should see a view of the current accesses in the cluster. The blue boxes are Kafka resources such as topics and the cluster itself and the green boxes are users. The arrows between them represent different types of operations, select one of the resources or users to see what type of operations are connected to it.  ![kafka-acl-viewer example]  Try creating some more KafkaUsers and KafkaTopics and watch the graph expand as you refresh the page. Or you can import the the example setup I use for testing which is available in the [kafka-acl-viewer repo].  Right now the main problem with the tool is when you have a big cluster with a lot of ACLs in it, the view becomes difficult to overview which defeats the point. I have begun trying out ways to filter the graph but I'm not quite happy with the results yet.  Feel free to try kafka-acl-viewer in your Kafka cluster, feedback and pull requests are always welcome!
 Continuous Integration servers have been around for quite a number of years. Mostly out of slentrian, I have stuck to CruiseControl since the alternatives  just haven't been that much better to motivate me to switch.  I just attended Kuhsuke Kawaguchi's [Hudson], it was just another CI server with a nice web GUI. The presentation today showed a completely different creature. Build distribution and scalability has obviously been the focus in the Hudson team, and the latest Hudson version comes with an extremely ambitious Clustering mechanism:  - Several nodes collaborate in a Master-Slave setup, with support for at least up to 100 slaves. - Master and slaves may run on heterogeneous hardware, with support for Solaris, most Linux dialects as well as windows. - Slaves can be automatically configured from the Master: when a new Slave node is added, it can have a specified version of the JVM, Ant and Maven libraries downloaded and installed automatically. Using the Hudson PXE protocol, a new slave node can even boot over the network, have the operating system installed from the Master before continuing the bootstrap! - Slave node system resources  can be automatically monitored by the Master, and slave nodes are automatically put offline if they degenerate - The cluster utilization itself is constantly monitored: How many nodes are busy? How long is the job queue? Using the Hudson [EC2] plugin, new Slave nodes can even be allocated dynamically in the Cloud, on demand! - You can even run other clustered applications like [Hadoop] on a Hudson cluster!  Wow! Not what I was looking for, but really cool stuff.
 This blog is based on two questions  and an example of how they can be solved using Mule ESB and the upcoming **nio-http-transport**.   Why does it have to be so hard to stream or push updates from a server to HTML based applications ?  Why does my ESB server go on its knees by having a large number of outstanding HTTP requests that it routed to backend systems, waiting for responses from slow backend system ?  The problem in question #1 comes from the communication protocol used between HTML clients and servers, HTTP. The HTTP protocol is only half-duplex so there is no natural way for a server to initiate the communication to push data to a client. Therefore all sorts of workarounds have been applied over the years including polling, long polling, piggybacking, Comet and Reverse AJAX  to say 10 000 mobile HTML clients with meaning _Used NNN just clicked on key ‘A’ on his keyboard_ is with this type of solutions challenging to say the least. This is however about to change…  The problem in question #2 comes from the historical way of handling synchronous communication, e.g. HTTP, in servers where each request is handled by a separate thread. Given an ESB server that mediates synchronous requests . This is also about to change…  The solution to problem #1 is called **WebSockets**, described as _Web Sockets are "TCP for the Web," a next-generation bidirectional communication technology for web applications_ in the initial publication of WebSockets, done by [Google back in December 2009] as good introductory material.  The solution to problem #2 is called [non-blocking I/O] has been lacking support for non-blocking I/O for HTTP traffic. This is again about to change…   On the QCon conference in San Francisco 2012 Ross Mason, founder and CTO of MuleSoft, did a presentation on the subject [Going real-time: How to build a streaming API]. The purpose of this transport is rather obvious, adding support for non-blocking I/O based on the Java SE NIO API. Looking into the details reveals that the new transport is based on Netty and also have support for WebSockets!  Based on a modified version of the code base in Ross demo I have setup a test case where a single Mule ESB server streams , receives messages and collects statistics of the received messages to verify that the test actually works.  I also wrote a HTML-WebSocket-client that makes the same as the load-test program but for only one connection at the time to verify that the test also works with HTML based applications.  Mule create one message per second and the payload pushed to the WebSocket clients is based on JSON and typically looks like:   **Where:**  *  the id of the payload-message *  when the message was created in the server *  the number of connected WebSockets clients in Mule at the time the message was created *  the id on the topic for this message. *  some varying data  The output from the HTML-client looks like:  ![]  From the screenshot above you can see an initial phase  where all WebSocket clients perform their connections to Mule and after that a steady phase where Mule stream 10 000 messages per second to the WebSocket clients.  Even though each message is very small and the overhead in the WebSocket protocol is minimal 10 000 msg/sec still creates a substantial network traffic, a bit over 2 MB/s in my case:   ![]  Resource usage by Mule ESB  was monitored using JConsole and the result from running this test looks like:  ![]  As can be seen in the screenshot the initial connect phase caused a bit higher usage of the cpu but after that initial phase both cpu usage and memory consumption is stabilized on a very moderate level.  But of most importance in this screenshot is the thread usage. After an initial peek of 40 threads the usage goes down to 31 threads in the steady state phase!  **31 threads serving  10 000 concurrent real time WebSocket clients!!!**  **C10k problem SOLVED!**  **Note #1:** The cpu usage is of course to a large extent depending on the number of connected clients and the frequency of the messages pushed to the clients.  **Note #2:** To be able to handle 10 000 WebSocket clients in a single Mule instance the maximal number of open files had to be increased significantly in the operating system .   I plan to publish the source code for these tests in the near future so that you can repeat them your self, but the code is currently a bit too messy in some parts to be published. But let's take a look at some of the most important code constructs:    It looks very similar to a regular blocking HTTP endpoint to me!  But if you look into the XSD namespace declaration of the namespace "http:" you will see the difference:   It is for sure the _http-nio-transport_ that is used and not the old-school _http-transport_!  Adding WebSocket semantics is done in the source code above by the declaration:   That is what I call a powerful abstraction!!! Read the WebSocket specs and you will see what you get by this one-line declaration   From an architectural point of view I would prefer to use a BAM/CEP engine, such as Esper or Drools, with the responsibility to produce the messages to be pushed to the WebSocket clients. The BAM/CEP engine would typically use wire tapping of the messages mediated by Mule ESB to detect both events and non-events of interest as a base for the messages pushed to the WebSocket clients.  For the scope of this test a simpler solution is used with a timer based creation of the messages:   Streaming messages to WebSocket clients is done with the one-liner:   Isn’t that, as well, a very nice level of abstraction?  Look out for a follow up blog on this blog where I’ll make the source code available for you to try it out on your own!
In this blog post I will describe how to create a Docker image that works on different hardware architectures and operating systems. I will create a Docker image based on a service written in Go and package it for use in both Linux and Windows on 64 bit Intel based hardware.  -[readmore]-  This blog post is part of the blog series - [Trying out new features in Docker].   Nowadays, Docker runs on different operating systems .  A drawback with this multi-platform support is that one Docker image has to be built for each specific target platform, i.e. a specific operating system and hardware architecture. So, if you want to be able to run your Docker container on both Linux and Windows using Intel 64-bit hardware you must create two Docker images, one for Linux and one for Windows. You also have to create each Docker image using a Docker engine running on the specific target platform.  When using the Docker images, you also have to specify what version you want to use, e.g. the Linux or Windows version. This makes it inconvenient to use together with tools like Docker Compose or a container orchestrator, e.g. Kubernetes or Docker in Swarm mode, since the configuration files  becomes platform specific.  Work is, however, ongoing to support building multi-platform Docker images that are composed by platform specific Docker images. A new Docker tool, `docker manifest`, is under development. For the time being, a [standalone manifest tool]  In September 2017, Docker announced that their official Docker images were updated to make them multi-platform aware, see the blog post [Docker official images are now multi-platform].  To inspect multi-platform support in a Docker image a containerized tool, , can be used. E.g. run the following command to inspect multi-platform support in the official Docker image for Go:  docker run --rm mplatform/mquery golang  Expected result:  Image: golang * Manifest List: Yes * Supported platforms: - linux/amd64 - linux/arm/v7 - linux/arm64/v8 - linux/386 - linux/ppc64le - linux/s390x - windows/amd64:10.0.14393.1884 - windows/amd64:10.0.16299.64  For more background information, see the DockerCon EU 2017 video: [Docker Multi-arch All the Things].   To create a multi-platform Docker image we need to:  1. Create deployment artifacts  for each target platform 2. Create a Dockerfile per target platform 3. Build a Docker image for each target platform 4. Push the target platform specific Docker image to a Docker registry, e.g. DockerHub 5. Create a multi-platform Docker image in the Docker registry based on the individual target platform Docker images  This process is summarized in the following picture:  ![overview]   I have developed a simple service, written in Go, that creates some random quotes about successful programming languages. See [go-quotes].  I want to run my service both as a Linux and a Windows container .  Implementation notes:  1. Go supports cross compilation, so I can create the executable files on the same developer machine.  2. To be able to create the Linux and Windows based Docker images I need access to Docker engines that runs on Linux and Windows.  3. To minimize the scope of the blog post, I don't want to involve the use of a CI environment or virtual machines for building my Docker images.  4. "Docker for Mac" only supports Linux containers, while "Docker for Windows" support both Linux and Windows containers.  5. So, for the scope of this blog post, I will use Windows instead of macOS as my development environment.  > **Note:** Currently you can only run either Linux or Windows containers at the time on a Windows PC but soon we will be able to run Linux and Windows containers concurrently on a Windows PC, see [Docker for Windows 17.11 with Windows 10 Fall Creators Update]: > > "*it will soon be possible to run Windows and Linux Docker containers side-by-side*"   I'm using:  1. Windows 10 Pro, Windows 10 Fall Creators Update  2. Docker for Windows v17.09.1-ce-win42 3. Go v1.9.2   Below follow instructions for how to build the platform specific Docker images and how to assemble a composed multi-platform Docker image.  Use a "*Windows PowerShell*" terminal window to execute the commands below.   git clone https://github.com/callistaenterprise/cadec-2017-service-discovery cd cadec-2017-service-discovery\go-quotes\   Compile the Go source code to an executable for Windows:  set GOOS=windows go build -o quotes-windows-amd64.exe  Try out the Windows executable without using Docker:  ./quotes-windows-amd64.exe  It should startup and say something like:  Starting ML Go version of quote-service on port 8080 2017/12/19 16:23:17 Starting ML HTTP service at 8080  Open another "*Windows PowerShell*" terminal window and try out the service using :  curl http://localhost:8080/api/quote -UseBasicParsing  Expected response:  StatusCode : 200 StatusDescription : OK Content : {"hardwareArchitecture":"amd64","operatingSystem":"windows","ipAddress":"4d131b511ab9/fe80::9846:5b e3:c0bb:2d91%Ethernet172.24.224.172","quote":"In Go, the code does exactly what it says on the page ."...  > **Note**: Pay special attention to the value of the fields  and  in the response . They will be used later on when we want to verify that we are communicating with a container on the expected platform.  Stop the program with .  Ensure that your "Docker for Windows" runs Windows containers. enter the command:  docker info  Look for the field , it should say .  If not, switch to Windows containers using the Docker menu:  ![windows]  The Dockerfile for building the Windows Docker image, , looks like:  FROM microsoft/nanoserver EXPOSE 8080  ADD quotes-windows-amd64.exe /  ENTRYPOINT ["./quotes-windows-amd64.exe"]  Build the Docker image and push it to DockerHub:  docker build -f Dockerfile-windows-amd64 -t magnuslarsson/quotes:24-go-windows-amd64 . docker push magnuslarsson/quotes:24-go-windows-amd64  > **Note #1:** Obviously you can't use my username, , when pushing images to DockerHub, you have to replace it with your own :-) > > **Note #2:** You might need to login to DockerHub first, using `docker login`.  You can start the Windows specific Docker image with:  docker run -d -p 8080:8080 --name quotes magnuslarsson/quotes:24-go-windows-amd64  The current version of "Docker for Windows" has a limitation that prevents access ports published by containers using , e.g. `curl http://localhost:8080/api/quote` does not work. For details, see [https://blog.sixeyed.com/published-ports-on-windows-containers-dont-do-loopback/].  Instead, we can use the PC's IP address. You can use  to get the IP address.  E.g.:  curl http://192.168.1.224:8080/api/quote -UseBasicParsing  Expect a similar response as from the  command above. Verify that  field has the value !  Stop the Windows container with the command:  docker rm -f quotes   Compile the Go source to an executable for Linux:  set GOOS=linux go build -o quotes-linux-amd64  Switch to Linux containers using the Docker menu:  ![linux]  The Dockerfile for building the Linux Docker image, , looks like:  FROM scratch EXPOSE 8080  ADD quotes-linux-amd64 /  ENTRYPOINT ["./quotes-linux-amd64"]  Build the Docker image and push it to DockerHub:  docker build -f Dockerfile-linux-amd64 -t magnuslarsson/quotes:24-go-linux-amd64 . docker push magnuslarsson/quotes:24-go-linux-amd64  You can start the Linux specific Docker image with:  docker run -d -p 8080:8080 --name quotes magnuslarsson/quotes:24-go-linux-amd64  Now, try out the service using curl:  curl http://localhost:8080/api/quote -UseBasicParsing  Expected response:  StatusCode : 200 StatusDescription : OK Content : {"hardwareArchitecture":"amd64","operatingSystem":"linux","ipAddress":"0c4e0824f479/172.17.0.2","qu ote":"I like a lot of the design decisions they made in the [Go] language. Basically, I like all of t...  Verify that  field now has the value !  Stop the Linux container with the command:  docker rm -f quotes   Now, it's time to combine the two platform specific Docker images into one common Docker image.  We will us the standalone tool . Executables can be downloaded from: [https://github.com/estesp/manifest-tool/releases].  I used version v0.7.0 of the tool compiled for macOS .  The manifest file, , looks like:  image: magnuslarsson/quotes:24-go manifests: - image: magnuslarsson/quotes:24-go-linux-amd64 platform: architecture: amd64 os: linux - image: magnuslarsson/quotes:24-go-windows-amd64 platform: architecture: amd64 os: windows  The multi-platform Docker image is create with the command:  ./manifest-tool-darwin-amd64 --username=magnuslarsson --password=xxx push from-spec manifest-quotes-multi-platform.yml > **Note:**  requires macOS, you can however try out the Windows version  on your own.  Verify that we now have a Docker image for our Go service that supports both Linux and Windows:  docker run --rm mplatform/mquery magnuslarsson/quotes:24-go  Expected response:  Image: magnuslarsson/quotes:24-go * Manifest List: Yes * Supported platforms: - linux/amd64 - windows/amd64:10.0.14393.1944  You can also take a look into DockerHub to see the resulting three Docker images, e.g. in my case: [https://hub.docker.com/r/magnuslarsson/quotes/tags/]  Expected result:  ![dockerhub]   Now we should be able to run our Go service in both Windows and Linux containers using one and the same Docker image: !  Since we currently have "Docker for Windows" configured to run Linux containers, let's start with trying it out on Linux:  docker run -d -p 8080:8080 --name quotes magnuslarsson/quotes:24-go  Test it using curl:  curl http://localhost:8080/api/quote -UseBasicParsing  Verify that  field in the response has the value !  Kill the Linux container:  docker rm -f quotes  Switch "Docker for Windows" to use Windows containers:  ![windows]  Start a Windows container:  docker run -d -p 8080:8080 --name quotes magnuslarsson/quotes:24-go  > **Note:** We are using exaclty the same command as when starting a Linux container!  Test it using curl:  curl http://192.168.1.224:8080/api/quote -UseBasicParsing  > **Note:** Remember to replace my IP Address with yours!  Verify that the  field in the response now has the value !  Wrap up with killing the Windows container:  docker rm -f quotes   For more blog posts on new features in Docker, see the blog series - [Trying out new features in Docker].
 In this blog we will show you how to develop non-blocking REST services using [Spring MVC]. We will also demonstrate the vast difference in scalability that non-blocking services provide compared to traditional blocking services.  We will use [Spring Boot] to load test the REST services.  -[readmore]-  But first some background on history and theory on the subject…  With an ever increasing number of connected devices, e.g. mobile devices and [Internet of Things], the requirement of handling large number of concurrent requests in the application servers becomes more critical in many projects.  The key technology for an application server to meet this requirement is the capability to handle requests in a non-blocking way, i.e. without allocating a thread to each request. This is equally important for both incoming and outgoing requests.  Non-blocking I/O has been supported by the Java platform since 2002 with Java SE v1.4 and its API’s called [New I/O , evolved to fill the gaps and today they provide a solid ground for non-blocking I/O, but with product specific API’s.  In December 2009 the [Servlet 3.0] or any of the commercial alternatives.  In December 2013 [Spring 4.0].  Before we jump into the code let’s look into, from an architectural perspective, how Spring MVC handles blocking and non-blocking REST services.  Developing a traditional blocking style REST service with Spring MVC is very straightforward:  Blocking REST service with Spring MVC   The code is compact and simple to understand since the annotations handles all the REST, JSON and XML machinery. The problem, from a scalability perspective, is that the request thread is locked during the processing of this method. If the method needs to make a long running call to an external resource, such as another REST or SOAP service or a database, the request thread will be blocked during the wait for the external resource to respond. The following picture illustrates this:  ![]  To avoid the blocking of the request thread the programming model is changed to a callback model. The REST service doesn’t return the actual result to the Servlet container but instead an object, called a , that will receive the result at some time in the future. The result will be filled in by some other thread, typically using a callback-object. Spring MVC will hand over the result to the Servlet container that sends it back to the client. In the REST service we have to initiate this processing before we return the  object to the Servlet container like:  Non-blocking REST service with Spring MVC   The callback object will be called some time in the future and it will then set the response in the  object to complete the processing of the request:  Callback class for on-blocking REST service with Spring MVC   …under the hood this maps up to the Servlet 3.0 specification with its corresponding support for Asynchronous Servlets.  Now a long running call to an external resource can take place without blocking the request thread.  Typically there are two scenarios to consider:  This case it’ straight forward, we just have to ensure that the callback from the external resource API fills in the  object on its completion. This will, as described above, notify the Servlet container of the completion of the request.  ![]  For blocking API’s we need to allocate a thread for the blocking processing, typically coming from a Thread pool allocated for the specific external resource.  ![]  **Note:** If we only perform calls to one and the same external resource using a blocking resource API in our REST Services we have actually more or less moved the problem one step back and not solved much of the scalability problem. In most cases however there is a mix of processing in the various REST services in a web server. Some don’t need access to resources at all, other can access resources using a non blocking API and some need to access resources using a blocking API. So overall the scalability will improve significantly if the blocking of threads can be moved back to the specific resource API’s that require blocking execution.  With this in mind let’s look at some real code!  The source code in this blog is based on the [blog] for background information.  If you want to check out the source code and test it on your own you need to have Java SE 7 and Git installed. Then perform:   This should result in a tree structure like:   In the [Spring Boot blog]`.  To simulate that our non-blocking REST service waits on an external resource we can’t use  since it will block our request thread. Instead we use the Java SE Scheduler that we ask to invoke our callback object when the simulated waiting of the external resource is over.  First we create our  that we use to initiate the callback object, task. Finally we schedule the task object for the calculated time-period:  Non-blocking REST Service   When the time period has elapsed the -method in our task object will be invoked by the Java SE Scheduler and the task object will create a simulated answer from the external resource and set it on the  object. This will cause the Servlet container to wake up and finalize the corresponding request by sending back the response we set on the  object:  Callback for the non-blocking REST Service   Time for a test run before we start the heavy lifting!  A test run Start the web app in an embedded Tomcat instance with the command:   Note: We are using Gradle as out build tool. If you want to know more about it, please read our [blog about Gradle].  Now, try out the blocking REST service with a command like:   Here we ask the blocking service to process our request and respond in between 1 and 2 secs. The response reports that the internal processing actually took 1374 ms.  Ok, lets try the non-blocking version as well:   Not that exiting, it simply works in the same way   Great, single requests works both for the blocking and the non-block service. Time to put the services under some pressure!  We will use Gatling as our load test tool. If you want to know more about Gatling and how we performed the tests, please read our [Gatling blog].  As described in the blog, the blocking REST service is not very sustainable to increasing numbers of concurrent users. Even if we increase its request thread pool to very high values it hits the roof after a while and gets unresponsive. A load test for a blocking service typically looks like :  ![]  As you can see from the figure above we ramp up the number of concurrent users to 5000  and the number of successful request falls down to below 50 reqs/sec.  If we look at the actual response times it actually looks even worse:  ![]  Initially the response times are as expected, between 1 – 2 secs but very soon they starts to raise  and after a while most requests turns into red.  You can find a full Gatling report in the folder  together with a screen shot from JConsole demonstrating the constant use of 500 threads during the load test.  Not so impressive…  Over to the non-blocking REST service!  To make it a bit more challenging we lowered the maximum threads in the request pool to 50, a tenth of what the blocking REST service was allowed to consume.  The following picture says it all:  ![]  Not a glitch during the whole load test. After the ramp up period the non-blocking REST service handles some 1400 requests/sec without any problems!  If we look at the response times it gets even more impressive:  ![]  The response times is, except for a few small exceptions, within the configures response time 1-2 secs!  If you look into the full test report in  you will find that the 99th percentile is at 1990 ms.  So even if it only got a tenth of the resources  it outperformed the blocking version when the load went up, exactly according to the theory!  We have seen how elegantly and efficiently Spring MVC and Spring Boot can help us to develop highly scalable non-blocking REST services that can be deployed on any Servlet 3.0 compliant web server!  In the next blog we will focus on how external resources are used from non-blocking REST services, both using blocking API’s and non-blocking API’s. If external resources are used incorrectly we can easily loose the scalability capabilities that we just achieved from using Spring MVC and its support for non-blocking I/O. So this is a very important aspect. Stay tuned…
 According to the Mozilla Blog, Firefox is about to be released in an enterprise version, with releases once every year instead of once every 6:th week.  The new version of the Firefox is called Firefox Extended Support Release or simply Firefox ESR. The intention is to provide security updates continously but let the web and add-ons platform be stable throughout the 1 year release.  -[readmore]-  I have experience from developing and maintaining both web frameworks and web applications at big companies and have noticed that Firefox's recent 6 weeks release cycle have created problems for developers keeping up with certifying the quality of their software with the latest browser release. Firefox ESR might be a welcome change for framework and application developer's in enterprises since it, in my humble opinion, might ease the the task of choosing Firefox as a tactical appointed enterprise platform for internal web-applications.  For more info, see the post at the Mozilla blog:  [http://blog.mozilla.com/blog/2012/01/10/delivering-a-mozilla-firefox-extended-support-release/]
  CDI is an abbreviation of "Contexts and Dependency Injection for the Java EE platform". First of all, I'd like to stress that CDI is not only for Java EE environments. It is equally applicable to Java SE applications, unit tests and other out-of-container environments.  The specification  defines its declared capabilities as follows:  > This specification defines a powerful set of complementary services that help improve the structure of application code. > > - A well-defined lifecycle for state-ful objects bound to lifecycle contexts, where the set of contexts is extensible > - A sophisticated, type safe dependency injection mechanism, including the ability to select dependencies at either development or deployment time, without verbose configuration > - Support for Java EE modularity and the Java EE component architecture---the modular structure of a Java EE application is taken into account when resolving dependencies between Java EE components > - Integration with the Unified Expression Language , allowing any contextual object to be used directly within a JSF or JSP page > - The ability to decorate injected objects > - The ability to associate interceptors to objects via type safe interceptor bindings > - An event notification model > - A web conversation context in addition to the three standard web contexts defined by the Java Servlets specification > - An SPI allowing portable extensions to integrate cleanly with the contain  I will walk you through all the above with samples on the upcoming [Cadec 2010]. In this blog entry, I'd like to give you a sense of the power of type-safe dependency injection in CDI.   If you are used to [Spring].  Type-safe dependency injection is about qualifying the requested bean among several possible ones, using annotations. Let's look at an example: injection of Spring JMS templates. Some applications use JMS extensively. There are many JMS service endpoints to interact with and there a also JMS endpoints for dealing with QoS, like destinations dedicated for logging and destinations dedicated for inbound business messages that could not be processed. The end result is the need for a potentially large number of pre-defined JMSTemplates - one for each destination.  Some destinations may need transitions, others don't. Oneway services will require XA support. RequestResponse services, Log services and error services will not.  Pre-CDI one would typically use configure each JMSTemplate as a named bean and then reference the name using an annotation    The template XML configuration would then reference an appropriate connection factory configuration .  With CDI, there is no standardized XML / externalized configuration language. The idea is to use annotations and Java code:   In this case, we assume that there is single JmsTemplate defined to be used to send failed inbound messages to an error destination.   simply means  that the field is subject for dependency injection. Autoinjection is default in CDI. There are several options for restricting the injection target among those that match the type of the field, one of being custom qualifiers. A custom qualifier is a custom annotation type that itself is annotated with the  annotation. Here's the source code for the  annotation that allows us to express which  that we expect to be injected, using the semantics of a policy:   When defining the JmsTemplate bean, we will annotate it with the same "policy", which is to say: "This JmsTemplate declaration conforms to the policy of an error destination" .   Since there is no external configuration language for bean instances in CDI, we will have to define the CDI correspondence of a Spring bean factory using Java annotations. In CDI, a bean factory is called a producer. A producer is implemented as a method annotated with @producer, which will return an instance of a JmsTemplate. For this simple sample, it was handy to define a single class that hosts all JmsTemplate producer methods along with declarations of the ConnectionFactories that needs to be injected into et producer method parameters:   We see another example of a producer: a field that is injected as a  and at the same time made available for injection into other beans .  The abstraction of policies have been used once more so that a semantic policy declaration in the form of a custom qualifier annotation could be used to specify the capabilities of the connection factory for each of the templates.  This translates into:  The service bean that processes an inbound JMS message says:  > I need a JmsTemplate that qualifies for sending error message  The producer method for JmsTemplates that claims to support the error handling policy says:  > I need a connection factory that is compatible with my policy for transaction handling, which is XA-enlisting.  The code also shows an example of a JmsTemplate producer that support a log policy. The log policy defines a substantially shorter timeout for invocation of the log service than does the error template producer.  Finally, there are samples of JmsTemplate producers for business service endpoints  qualifier annotation type, that is convenient to use when there is no added value of abstracting the matching of an injection point with a bean.   The  annotation used in the producer samples, is a custom annotation used in conjunction with the mata-programming capabilities of the CDI SPI, extend the annotation-based configuration language for our purpose :   The annotation is processed by a class that utilizes parts of the CDI SPI for meta programming:    The idea of having a 100% java-based dependency injection model, based on annotations require some innovation to take place. CDI is a good example of what it takes. CDI also raises the bar in terms of type-safe dependency injection. There are several interesting innovations to explore in CDI.
 I have used [EasyMock] for Mock Object creation since version 1.0 in 2001. It has never been perfect, but good enough. The need to explicitly work with a separate Control object for every Mock object created was a pain, but that was changed in version 2.0. EasyMock is a decent Mock Object framework.  Still, in lectures and tutorials we do on Mock Object usage, I have always had a bit of a problem explaining how tests using EasyMock fits into the generic _Arrange-Act-Assert_ form of most unit tests. The "recording" part of an EasyMock test tends to fall into both the Arrange and the Assert category: it governs the behavior of the mock object **and** it records an expectation:   So what, one might ask? Isn't that just of interest for a purist? Well, not only. Besides the fact that the assertions on correct usage of the mock object becomes hidden in the Arrange part, the assertions are **always** implicit there, regardless of whether they make sense or not. Over and over again, I find that I only need the Arrange part: program the mock object with how it should behave. The implicit assertions then just gets in the way, often leading to false negatives.  I have learned to live with this, but last week when struggling with some refactoring work in an area with tons of mock-based tests, I had enough. There must be a better way!  Of course there is. Half an hour surfing, and I discovered [Mockito]. It is an EasyMock clone that does exactly what I was looking for: Allowing Mock Object expectations to be specified separately from Mock Object behaviour:   No big deal, one might think. But having worked with it for a couple of days, it sure made my refactorings **a whole lot** easier. It was well worth the effort switching framework.
 This blog series will serve as an introduction on how to build reactive web applications using Spring Boot, Project Reactor and WebFlux. It will be a beginners guide to the reactive world, but the reader is assumed to have previous knowledge of Java and Spring Boot.  Part one provides an overview of the different concepts behind reactive programming and its history. Part two serves as an introduction to Project Reactor with a lot of short code examples. Part three covers WebFlux - Spring's reactive web framework. And finally, part four will be about R2DBC - Reactive Relational Database Connectivity.  -[readmore]-    - [Part 1: An Introduction to Reactive Programming] - [Part 2: Project Reactor] - [Part 3: WebFlux] - Part 4 - R2DBC   Stay tuned for more posts! 
 Mule 3 introduced an [application-level component model] allowing for fine grained deployment of Mule applications. One disadvantage with this model is it that HTTP services packaged in different applications can't share HTTP ports. This leads in many cases to Mule instances that use several HTTP ports. This is not preferable, for example, from a network security point of view since it force firewalls to accept traffic through a number of HTTP ports.  In Mule 3.5.0 this problem can be resolved using [shared resources]. In this blog we will show you how to develop two Mule applications each exposing HTTP services that share a common HTTP port.  -[readmore]-  We will use Anypoint Studio . For the scope of this blog we assume that Mule ESB 3.5.0 CE standalone runtime is installed in the folder .  To be able to use the same HTTP port from the two applications we need a shared HTTP connector. This can be setup by creating a file named  under  with a content like:   Note that we named the HTTP connector , we will refer to the connector in the applications below using this name.  Also note that you can create you own domain for more fine grained control over shared resources, see [shared resources] for more information.  In Anypoint Studio create a new project using `File --> New --> Mule Project`, set the project name to , ensure that the runtime named `Mule server 3.5.0 CE` is selected and hit the  button, e.g.:  ![Create Mule Project]  The project will be created and a graphical flow editor will be opened. Create a simple HTTP service by dragging in a HTTP connector and an Echo component. Set the HTTP port and path to values of your choice,  and  in my case:  ![Create Mule FLow]  You can now try to run the Mule flow inside Anypoint Studio by right-clicking on the flow file ( and select `Run As --> Mule Application`. Then within a Web Browser try out the application. A request sent to  should respond  like:  ![Run Flow inside Studio ]  After trying it out stop the application to allow Mule standalone later on to use the port.  Anypoint Studio does not yet support the concept of shared resources so we have to edit the underlying XML configuration file. Switch to the `Configuration XML` tab in the flow editor and add  to the http-inbound-endpoint, see:  ![Edit Flow XML file ]  *Note:* We can no longer run the flow in Anypoint Studio since it doesn't  understand the concept of shared resources!  Now we can create the deployable file for our application by right-clicking on the project folder  and select `Export... --> Mule --> Anypoint Studio Project to Mule Deployable Archive` and specify a proper name of the zip-file, e.g. .  To create the other application repeat the steps above, but name the application  instead. Ensure that you use the same HTTP port, but another path as in the first application. I used  and  for my second application.  This can be done simply by copying  and  to . If Mule already is running they will be automatically deployed.  We should now be able to start Mule ESB with the shared HTTP connector and the two applications that expose HTTP services using one and the same HTTP port. The startup should result in an output like:   Use curl to try out if the two applications actually can share one and the same HTTP port:   ..and if you look into the log files of each application you will see that that each application got its request as expected:    You have seen how one of the most problematic disadvantages of the deployment model in Mule 3 now is resolved. Using shared resources in Mule 3.5.0 we can enable applications to share one and the same HTTP port. Actually shared resources can be used in other areas such as sharing the same VM-, JMS- and JDBC-endpoints resulting in new interesting opportunities, but we leave that for a later blog to delve into. As of today the development environment, Anypoint Studio, is lagging a bit not supporting the concept of shared resources. Hopefully it won't take long until that issue also is resolved.
 [Event Driven Architectures] in general and [Apache Kafka] specifically have gained lots of attention lately. To realize the full benefits of an Event Driven Architecture, the event delegation mechanism must be inherently asynchronous. There may however be some specific use cases/flows where a Synchronous Request-Reply semantics is needed. This blog post shows how to realize [Request Reply] using Apache Kafka.  -[readmore]-  [comment]: #  [Apache Kafka]: Https://kafka.apache.org/ [Event Driven Architectures]: https://martinfowler.com/articles/201701-event-driven.html [Request Reply]: https://www.enterpriseintegrationpatterns.com/patterns/messaging/RequestReply.html [Return Address]: https://www.enterpriseintegrationpatterns.com/patterns/messaging/ReturnAddress.html [Spring Kafka]: https://spring.io/projects/spring-kafka [github.com/callistaenterprise/blog-synchronous-kafka]: https://github.com/callistaenterprise/blog-synchronous-kafka  Apache Kafka is by design inherently asynchronous. Hence Request-Reply semantics is not natural in Apache Kafka. This challenge is however not new. The [Request Reply] Enterprise Integration Pattern provides a proven mechanism for synchronous message exchange over asynchonous channels:  ![Request Reply]  The [Return Address] pattern complements [Request Reply] with a mechanism for the requestor to specify to which address the reply should be sent:  ![Return Addess]  Recently, [Spring Kafka] 2.1.3 added support for the Request Reply pattern out-of-the box, and version 2.2 polished some of it's rough edges. Let's have a look at how that support works:   The well known **** abstraction forms the basis for the client-side part of the Spring Request-Reply mechanism.   That's fairly straight forward: We setup a ReplyingKafkaTemplate that sends Request messages with String keys, and receives Reply messages with String keys. The ReplyingKafkaTemplate however needs to be backed by a Request ProducerFactory, a ReplyConsumerFactory and a MessageListenerContainer, with corresponding consumer and producer configs. Hence the needed config is rather extensive:   With that in place, using the replyKafkaTemplate to send a synchronous reqeust and get a reply back looks like this:   Lots of boiler plate code and low level api's there as well, and that old  API instead of the modern CompletableFuture. The requestReplyKafkaTemplate takes care of generating and setting a  header, but we have to set the  header on the request explicitly. Note also that this same reply topic was redundantly wired into the replyListenerContainer above. Yuck. Not quite what I expected from a Spring abstraction.   On the server side, a regular KafkaListener listening on the request topic is decorated with an additional  annotation, to provide the reply message. The object returned by the listener method is automatically wrapped into a reply message, the  added, and the reply is posted on the topic specified by the .   Also quite some configuration needed, but configuration of the listener is easier:    It sort of works, as long as we don't use multiple consumer instances. If we have multiple client instances, we must make sure that the reply is sent back to the correct client instance. The Spring Kafka documentation suggests that each consumer may use a unique topic, or that an additional  header value is sent with the request, a four byte field containing a BIG-ENDIAN representation of the partition integer. Using separate topics for different clients is clearly not very flexible, hence we opt for setting the  explicitly. The client will then need to know which partition it is assigned to. The documentation suggests using explicit configuration to select a specific partition. Let's add that to our example:    Not pretty, but it works. The configuration needed is extensive, and the APIs are kind of low level. The need for explicit partition configuration adds complexity if we need to dynamically scale number of clients. Clearly, we could do better.   Let's start with encapsulating the [Return Address] pattern, passing along the reply topic and partition. The Reply topic needs to be wired into the RequestReplyTemplate, and hence shouldn't be present in the API at all. When it comes to the reply partition, let's do it the other way around: Retrieve which partition the reply topic listener has been assigned, and pass that partition along automatically. This eliminates the need for the client to care about these headers.  While doing this, let's also complete the API to resemble the standard KafkaTemplate :   Next step: Let's adapt the ListenableFuture to the more modern CompletableFuture.   Pack that up in a utility library, and we now have an API that is much more in line with the general Convention over Configuration design philosophy of Spring. This is the resulting client code:    To summarize, Spring for Kafka 2.2 provides a fully functional implementation of the Request-Reply pattern over Apache Kafka, but the API still have some rough edges. In this blog post, we have seen that some additional abstractions and API adaptations can give a more consistent, high-level API.  __Caveat 1__: One of the principal benefits of an Event Driven Architecture is the decoupling of event producers and consumers, allowing for much more flexible and evolvable systems. Relying on a synchronous Request-Reply semantics is the exact opposite, where the requestor and replyer are tightly coupled. Hence it should be used only when needed.  __Caveat 2__: If synchronous Request-Reply is required, an HTTP-based protocol is much simpler and more efficient than using an asynchronous channel like Apache Kafka.  Still, there may be scenarios when synchronous Request-Reply over Kafka makes sense. Choose wisely the best tool for the job.  A fully working example can be found at [github.com/callistaenterprise/blog-synchronous-kafka].
 Suppose you have an web application with a service layer implemented by spring beans and want to get a richer user interface ? One way to do it to use Flex.  Parts of the Flex development platform has become Open Source. What you absolutely need is addition to that is the Flex Builder. It enables debugging of the Flex application and without that you are completely lost. The license is 150 Euro so it is no big deal.  So the User Interface is produced using Flex and to take care of the remoting we can use BlazeDS which is Open Source from Adobe. BlazeDS is deployed as an web appication.  My Helloworld sample version 1 uses a simple java class on the server side. Version 2 will use spring.  The class has a method   Topic contains two attributes speaker and name.  In Flex I define a RemoteObject to describe the communication with the server   The method tag tells Flex that whenever the  Method is called the return value will be handled by the  method which is defined like this   It simply saves the result to a variable topics. Finally a DataGrid is defined to display the result   When the page is loaded a call to  is made   and when the reply arrives the grid is populated and looks like this  _Bild saknas_  BlazeDS need to know what the destination="cadec-server" means. The information is supplied in file named    This will create an instance of the CadecService . The classes are put in WEB-INF classes.  HelloWorld sample version 2 uses spring on the server side.  To enable that I followed the instructions outlined by [http://coenraets.org/flex-spring/]  2 classes  and  needs to be present att .  To enable the spring integration these classes needs to be configured in the services-config file   Now the  destination can be changed to use the SpringFactory to get the bean from spring instead of initializing it. Obviously this means that the  class will only be instatiated once.   Now everything works again.  If this wasn't simple enough we just have to wait for the new spring/flex integration project to deliver  [http://www.infoq.com/news/2009/01/spring-adobe-blazeds;jsessionid=8F49E7F3F0F00D6880768A222111D793]
 It's best practice to tag each of your stable releases in your version control system for futurereference. However, it is very rare that this sort of work is free from failure. Like many tedious, error-prone tasks, it is one of those things that could do with a bit of automation.  Luckily, the **Maven Release plugin** can help you automate the whole process of upgrading your POM version number and tagging a release version in your version control system, and all of this with a single maven command. You can even automate the entire release process by running the command in a cronjob or a Continuous Integration system. Amazing!  Here is an extract from a project's top POM file, showing the version number that uniquely identifies this version.   The SNAPSHOT suffix means that each time I deploy this version, a new snapshot will be deployed into my local Maven repository. Anyone who wants to use the latest, bleeding-edge SNAPSHOT version can add a SNAPSHOT dependency in their project. Snapshots, by definition, tend to be fairly unstable beasts.   When the version 1.1 is ready, you need to:  1. update the POM file 2. commit the new POM file to version control 3. tag this version as a release 4. and then move on to work on version 1.2  The Maven Release plugin can automate much of this process. However, before the Maven Release plugin can do its work, you need to make sure you have everything it needs set up in your POM file.  First of all, you need to be working with a SNAPSHOT release. However, when you are ready to release your new version, you should **remove any references to snapshots in your dependencies**. This is because a release needs to be stable, and a build using snapshots is, by definition, not always reproducible. Maven will not accept any snapshot references and force the user to change it during the release process. If you answer is no  to the question of resolving dependencies or not, Maven will fail your build.   The next thing you need is another section in your POM file. Maven will tag your project and commit it to your source control system. So it needs to know the location of your source control system. You generally don’t need to specify credentials as Maven inherits those from the environment. If Maven needs credentials it will prompt you during the release process.   You can find reference documentation for the  tag [here].  Next, you need to configure the Release plugin itself. This mainly involves telling Maven where your release tags go, via the "tagBase" configuration element. If you are using the Subversion trunk/tags/branches convention, Maven will automatically put release tags in the "tags" directory. I prefer to use a a slight variation on the normal convention, and place releases in the "tags/releases" directory:   Now it's time to get down to business, and try out a automated release. The first thing you need to do is to make sure all your latest changes have been committed to your SCM system. If there are any outstanding changes, Maven won't let you do a release. First of all, you need to prepare the release, using the "prepare" goal:   This goal will ask you a series of questions to confirm:  - what version number you want to release - what new snapshot version number you want to use - where you want to place the release tag.  If you have set up your POM file correctly, these will have sensible defaults, and you won't have to do much thinking. If you which you can disable these questions using the "--batch-mode" command line option. But to me it's a good last healthy check walking through these questions.  If you want to know exactly what Maven will do to your POM file and your SCM ahead of time , you can run the operation in the "dry-run" mode, as shown here.   This useful trick simulates the SCM operations by writing them out to the console, and creates three sample POM files that you can consult:  - , which is the pom file that will be committed to Subversion and tagged - , which contains the next snapshot version number  I recommend you to do a dry-run the first time you're doing you use the Maven Release plugin. It is very useful and will learn you about the set up of your own project and sub-projects. When you're satisfied with what Maven will do, you can do it for real:   Things you should know about the prepare goal:  1. The command checks for local modifications. If you have any modified files but not checked in, the command will stop and ask you to first check-in modified files. 2. Your project cannot have any SNAPSHOT dependencies in your project. It's a requirement. 3. You will be asked to specify versions for the release and the next SNAPSHOT . Accept the default values if unsure. 4. Maven will run all your unit tests to make sure they work after changing the project version in . 5. Commit the changes made to the POM file 6. If all tests pass, Maven will tag the source in the source control with the release version you specified in step 3 above. 7. Update the SNAPSHOT version number to a new SNAPSHOT version  8. Maven collects all the information that you provide and write a local file called . This file lets you continue from where you left in case of errors. 9. You can undo everything you’ve done so far with `mvn release:clean` to start all over.  Indeed, the prepare goal does quite a lot. Once you're finished, you have your release version tagged in Subversion and you are working on a new SNAPSHOT version.   So far we have only set everything up in preparation for the release. Nothing have actually been released yet. But dont worry, performing the release is easy. Just use `mvn release:perform` command:   This will effectively do a mvn deploy with the release we have just created. More precisely, it will use the release.properties file generated by the release:prepare goal to do the following:  - Check out the release we just tagged - Build the application  - Deploy the release version to local and remote repositories  However, both of these steps can be made into a single line command and placed, for example, on a Hudson server.   This single line command will tag source control, bump up the current pom.xml, version and push a tagged artifact  to a remote repo.   For example, if your current project version was 1.1.2-SNAPSHOT  1. The release version will be 1.1.2 2. The next development version will be 1.1.3-SNAPSHOT 3. The SCM tagged version will be artifactId-1_1_2  If you want to specify the versions manually, then you use the commands described earlier. A single line release command is good for productivity and great for automation. You can make a build plan on your Continuous Integration server with the above command to automatically tag the current version, bump the SNAPSHOT version and deploy to a repository of your choice.   After versioning your project, Maven can deploy, if configured, your project artifact  to a remote repository of your choice. You can do this by using scp, webdav, sftp etc. I will descripe how to set up your pom.xml for scp and sftp.  Assume you have a Maven repository and you have SSH access to the repository, then you can publish your artifacts using **scp**:   When using **sftp**, the XML configuration looks like this   If using either **scp** or **sftp** for publishing, and having SSH as your underlying transfer mechanism, you could setup SSH without password on the repository machine. If not, Maven will ask you to enter password each time doing these operations.  Alternatively you can specify the remote repository credentials in your  file. On Linux/Mac OS X operating systems you should find the file here:   Open the file and use these setting   As an alternative of deploying your artifacts to a remote Maven repository , you could use the following command:   The command will deploy your artifacts to the directory  on your local machine. The command binds by default to the lifecycle phase: deploy.   Releasing software is difficult. It is usually tedious and error prone, full of manual steps that need to be completed in a particular order. Worse, it happens at the end of a long period of development when all everyone on the team wants to do is get it out there, which often leads to omissions or short cuts. Finally, once a release has been made, it is usually difficult or impossible to correct mistakes other than to make another, new release.  To make the release management process smooth, consistent and free from errors, working with a good tool is vital. Maven provides a release plugin that provides the basic functions of a standard release process. If you have work with Maven before, try it out. In the end, it will save you a lot of valuable time.   - [Official Maven Release Plugin documentation]
In the [last part], we implemented the Shared Database with Discriminator Column pattern usign Hibernate Filters. We observed that it will scale well, but the data isolation guarantee is troublesome due to shortcomings in the Hibernate Filter mechanism.  In this part, we will tweak the solution and redo the critical Filtering part using an advanced database mechanism: Row Level Security.  -[readmore]-  [comment]: #  [Multi Tenancy]: https://whatis.techtarget.com/definition/multi-tenancy [SAAS]: https://en.wikipedia.org/wiki/Software_as_a_service [Spring Framework]: https://spring.io/projects/spring-framework [Spring Boot]: https://spring.io/projects/spring-boot [Spring Data]: https://spring.io/projects/spring-data [Hibernate]: https://hibernate.org/orm/ [JPA]: https://en.wikipedia.org/wiki/Jakarta_Persistence [Cross Cutting Concern]: https://en.wikipedia.org/wiki/Cross-cutting_concern [Aspect]: https://en.wikipedia.org/wiki/Aspect-oriented_software_development [AspectJ]: https://www.eclipse.org/aspectj/ [Row Level Security]: https://www.postgresql.org/docs/9.5/ddl-rowsecurity.html [Github repository]: https://github.com/callistaenterprise/blog-multitenancy [shared_database_postgres_rls branch]: https://github.com/callistaenterprise/blog-multitenancy/tree/shared_database_postgres_rls  [comment]: #  [Lock]: /assets/blogg/multi-tenancy-with-spring-boot/Lock.png [SingleDatabase]: /assets/blogg/multi-tenancy-with-spring-boot/SingleDatabaseMultiTenancy.png  - [Part 1: What is Multi Tenancy] - [Part 2: Outlining an Implementation Strategy for Multi Tenant Data Access] - [Part 3: Implementing the Database per Tenant pattern] - [Part 4: Implementing the Schema per Tenant pattern] - [Part 5: Implementing the Shared database with Discriminator Column pattern using Hibernate Filters] - Part 6: Implementing the Shared database with Discriminator Column pattern using Postgres Row Level Security  - Part 7: Summing up    In order to achieve proper data isolation between different tenants, we need to include an extra  condition on the tenantId for all data access. Doing so in application code can be troublesome and error-prone, as we saw. The burden of proof lies on us that the mechanism is properly implemented and applied. Clearly, it would be better to get that guarantee from the Database instead.  ![Lock][Lock]  Modern databases like Postgres or SQLServer provides a [Row Level Security] mechanism, where access to individual rows can be declaratively and transparently restricted to specific users based on various criteria. It can thus be used to implement the data isolation between tenants.   In Postgres, this means, for each table  1. enabling Row Level Security for the table, and 2. define a Policy for the table, referencing the  discriminator column.  In the Postgres documentaion examples on defining policies,  is used to define the policies. That won't work in this case, since we don't have separate database users per tenant. Instead, we can utilize a custom *session parameter* e.g.  to associate current tenant with a database session . Setting a session parameter is done using a Postgres-specific SQL statement:   The session parameter can be referenced in the policy definition. Wrapped as a Liquibase changeset, it could look like this:    Row Level Security policies are by default **not** applied for the table owner  :    With the Row Level Security policy in place, we now need to set the current tenantId on each database connection before using it, and remove the tenantId once done with the connection. Hence we need a Tenant-Aware DataSource that transparently manages the decoration of tenantId on connections:   A bit bulky, but a well proven mechanism to decorate a datasource with additional functionality.   We need to configure two DataSources: One master DataSource for Liquibase Database migrations, and one tenant-aware datasource for the application to use.   Just as before, since we mark the tenantDataSource as @Primary, it will be used by default in any component that autowires a DataSource.   The EntityListener mechanism for setting the tenantId when creating new entities remains the same:   And just as before, all entities will need to extend :   And finally the externalized configuration in application.yml:    We now have a much simplified implementation of the Shared Database with Discriminator Column pattern. The data isolation guarantee between tenants is provided by the Row Level Security mechanism in Postgres . This solution should be both robust and highly scalable.  A fully working, minimalistic example can be found in the [Github repository] for this blog series, in the [shared_database_postgres_rls branch].   In the next and final part, we'll recapitulate the pros and cons of the different patterns, and discuss some migration strategies and practices to be able to start with one and but be prepared to migrate to another pattern if necessary.   The following links have been very useful inspiration when preparing this material:  [aws.amazon.com/blogs/database/multi-tenant-data-isolation-with-postgresql-row-level-security]  [www.bytefish.de/blog/spring_boot_multitenancy_using_rls.html]
What was a given three years ago when I started working on the material used in this blog series has in many ways changed or evolved and I've also learnt a lot on the way. I've come to realize that the Go code and how the microservices are built and deployed were long due a substantial overhaul. Read on for a primer on go modules, go-chi, testify and other Go-related stuff I've come to really like and adopt.  1. Introduction 2. Go project structure 3. Go modules 4. Dependency injection 5. Configuration of our microservices 6. HTTP routing with Chi 7. Unit-testing 8. Summary  In this and an upcoming part of the [blog series], we'll do a major overhaul of both our Go code and how we write, build and deploy our microservices to Docker Swarm.  In this blog post, we'll update our core Go-based microservices to use more idiomatic coding style, go modules, a new project structure, revamped configuration, a new HTTP router and revised unit-testing.  Just a quick recap on the make-believe system landscape consisting of five discrete microservices deployed on Docker in swarm mode:  ![overview]  There's also a whole bunch of supporting services  omitted from the figure that we'll revisit in Part 17.   The finished source code can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P16   When I started coding Go back in late 2015, most examples and tutorials I read typically put the _main.go_ file in the root of the source tree and then introduced various packages for things like "model", "service" etc.  Example:  - main.go - service/handlers.go - service/handlers_test.go - model/account.go  However, I never really felt fully comfortable with that approach, so when I sometime later discovered [golang-standards/project-layout], I've gradually adopted that layout for new projects as well as updating some existing ones.  After updating the microservices with the golang-standards project layout, the code is structured like this:  - cmd/[servicename]/main.go - internal/app/service/handlers.go - internal/app/service/handlers_test.go - internal/app/model/account.go  Note the _[servicename]_ placeholder which will be replaced per-service, for example _cmd/accountservice/main.go_.  By standardizing where to find the _main.go_ file, it makes it easier to write clean build scripts as well as adding additional executables related to the microservice at hand - migrations for example.  The root folder for each microservice is now much cleaner, typically having only a [Makefile]  The new layout also affects build scripts, Dockerfiles etc which will be revisited later as well.  __   The first major change to the codebase is making all our microservices as well as the _common_ code into [go modules]. Go modules was introduced in Go 1.11 as the official dependency management tool and I personally am quite happy with it given its relative simplicity and useful toolset.  Using go modules also has the upside of _not_ forcing your codebase to live beneath a GOPATH anymore, so now one can just clone the source of this blog series to any folder and start building it.  Adding go module support is quite easy. With the _accountservice_ as an example:  > cd accountservice > export GO111MODULE=on > go mod init github.com/callistaenterprise/goblog/accountservice > go build  That's all there is to it. By the way, in Go 1.13 go modules are supposed to be turned on by default, so no more need to set the GO111MODULE env var.  The commands above should have created two files in the root of the _accountservice_ directory: _go.mod_ and _go.sum_.  go.mod:  module github.com/callistaenterprise/goblog/accountservice  go 1.12  require ( github.com/callistaenterprise/goblog/common v0.0.0-20190713133714-ded5832e931e github.com/gorilla/mux v1.7.3 github.com/graphql-go/graphql v0.7.8 github.com/graphql-go/graphql-go-handler v0.2.3 github.com/graphql-go/handler v0.2.3 // indirect github.com/myesui/uuid v1.0.0 // indirect github.com/opentracing/opentracing-go v1.1.0 github.com/prometheus/client_golang v1.0.0 github.com/sirupsen/logrus v1.4.2 github.com/smartystreets/goconvey v0.0.0-20190710185942-9d28bd7c0945 github.com/spf13/viper v1.4.0 github.com/stretchr/testify v1.3.0 gopkg.in/h2non/gock.v1 v1.0.15 )  The go.mod file has been generated by the go tools. It starts by specifying the unique identifier of your module, which by convention usually is the absolute path to it's source-control repository and possibly subfolder.  The neat thing here is that the go tools have scanned your source code and generated the _require_ block listing all direct  and also _indirect_ dependencies, i.e. dependencies of your direct dependencies.  For example, we can see that the UUID generator library _github.com/myesui/uuid v1.0.0_ is declared as an indirect dependency. We can ask the go tools which dependency that's pulling in that indirect dependency.  > go mod why github.com/myesui/uuid v1.0.0 # github.com/myesui/uuid github.com/callistaenterprise/goblog/accountservice/internal/pkg/model github.com/callistaenterprise/goblog/common/model github.com/twinj/uuid github.com/twinj/uuid.test github.com/myesui/uuid  The listing above shows us that the _model_ package of our accountservice is importing our _common/model_ package, which in it's turn is pulling in github.com/twinj/uuid and so on.  Note the _github.com/callistaenterprise/goblog/common/model_. While we have the _common_ folder in the root of our checked-out code, we're actually pulling in the code for _github.com/callistaenterprise/goblog/common/model_ directly from the latest published commit on the master branch from github.  This means that any local changes performed in _/common_ **won't affect** a locally built accountservice _before_ the changes to common has been pushed to github.  If that sounds rather inconvenient when developing - it is. Luckily, the go modules tools allows us to declare an override for how dependencies are resolved using the [replace] directive.  For local development, one can add this one-liner to _go.mod_ in order to resolve the common module from a relative path on our local file-system instead:  module github.com/callistaenterprise/goblog/accountservice  go 1.12  replace github.com/callistaenterprise/goblog/common => ../common  require ( github.com/callistaenterprise/goblog/common v0.0.0-20190713133714-ded5832e931e .... )  This is not meant to be an in-depth explanation of Go modules. Just a quick introduction and one of the more prominent changes in the overhaul of the code and infrastructure tools for the blog series!  In hindsight, I adopted some bad patterns when I first started to code Go. I relied too much on exported variables or package-scoped state instead of proper encapsulation into structs and definition of behaviour using interfaces.  Now, _main.go_ is responsible for creating the core struct:  func main { ... cfg := cmd.DefaultConfiguration arg.MustParse  gormclient := dbclient.NewGormClient // create DB client handler := service.NewHandler // create Handler struct with HTTP handler funcs  server := service.NewServer // create HTTP server with handler and config injected server.SetupRoutes ... }  A quick look at the _Handler_ struct and its companion constructor function, which encapsulates the actual business logic in its methods and whatever dependencies or state the application needs to fulfill its business requirements. In the case of the _dataservice_ to orchestrate calls to the database using the _IGormClient_.  type Handler struct { dbClient dbclient.IGormClient myIP string isHealthy bool }  func NewHandler *Handler { myIP, err := util.ResolveIPFromHostsFile if err != nil { myIP = util.GetIP } return &Handler }  The most common pattern of constructor functions is to return a _pointer_ to the created struct. Note also how we can embed some code into the constructor function to look-up our own IP.  _GetAccount_ hasn't changed much, the main difference being that it's now a method on the Handler struct and that the _dbClient_ embedded in the struct is used instead of a package-scoped one.  func  {  // Read the 'accountId' path parameter var accountID = chi.URLParam account, err := h.dbClient.QueryAccount ... }  We'll look at how this change facilitates easier unit-testing in section 7.   Given [12-factor] and the general shift towards the use of environment variables as configuration injected as "secrets" into the runtime docker container, I've decided to stop using Spring Cloud Config or any other config-server.  My last projects .  While my own experience may be anecdotal, I do think it works quite well and thus I'm migrating all the microservices in this blog series to use env vars as configuration.  Just as a recap, the old method used command-line flags hard-coded into our Dockerfile in order to pass the "bootstrap" config to the service, e.g. where to find the config server that held the actual config parameters, e.g:  ENTRYPOINT ["./accountservice", "-configServerUrl=http://configserver:8888", "-profile=test", "-configBranch=P16"]  The new solution will be based on the excellent [go-arg] library that does exactly what I want in just a few lines of code. I want:  - Precedence: Command-line flags > Env vars > Default values. - Struct-based configuration: Instead of populating a **global** _viper_-based store with whatever key-value pairs we read from our config-server, we'll instead use a Go struct that is automatically populated by _go-arg_ from flags, env vars and defaults, which we then can pass as an argument into constructor functions whenever configuration values are required. We create several domain-specific structs with related fields tailored for _each_ microservice, and then compose them together to form the full config. This removes configuration as a global state, which may introduce unwanted side-effects such as harder testing.  Example from our _accountservice_:  // Config is our root config struct. Note usage of go-arg struct tags. // The env part is optional, allows explicit mapping of env var to struct field. // Otherwise, the library automatically maps "SNAKE_CASE" env vars to struct fields in "PascalCase". type Config struct { ZipkinServerUrl string  ServerConfig AmqpConfig }  // HTTP config for our service type ServerConfig struct { Port string  Name string  }  // AMQP / RabbitMQ connection URL type AmqpConfig struct { ServerUrl string  }  // DefaultConfiguration specifies default values and returns a pointer to a struct populated with our defaults. func DefaultConfiguration *Config { return &Config{ ZipkinServerUrl: "http://zipkin:9411", ServerConfig: ServerConfig{ Name: "accountservice", Port: "6767", }, AmqpConfig: AmqpConfig{ ServerUrl: "amqp://guest:guest@rabbitmq:5672/", }, } }  Then, in our _main.go_, all the viper stuff is now gone and replaced by these few lines:  func main { logrus.SetFormatter logrus.Infof  // Initialize config struct and populate it froms env vars and flags. cfg := cmd.DefaultConfiguration arg.MustParse  initializeTracing .... truncated for brevity... }  func initializeTracing { // Note how we pass the cfg as argument and use the field on the struct now rather that viper.GetString tracing.InitTracing }  Passing configuration is a breeze now. Our Dockerfile should _never_ use command-line args directly, e.g we're down to:  ENTRYPOINT ["./accountservice"]  For running locally, we can use command-line args to override defaults or env vars:  > ./bin/accountservice --port=1337 --environment=loadtest  When using docker compose for testing locally or in test environments, we typically have separate compose files for each environment. One example:  services: accountservice: image: someprefix/accountservice environment: ENVIRONMENT: "test" AMQP_SERVER_URL: amqp://user:password@rabbitmq:5672/ ...  For staging/production, the env vars are typically injected from "secrets" by your container orchestrator or some other mechanism. This is not in scope for this blog post.  There's nothing wrong with [gorilla] and declare routes, including setting up sub-routers.  Here's some example routes using go-chi from the blog series, see inlined comments:  // Server is a struct that encapsulates HTTP config, the router instance // and the handler struct having the actual handler methods type Server struct { cfg *cmd.Config r *chi.Mux h *Handler }  // SetupRoutes creates a new chi router and then uses the fluent DSL to set up our routes, // including middlewares func  { s.r = chi.NewRouter  // Setup global middlewares , note that order of declaration matters! // These are recommended ones from go-chi s.r.Use s.r.Use s.r.Use s.r.Use s.r.Use  // Sub-router for /accounts // Note use of With for adding middlewares for Zipkin tracing and Prometheus monitoring s.r.Route { r.With. With. Get // LOOK HERE!  ... other routes omitted for brevity ... }) ... }  The _Get_ on the new _Handler_ struct we looked at in section 4.  The _Trace middleware we've defined ourselves:  func Trace http.Handler {  // Returns function having the correct middleware signature return func http.Handler {  // Returns the actual middleware "worker" function. // Note how opName from the partent scope is embedded in the returned func. return http.HandlerFunc { span := tracing.StartHTTPTrace defer span.Finish ctx := tracing.UpdateContext next.ServeHTTP }) } }  go-chi middlewares accepts functions having the _func, we need to perform some function wrapping to embed the opName into the returned middleware.  The separation of the _routes_ from the _handlers_ makes it much easier to unit-test the router, which brings us to our next change; how we unit-test.  Before this update, I used [goconvey] module provides a slightly better developer experience.  For mocking outgoing HTTP requests .  For mocking out dependencies injected into our structs  for a given interface as they provide type-safe mocking of methods and a clean DSL for setting up various expectations, side-effects and return values.  First, one needs to install mockgen:  go get github.com/golang/mock/gomock go install github.com/golang/mock/mockgen  After that generating a mock for a given interface is just a simple one-liner. Example from the _dataservice_ where a mock is generated for all interfaces in _cockroachdb.go_, i.e. the IGormClient:  mockgen -source internal/pkg/dbclient/cockroachdb.go -destination internal/pkg/dbclient/mock_dbclient/mock_dbclient.go -package mock_dbclient  Setting up the behaviour of a mock in a unit test is then really simple.  mockRepo := mock_dbclient.NewMockIGormClient mockRepo.EXPECT. QueryAccount. Return. Times  In this example, we tell the mock to expect a single call to the QueryAccount method with anything as the first parameter and the string "123" as the second one. If such an invocation is received by the mock, the specified _AccountData_ struct and _nil_ error is returned.  Since we've moved to a struct- and "dependency injection" based architecture for our go services, it's actually much easier to set up the tests.  Here's an example unit-test of the _dataservice_, beginning with a simple setup function:  // Run this first in each test, poor substitute for a proper @Before func func setup *Server { tracing.SetTracer h := NewHandler s := NewServer s.SetupRoutes return s }  Note how we create a _service.Handler_ instance with our MockIGormClient as argument, and how the handler then is passed to the NewServer constructor function along with the default config.  Here's the actual "happy path" unit test for the _QueryAccount_ method:  func TestQueryAccount { ctrl := gomock.NewController defer ctrl.Finish mockRepo := mock_dbclient.NewMockIGormClient mockRepo.EXPECT  s := setup  req := httptest.NewRequest resp := httptest.NewRecorder  s.r.ServeHTTP  account := model.AccountData _ = json.Unmarshal  assert.Equal assert.Equal assert.Equal }  Some notable changes to how the test  is written:  - The _TestQueryAccount_ is a bit cleaner without the GoConvey Given-When-Then constructs. - Note how we set up the mocked IGormClient with the _mockRepo.EXPECT_ call. - The invocation of the router using the httptest request and response objects happens directly on the _r *Router_ dependency we injected into our server with the ServeHTTP method. - We're still using the exact same [httptest] request and response. - Finally, note how we've replaced the GoConvey _So. While the GoConvey "So" approach is just fine, the assert module probably feels more familiar to people like me who's used JUnit for the last decade or two...  Another popular approach to testing in Go is [table-driven tests]. I'm personally a bit divided on table-driven testing, but I'm planning on using TDT when I get to integration testing in Part 18.  In this installment of the [blog series], we've refactored most of the codebase to better comply with idiomatic go coding styles, go modules, 12-factor configuration and cleaner tests.  In the next part, I'll continue the overhaul by introducing Makefiles and Docker compose / Docker stack for building and deploying.  Please help spread the word! Feel free to share this blog post using your favorite social media platform, there's some icons below to get you started.  Until next time,  // Erik
 A successful continuous delivery  pipeline requires a high level of automated testing. It is essential that tests are reliable to ensure that nothing unexpected slips into your production environment. Swift execution is also desirable to provide timely feedback to developers.  Testing asynchronous processes provide a different set of challenges from testing a synchronous request-response scenario. In this 2 part blog post I will investigate how to test an application that publishes events via [Kafka] I will look at how this can be made faster.  The scenario presented in these blog posts is inspired by a real-life case. The following link will take you to the [source code for the example application].  -[readmore]-   * This line will be replaced with the     The example application uses [Spring Boot] to expose a single RESTful api. A call to the  endpoint will perform some internal state change, publish some events and finally send a response back to the client.  Events are published in a JSON format that looks something like this:   In this case the  is intended to be a unique id related to the request. A single transaction will consist pf 5 events and each event will have a unique  value. The  field is intended to simulate a large message.  In the real-life scenario that this example is based on the event stream was used by a downstream application to apply each state change to its own representation of a business object. The intended aim was to maintain a consistent state for the object in both applications. This required that the downstream application consumed events in sequential order. The requirement proved difficult to enforce on the consumer so, as a compromise, the publisher guaranteed to publish events in ascending order.  This requirement has been applied to the example application and the publisher looks something like this:   A Kafka topic is split into a number of partitions. As we will see in part 2 sequential consumption is difficult if events are spread across multiple partitions. To make life easier for the consumer the publisher guaranteed to publish all events for a transaction to the same partition. For simplicity the example application will publish all events to a single partition:   Now that the publisher is in place the next step is to test that the guarantee of sequential publishing holds.  Both the Spring Boot application and a Kafka broker must be available in order to test the publisher. I will use the term "integration test" for testing how the running Spring Boot application behaves with the various integration points. Integration tests will be performed seperately from unit tests .  The [Gradle build tool] has been chosen to build and test the application. The Gradle project contains the  plugin which performs unit testing as part of the build. I have extended this by adding a custom  task to run the integration tests using a seperate source set with path .  Some other tasks that have been added to the Gradle build are: *  - builds a [Docker] image containing a Java 11 JRE and the Spring Boot application as an executable Jar. *  - uses [Docker Compose] to start a Kafka broker and the Spring Boot application *  - uses [Docker Compose]  Executing the Gradle  task  will build the application, start the services, and once the tests are complete stop them all.  The first test will use [JUnit5] to send a request to the  endpoint. The test will then verify that the request was accepted successfully and will then check that 5 events have been published and that the events are in sequential order:   Running this test with Gradle  will more than likely fail. Why?  The tasks above use the Gradle  task to execute a command line instruction - in this case starting the application in a Docker container. Although the application container starts quickly the Spring Boot application itself takes a few seconds to start up. Gradle is unaware of the state of the application and eagerly starts to run the integration tests which then fails. The integration test needs some way to check the readiness of the application under test.  In this example I will use the [Spring Boot Actuator] extension is introduced that extends the behaviour of the  lifecycle phase. Test execution is paused whilst the extension polls the actuator's  endpoint. If the  endpoint is not healthy after a reasonable amount of time, one minute say, then the tests will be aborted:   This example contains only contains one test class so the simplest way to add the extension is to annotate the test class:   Now the integration test can be certain that the Spring Boot application is ready when tests are executed. Another benefit of this approach is that this acts as an indirect test that the healthcheck endpoint is working as expected.  Running this test will give a successful result!  This test make two assumptions: * events will be published sequentially to ensure sequential consumption * all events consumed during test execution are related to the test case  What happens when we challenge these assumptions?  As stated before there is a requirement that consumption must be performed sequentially. This is not actually a requirement on the producer - the producer is just being helpful. Being helpful does have consequences - in the real-life case the response was sent to the client once all events had been published adding between 100ms to 200ms to the elapsed time of the request, which was quite significant.  Let's speed up the example application by publishing events asynchronously. First enable asynchronous processing:   And make publication asynchronous by using the  annotation. Let's also make things more complicated by allowing the publisher to choose any of the 10 partitions on the topic:   *Note: this is a naive example where failure to publish a message has no consequences for the request as a whole. Publishing an event becomes a "best effort" task.*  If we look in the application logs we can see that events are no longer being published sequentially:   Running the test now will result in a consistent failure.  Sequential publication is no longer a requirement so the test can be altered to verify that in the 5 published events there are 5 unique sequence ids from 1 to 5 :   And the test succeeds again.  Let's now challenge the assumption that all published events are related to the test. This is an unlikely assumption in a complicated application and certainly not true in a production environment.  To simulate this we will introduce the  that will publish a random event every 3 seconds:   Now tests become flakey - sometimes, but not always, a 6th or 7th event is consumed during the 10 seconds of polling. What is needed is a way to seperate events that are related to the test from those that can be ignored.  There are many ways to approach linking tests with events. It is not considered good practice to engineer an application to meet the needs of the test harness  to provide a relatively unobtrusive method of linking using distributed tracing. In this case I will argue that a distributed tracing feature can be considered of value to the application.  Spring Cloud Sleuth has been added to the Gradle project:   Every incoming request will start a new [Span] which effectively assigns each incoming request a unique trace id. In the example logs above you can see that  is a `trace id`. The next step is to make the test aware of the of the `trace id`.  First the event schema is extended to add some metadata that contains the `trace id`:   Next a  filter adds the `trace id` to an Http header on the response called .   Now our test can match the `trace id` in the response to that in the metadata of the event. This allows us to simply filter and ignore any events that are not relevant to our test during execution time!   And the test succeeds again!  This blog post introduces a pattern using [JUnit5]. This pattern can be extended and repeated to provide a rich and full test suite as the application evolves.  In [part 2], and offer one solution for how to reduce long execution times.
I'm at QCon listening to sessions of "The Cloud as the New Middleware Platform" track. One of the sessions was about the platform of SalesForce.com. SalesForce is a Software As A Service  application within the CRM domain. It is built on a generic service development platform called "Force.com". So SalesForce is just an example of an application built on the Forces.com SaaS platform.  The platform has a lot of nifty 4GL features that boosts development of business applications . The more I heard about Force.com, the more it made me think about SAP MySap ERP application and the business application platform on which SAP applications have been built for ages.  The loop was closed when the SalesForce architect revealed a proprietary business logic language named Apex . As SaaS grows, it will be interesting to see if Force.com becomes the "SAP of SaaS".
 In this blog post, we'll build a simple microservice using [Golang] microservice landscape.  -[readmore]-   Building microservices using Golang may not be the first choice when you are targeting a [Spring Cloud] environment where most if not all participating applications are Java or JVM-based. However, Go have a number of things going for it - low memory footprint, fast startup/shutdown and being statically compiled for a specific OS / CPU architecture it puts very few requirements on the host OS - no Java Runtime Environment for example.  In this blog post, we won't use any existing Go microservice frameworks such as [go-kit].  ![overview]  We can see the new 'Vendor' service in the bottom left corner with its interactions with the Eureka discovery service added for clarity.  The source code for the Go microservice can be found on [github].  The updated final source code for the Spring Cloud landscape [can be found here].   The creation of a Go-based microservice with Eureka registration and adding it to the Spring Cloud landscape can be divided into these five relatively distinct steps:  1. Implement a HTTP REST service for serving "vendors", /health, /info etc. Uses [gorilla] web toolkit. 2. Implement Eureka client functionality for registration, heartbeat and deregistration. 3. Make a Dockerfile and build the Go executable into amd64/linux. 4. Add the Go microservice to Magnus landscape, e.g. docker-compose.yml 5. Add usage of the new Vendor microservice to the CompositeServices, e.g. make sure Ribbon can discover and load-balance it.   As previously stated, we'll build most things from scratch this time. The [main method is of course a good starting point for looking at the Go code that starts our little service and registers with Eureka:  func main { handleSigterm // Handle graceful shutdown on Ctrl+C or kill  go startWebServer  eureka.Register // Performs Eureka registration  go eureka.StartHeartbeat  // Block... wg := sync.WaitGroup // Use a WaitGroup to block main exit wg.Add wg.Wait }  The "vendor" microservice written in Go will return a JSON array of vendors for a given productId, e.g:  GET /vendor/  []  The HTTP server with routing is loosely based on [this article]_ function from the core http library.  func startWebServer { router := service.NewRouter log.Println err := http.ListenAndServe if err != nil { log.Println } }   The NewRouter function returns a pointer to a mux.Router where the routes are fed into the router:  type Route struct { Name string Method string Pattern string HandlerFunc http.HandlerFunc }  type Routes []Route // An array / slice of routes  var routes = Routes{ // Other routes omitted for clarity Route{ "VendorShow", "GET", "/vendors/", VendorShow, // Func reference. },  }   Finally, the _VendorShow_ method builds and returns a hard-coded list of vendors:  func VendorShow { vars := mux.Vars var productId int var err error if productId, err = strconv.Atoi; err != nil { panic } fmt.Println vendors := make v1 := model.Vendor v2 := model.Vendor vendors = append if len > 0 { w.Header w.WriteHeader if err := json.NewEncoder; err != nil { panic } return }  // If we didn't find it, 404 w.Header w.WriteHeader if err := json.NewEncoder; err != nil { panic } }    Eureka is the discovery server from Netflix used in Spring Cloud. Non-java services can register with Eureka using the [REST API] of Eureka. Please note that Spring Cloud does _not_ provide the /v2/ endpoints from the linked document. Registration, heartbeat and deregistration works fine as long as the /v2/ path segment is omitted.  A Eureka client library for golang called [hudl/fargo] actually exists. However, I've chosen to do some plain REST calls myself this time in order to understand the registration and heartbeat process better.   Given this code, a HTTP POST will be sent to the Eureka REST endpoint for registrations. Please note that the code above uses a hard-coded URI to Eureka. This should of course be externalized in some manner. For the registration POST body, one can choose between XML and JSON format, I've picked JSON:  var instanceId string  func Register { instanceId = util.GetUUID; // Create a unique ID for this instance  dir, _ := os.Getwd data, _ := ioutil.ReadFile // Read registration JSON template file  tpl := string tpl = strings.Replace // Replace some placeholders tpl = strings.Replace tpl = strings.Replace  // Register. registerAction := HttpAction { // Build a HttpAction struct Url : "http://192.168.99.100:8761/eureka/apps/vendor", // Note hard-coded path to Eureka... Method: "POST", ContentType: "application/json", Body: tpl, } var result bool for { result = DoHttpRequest // Execute the HTTP request. result == true if req went OK if result { break // Success, end registration loop } else { time.Sleep, } // retry in 5 seconds. } }  For details about how we send the HTTP POST, see [DoHttpRequest] of my little Golang projects.  What's more interesting is the JSON body we've posted:  { "instance": { "hostName":"$", // We're dynamically setting non-loopback IP adress here "app":"vendor", // Name seen in Eureka "ipAddr":"$", "vipAddress":"vendor" // Important, used by Ribbon to look up endpoint adress "status":"UP", "port":"$", "securePort" : "8443", "homePageUrl" : "http://$/", "statusPageUrl": "http://$/info", "healthCheckUrl": "http://$/health", "dataCenterInfo" : { "name": "MyOwn" }, "metadata": { "instanceId" : "vendor:$" // Metadata entry to differentiate instances when scaling. } } }  The corresponding XSD , it maps 1:1 to the JSON structure used above. The page urls are actually implemented by the routes.go and handlers.go files but are of course mainly placeholder implementations just there give some semi-proper response. For example, _dataCenterInfo_ must have either 'MyOwn' or 'Amazon' as values. There is a complex 'amazonMetadataType' with many Amazon-specifics we ignore in this blog post since we're gonna run this on-premise using docker-compose.   In a microservice landscape, a container running a given service may be started or shut down at any time. We want our discovery service to be as up-to-date as possible with available service instances. What good is a large number of instances of a given microservice if the consumers cannot find them? The opposite is just as important - how do we make sure that our go microservice deregisters itself with Eureka when a supervising container framework decides to decrease the number of running instances? How a particular container handles shutdowns may vary. For this particular example with a Go-based microservice running in a docker container managed by docker-compose, it is sufficient to capture whenever the OS sends an interrupt signal or when a SIGTERM signal is received, e.g. Ctrl+C.  This piece of code listens for such signals and passes them onto a channel supervised by an anonymous function running in a goroutine:  func handleSigterm { c := make // Create a channel accepting os.Signal // Bind a given os.Signal to the channel we just created signal.Notify // Register os.Interrupt signal.Notify // Register syscall.SIGTERM  go func { // Start an anonymous func running in a goroutine <-c // that will block until a message is recieved on eureka.Deregister // the channel. When that happens, perform Eureka os.Exit // deregistration and exit program. } }  The code above makes sure Eureka gets a deregistration HTTP DELETE before the container is shut down.   To speed things up, I created a little bash [shell script] to help with the build process. It's actually quite informative:  _buildall.sh_  #!/usr/bin/env bash  export GOARCH=amd64 export GOOS=linux go build -o bin/goeureka src/github.com/eriklupander/goeureka/*.go docker build -t vendor . export GOARCH=amd64 export GOOS=darwin  The GOARCH and GOOS environment variables tells go build what CPU architecture and OS to build the binary for. Since our docker base image is a linux/amd64 one, we start by setting this before running _go build_. Next, we build the docker container giving it the name 'vendor' with the current directory as base path. Finally, we change GOOS och GOARCH back to my developer laptop settings .  Next stop is obviously the [Dockerfile] that specifies what base image to use, which files to add and what to execute when the container starts:  _Dockerfile_  FROM ofayau/ejre:8-jre MAINTAINER Micro Service <micro.service@gmail.com>  EXPOSE 8080  ADD bin/goeureka goeureka ADD templates/*.json templates/  ENTRYPOINT ["./goeureka"]  _ofayau/ejre:8-jre_ is a bit of an overkill since the image bundles a 32-bit JRE we have absolutely no use for when running a Golang microservice, but it keeps things consistent with the blog series. Next, we tell the container to expose port 8080  and add the _goeureka_ binary and any JSON files in the _templates/_ directory to the image. Finally, we specify that the container shall execute ./goeureka on startup. Doesn't get much simpler, right?   As previously stated, we'll integrate the Go microservice into a Spring Cloud landscape.  Now, we're done with the Go code and can move over to the Spring Cloud environment from the blog series. If coding along - check out, switch to the M6-GO branch and build everything:  git clone git@github.com:callistaenterprise/blog-microservices.git cd blog-microservices git checkout M6-GO ./buildAll.sh  __   In the branch checked out above all the changes below have already been applied. However, let's walk through a number of the changes performed beginning with the easiest part, adding the vendor service to the end of _docker-compose.yml_  vendor: image: vendor links: - discovery  This little snippet tells docker-compose to use the _vendor_ image for the same named logical name _vendor_. Finally, the _links_ attribute tells docker-compose that this service is allowed to access the _discovery_ service, e.g. Eureka.  Try to start things up. The vendor microservice won't be accessible from outside, but it might be a good idea to first make sure its registration with Eureka works properly:  docker-compose up -d  It will probably take a minute or two until everything has started. Since the discovery service is declared with "ports" in _docker-compose.yml_, it is accessible from outside of the internal docker landscape on port 8761. E.g:  discovery: image: callista/discovery-server ports: - "8761:8761"  Unless you've entered a hostname record in your /etc/hosts file  for the 'docker' host, you may need to look up the IP address of the docker network. One way to do this is by using _docker-machine ls_:  > docker-machine ls  NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default * virtualbox Running tcp://192.168.99.100:2376 v1.11.0  Open a web browser and enter your equivalent of http://192.168.99.100:8761 and you should end up at the Eureka start page.  ![eureka1]  It's alive! Indeed, we see 'VENDOR' neatly in the list. Note that the 172.17.0.x links are unclickable - your browser has no access to the private network of the docker environment.  Now, shut it down again:  > docker-compose down   Time for some Java coding! Well - not much coding actually, mainly copy+paste and renaming stuff from blog series source. In [ProductCompositeService.java], we've added some code to the _getProduct_ method that will fetch vendors for us:  // 4. Get optional vendors ResponseEntity<List<Vendor>> vendorsResult = integration.getVendors; List<Vendor> vendors = null; if  { // Something went wrong with getVendors, simply skip the vendors-information in the response LOG.debug; } else { vendors = vendorsResult.getBody; }  return util.createOkResponse;  Nothing fancy, the cool stuff happens inside _integration.getVendors_:  @HystrixCommand public ResponseEntity<List<Vendor>> getVendors { LOG.info;  URI uri = util.getServiceUrl; // This is the cool part!!  String url = uri.toString + "/vendors/" + productId; LOG.debug;  ResponseEntity<String> resultStr = restTemplate.getForEntity; LOG.debug; LOG.debug;  List<Vendor> vendors = response2Vendors; LOG.debug;  return util.createOkResponse; }  Except some code that converts stuff etc. the nice thing here is that our new Vendor-fetching Java code works 100% the same way accessing our Golang-based service as fetching those Spring Boot-based review and recommendation services does. We look up the URL to the vendor service by asking [Ribbon] for the address for 'vendor'. This 'vendor' is the same string we entered in the registration JSON for 'vipAddress', i.e. Virtual IP Address. After that it's just a matter of asking the RestTemplate to call the service for us att the returned URL, process the response and pass it up the call stack.  We also had to add a single line to ProductCompositeIntegration.java in order to enable load-balancing using Ribbon:  @Autowired @Qualifier // ADDED THIS LINE! private RestTemplate restTemplate;  Example console output after having fetched an [OAuth] token:  > export TOKEN=f7e6bec9-124d-4bf0-83d0-dfe27e5b3a20 > curl -H "Authorization: Bearer $TOKEN" -k 'https://192.168.99.100/api/product/1046'  { "productId":1046,"name":"name","weight":123, "recommendations":[], "reviews":[], "vendors":[] }  Awesome - we have the Go-based 'vendors' microservice fully functional!   Of course, we can scale our vendor service just like any other service. Let's add second instance of the vendor service.  > docker-compose scale vendor=2 Creating and starting blogmicroservices_vendor_3 ... done  Behind the scenes, docker-compose works its magic, spins up a new container with the 'vendor' image, launches the Go application which immediately registers itself with the Eureka server, making it visible to the Ribbon load-balancer.  ![eureka2]  Call the /api/product/ a few times, four in our case, and then check the log:  > docker-compose logs | grep vendors  vendor_1 | Loading vendors for product 1044 vendor_2 | Loading vendors for product 1042 vendor_1 | Loading vendors for product 1041 vendor_2 | Loading vendors for product 1046  Indeed, we see that the two 'vendor' instances are taking turns serving the product requests.   Running standalone on my OS X laptop, the 'vendor' microservice uses about 2 mb of RAM after starting up and running a dozen requests or so.  Moving on to docker, we can check memory use of our microservice landscape :  > docker stats CONTAINER CPU % MEM USAGE / LIMIT 31d0c6ab9012 0.06% 126.2 MB / 2.1 GB <-- Monitor dashboard e3c8c35def3b 0.27% 185 MB / 2.1 GB <-- Edge server d5582272fe18 0.47% 220 MB / 2.1 GB <-- Product composite service cb1d234fba5d 0.49% 218 MB / 2.1 GB <-- Product API service 9a9442abd475 0.33% 168.1 MB / 2.1 GB <-- Product service 21c0743bb845 0.27% 170.1 MB / 2.1 GB <-- Review service 6a300b04ce36 0.30% 163.1 MB / 2.1 GB <-- Recommendation service 6fd8d08cbf1c 0.00% 8.139 MB / 2.1 GB <-- Vendor service 602ff6640a7b 0.06% 146.5 MB / 2.1 GB <-- Auth server 3606d1ab8a55 0.72% 202.3 MB / 2.1 GB <-- Eureka 83e677dbc515 0.34% 97.89 MB / 2.1 GB <-- RabbbitMQ   Indeed, our running 'vendor' container uses ~8 MB of RAM while the similar review and recommendation services uses 160-170 MB each. If RAM is a limiting factor when scaling up a microservice environment, using Go may definitely be an option. However, the figures above is after just a dozen requests or so - memory use under load may be very different and the Spring Boot-based microservices may very well scale better without increasing their memory footprint significantly.  Both binaries I've built  produces an executable about 8 mb in size. Not very compact, but as previously stated these Go programs can be run on very "bare" containers.  Execution speed for returning the 'vendor' list from localhost is in the < 1ms range, though measuring latencies for a "dummy" service running on localhost is next to meaningless. I've done a bit of benchmarking of gorilla/mux in the past and it performs quite well, almost on par with Spring Boot. The case was a simple HTTP service fetching some data from a local MongoDB instance, both Spring Boot and Go could handle about 4K concurrent requests with 2-3 ms latency.   First and foremost, the vendor microservice itself has no security whatsoever, it fully depends on Docker mechanics service exposed directly on the internet as net/http with Gorilla should be viewed more as a HTTP framework than a full-fledged production-hardened web server.  Furthermore, using a Go-based microservice as a replacement for the more complex "productcomposite service" would require implementing your own Discovery service lookups for other participating services, load-balancing those, circuit breakers and composing responses from aggregated data. Hopefully I can return to that topic later.
 A while ago I started to implement TLS/SSL mutual authentication on Android. How to actually implement the functionality on the Android platform is covered in my other article [Android - TLS/SSL Mutual Authentication]. Before such implementation can be done, it is important to have the keys and certificates prepared. In this article demonstrate how you can create these. However, this article is not just applicable to Android and should be usable in other scenarios as well.  -[readmore]-  For this article to be useful, the required tools are: openssl, Java's Keytool and the [BouncyCastle-provider]. There are also some resources that I strongly recommend and has been very useful:  * [SSL Certificates Howto] * [OpenSSL Keys Howto] * [OpenSSL Certificates Howto] * [Java Keytool]  One might argue why I don't use keytool to generate the keys and certificates and use them right away. Well, I was very curious about learning more about openssl and how to deal with various formats of keys and certificates.  Let's start from scratch. First of all we need private keys. We use openssl to create these:   This will create the two keys;  and . We will use these in the next step to sign our certificates with. In normal cases we would create a CA-signing request, that is sent to a CA who will issue your certificates. But since we want to self-sign our certificates this step is redundant.    Additionally, instead of being prompted for the certificate's subject line you can use the  parameter and pass it to the `openssl req` command. What we just did was basically creating a CA signing request using our private keys to sign the outgoing x509-certificates. The certificates will be coded in pem-format and valid for 365 days.  In order to use our keys and certificates in Java applications we need to import them into keystores. First of all, we want the client to trust the server certificate. To do this we must create a client trust store and import the server’s certificate.   > **Note:** On the client side, which in our case will be an Android app we use Bouncy Castle as our provider since it is supported on the Android platform.  Create a trust store for the server and import the client's certificate into it.   Currently, we have two trust stores one for the server in which we imported the client’s certificate and one for the client in which we imported the server’s certificate.  A problem with Java’s keytool application is that it won’t let us do such a simple thing as importing an existing private key into a keystore. The workaround to this problem is to combine the private key with the certificate into a pkcs12-file  and then import this pkcs12 keystore into a regular keystore.  Combine the certificate and the private key for the server and client respectively:   Import the created keystores to new ones with common formats:   We should now have all files we need for a successful TLS/SSL mutual authentication. The files we move to our Android project will be:  and . The files we move to our server will be:  and .
 This blog post provides an overview of the different concepts behind reactive programming and its history.  -[readmore]-    Reactive programming has been around for some time but gained much higher interest during the last couple of years. The reason for this relates to the fact that traditional imperative programming has some limitations when it comes to coping with the demands of today, where applications need to have high availability and provide low response times also during high load.  To understand what reactive programming is and what benefits it brings, let’s first consider the traditional way of developing a web application with Spring - using Spring MVC and deploying it on a servlet container such as Tomcat. The servlet container has a dedicated thread pool to handle the http requests where each incoming request will have a thread assigned and that thread will handle the entire lifecycle of the request , the higher thread pool size we configure, the higher the memory consumption.  If the application is designed according to a microservice based architecture, we have better possibilities to scale based on load, but a high memory utilization still comes with a cost. So, the thread per request model could become quite costly for applications with a high number of concurrent requests.  An important characteristic of microservice based architectures is that the application is distributed, running as a high number of separate processes, usually across multiple servers. Using traditional imperative programming with synchronous request/response calls for inter-service communication means that threads frequently get blocked waiting for a response from another service - which results in a huge waste of resources.  Same type of waste also occurs while waiting for other types of I/O operations to complete such as a database call or reading from a file. In all these situations the thread making the I/O request will be blocked and waiting idle until the I/O operation has completed, this is called blocking I/O. Such situations where the executing thread gets blocked, just waiting for a response, means a waste of threads and therefore a waste of memory.  ![thread blocked]<br> _Figure 1 - Thread blocked waiting for response_  Another issue with traditional imperative programming is the resulting response times when a service needs to do more than one I/O request. For example, service A might need to call service B and C as well as do a database lookup and then return some aggregated data as a result. This would mean that service A’s response time would besides its own processing time be a sum of: - response time of service B  - response time of service C  - response time of database request   ![calls in sequence]<br> _Figure 2 - Calls executed in sequence_  If there is no actual logical reason to do these calls in sequence, it would certainly have a very positive effect on service A’s response time if these calls would be executed in parallel. Even though there is support fo doing asynchronous calls in Java using CompletableFutures and registering callbacks, using such an approach extensively in an application would make the code more complex and harder to read and maintain.  Another type of problem that might occur in a microservice landscape is when service A is requesting some information from service B, let’s say for example all the orders placed during last month. If the amount of orders turns out to be huge, it might become a problem for service A to retrieve all this information at once. Service A might be overwhelmed with the high amount of data and it might result in for example an out of memory-error.  The different issues described above are the issues that reactive programming is intended to solve. In short, the advantages that comes with reactive programming is that we: - move away from the thread per request model and can handle more requests with a low number of threads - prevent threads from blocking while waiting for I/O operations to complete - make it easy to do parallel calls - support “back pressure”, giving the client a possibility to inform the server on how much load it can handle    A short definition of reactive programming used in the Spring documentation is the following:  >"In plain terms reactive programming is about non-blocking applications that are asynchronous and event-driven and require a small number of threads to scale. A key aspect of that definition is the concept of backpressure which is a mechanism to ensure producers don’t overwhelm consumers."  So how is all of this achieved?<br> In short: by programming with asynchronous data streams. Let’s say service A wants to retrieve some data from service B. With the reactive programming style approach, service A will make a request to service B which returns immediately . Then the data requested will be made available to service A as a data stream, where service B will publish an onNext-event for each data item one by one. When all the data has been published, this is signalled with an onComplete event. In case of an error, an onError event would be published and no more items would be emitted.  ![data stream]<br> _Figure 3 - Reactive event stream_  Reactive programming uses a functional style approach , which gives the possibility to perform different kinds of transformations on the streams. A stream can be used as an input to another one. Streams can be merged, mapped and filtered.  Reactive programming is an important implementation technique when developing "reactive systems", which is a concept described in the "Reactive Manifesto", highlighting the need for modern applications to be designed to be: - Responsive  - Resilient  - Elastic  - Message Driven   Building a reactive system means to deal with questions such as separation of concerns, data consistency, failure management, choice of messaging implementation etc. Reactive programming can be used as an implementation technique to ensure that the individual services use an asynchronous, non-blocking model, but to design the system as a whole to be a reactive system requires a design that takes care of all these other aspects as well.   In 2011, Microsoft released the Reactive Extensions  library for .NET, to provide an easy way to create asynchronous, event-driven programs. In a few years time, Reactive Extensions was ported to several languages and platforms including Java, JavaScript, C++, Python and Swift. ReactiveX quickly became a cross-language standard. The development of the Java implementation - RxJava - was driven by Netflix and version 1.0 was released in 2014.  ReactiveX uses a mix of the Iterator pattern and the Observer pattern from Gang of Four. The difference is that a push model is used compared to Iterators normal pull-based behaviour. Along with observing changes, also completion and errors are signalled to the subscriber.  As time went on, a standardisation for Java was developed through the Reactive Streams effort. Reactive Streams is a small specification intended to be implemented by the reactive libraries built for the JVM. It specifies the types to implement to achieve interoperability between different implementations. The specification defines the interaction between asynchronous components with back pressure. Reactive Streams was adopted in Java 9, by the [Flow API]. The purpose of the Flow API is to act as an interoperation specification and not an end-user API like RxJava.  The specification covers the following interfaces:  __Publisher:__<BR> This represents the data producer/data source and has one method which lets the subscriber register to the publisher.   __Subscriber:__<BR> This represents the consumer and has the following methods: -  is to be called by the  before the processing starts and is used to pass a  object from the  to the   -  is used to signal that a new item has been emitted  -  is used to signal that the  has encountered a failure and no more items will be emitted  -  is used to signal that all items were emitted sucessfully  __Subscription:__<BR> The subscriptions holds methods that enables the client to control the Publisher's emission of items .  -  allows the  to inform the  on how many additional elements to be published  -  allows a subscriber to cancel further emission of items by the   __Processor:__<BR> If an entity shall transform incoming items and then pass it further to another , an implementation of the  interface is needed. This acts both as a  and as a .  Spring Framework supports reactive programming since version 5. That support is build on top of Project Reactor.  Project Reactor  is a Reactive library for building non-blocking applications on the JVM and is based on the Reactive Streams Specification. It’s the foundation of the reactive stack in the Spring ecosystem. This will be the topic for the second blog post in this series!   [A brief history of ReactiveX and RxJava]  [Build Reactive RESTFUL APIs using Spring Boot/WebFlux]  [Java SE Flow API]  [Project Reactor]  [Reactive Manifesto]  [Reactive Streams Specification]  [ReactiveX]  [Spring Web Reactive Framework]
I attended a two days workshop before React Europe in Paris. It was two intense days where I picked up some new things and gor a deeper understanding of react native. The first day was mainly about react-navigation an then a shorter section about react-native-web, Second day was the main topics performance and animations.  -[readmore]-  Preparations before the workshop was to have the computer setup with react native and expo. So you could run expo-cli and especially
 This is not yet another introduction to microservices, for a good introduction please read [Fowler - Microservices]. Instead this blog post takes off assuming that we already have started to use microservices to decompose monolithic applications for improved deployability and scalability.  When the number of deployed microservices increase in a system landscape new challenges will arise not apparent when only a few monolithic applications are deployed. This blog post will focus on these new challenges and define an operations model for a system landscape deployed with a large number of microservices.  -[readmore]-  The blog post is divided in the following sections:  1. Prerequisites 2. Scaling up 3. Questions 4. Required components 5. A reference model 6. Next step   To start with, what is required to roll out large numbers of microservices in the system landscape?  According to [Fowler's blog post], this is what we want to achieve:  <img src="https://callistaenterprise.se/assets/blogg/an-operations-model-for-microservices/microservices-1.png" width="700" />   However, before we can start to roll out large number of microservices in our system landscape replacing our monolithic applications there are some prerequisites that needs to be fulfilled . We need:  * a target architecture * a continues delivery tool chain * a proper organization  Let's briefly look a each prerequisite.   First we need an architectural idea of how to partition all our microservices. We can *for example* partition them vertically in some layers like:  * *Core services* Handling persistence of business data and applying business rules and other logic * *Composite services* Composite services can either orchestrate a number of core services to perform a common task or aggregating information from a number of core services. * *API services* Expose functionality externally allowing, for example, third parties to invent creative applications that use the underlying functionality in the system landscape.  ...and horizontally we can apply some domain driven partitioning. This can result in a target architecture like:  <img src="https://callistaenterprise.se/assets/blogg/an-operations-model-for-microservices/microservices-architecture.png" width="700" />  **Note:** This is only a sample target architecture, your architecture can be completely different. The key thing here is that you need to have an target architecture established before you start to scale up deploying microservices. Otherwise you might end up in a system landscape that just looks like a big bowl of spaghetti with even worse characteristics than the existing monolithic applications.  We also assume that we have some kind of continuous delivery tool chain in place so that we can roll out our microservices in an efficient repeatable and quality driven way, e. g.:  <img src="https://callistaenterprise.se/assets/blogg/an-operations-model-for-microservices/microservices-1-2.png" width="700" />   Finally, we assume that we have adopted our organization to avoid issues with [Conway's law]. Conway's law state that:  > Any organization that designs a system  will produce a design whose structure is a copy of the organization's communication structure.  <img src="https://callistaenterprise.se/assets/blogg/an-operations-model-for-microservices/microservices-1-1.png" width="700" />    So, now over to the part that this blog post will focus on:  > What will happen in a system landscape when we start to split up a few monolithic applications and replace them with a large number of microservices?  1. **Larger number of deployed units** Many small microservices instead of a few big monolithic applications will, of course, result in a significantly increased number of deployed units to manage and keep track of.  1. **The microservices will both expose and consume services** This will resulting in a system landscape where the most of the microservices are interconnected with each other  1. **Some microservices will expose an external API** These microservices will be responsible for shielding the other microservices from external access  1. **The system landscape will be more dynamic** New microservices are deployed, old ones are replaced or removed, new instances of existing microservices are started up to meet increased load. This means that services will come and go at a much higher frequency then before.  1. **MTBF will decrease, e.g. failures will happen more frequently in the system landscape** Software components fails from time to time. With a large number of small deploy units the probability that some parts  in the system landscape is failing will increase compared to a system landscape with only a few big monolithic applications.   This will result in a number of important and in some cases new runtime related questions:  1. **How are all my microservices configured and is it correct?** Handling configuration is not a major issue with a few applications, e.g. each application stores its own configuration in property files on disk or configuration tables in its own database. With a large number of microservices deployed in multiple instances on multiple servers this approach becomes trickier to manage. It will result in a lot of small configuration files/tables spread all over the system landscape making is very hard to maintain in an efficient way and with good quality.  1. **What microservices are deployed and where?** Keeping track of what host and ports services are exposed on with a few number of applications is simple due to the low numbers and a low change rate. With a large number of microservices that are deployed independently of each other there will be a more or less continuous changes in the system landscape and this can easily lead to a maintenance nightmare if handled manually.  1. **How to keep up with routing information?** Being a consumer of services in a dynamic system landscape can also be challenging. Specifically if routing tables, in for example reverse proxies or the consumers configuration files, needs to be updated manually. Basically there will be no time for manual editing of routing tables in a landscape that is under more or less constant evolution with new microservices popping up on new host/port addresses. The delivery time will be far too long and the risk for manual mistakes will risk quality aspects and/or make the operations cost unnecessary high.  1. **How to prevent chain of failures?** Since the microservices will be interconnected with each other special attention needs to be paid to avoid chains of failure in the system landscape. E.g. if a microservice that a number of other microservices depends on fails, the depending microservices might also start to fail and so on. If not handled properly large parts of the system landscape can be affected by a single failing microservice resulting in a fragile system landscape.  1. **How to verify that all services are up and running?** Keeping track of the state of a few applications is rather easy but how do we verify that all microservices are healthy and ready to receive requests?  1. **How to track messages that flow between services?** What if the support organization starts to get complaints regarding some processing that fails? What microservice is the root cause of the problem? How can I find out that the processing of, for example, order number 12345 is stuck because microservice A is not accessible or that a manual approval needs to be performed before microservice B can send an confirmation message regarding that order?  1. **How to ensure that only the API-services are exposed externally?** E.g. how do we avoid unauthorized access from the outside to internal microservices?  1. **How to secure the API-services?** Not new or specific question related to microservices but still very important to secure the microservices that actually are exposed externally.   To address many of these questions new operations and management functionality is required in a system landscape not required, or at least not to the same extent, when only operating a few applications. The suggested solution to the questions above include the following components:  1. **Central Configuration server** Instead of a local configuration per deployed unit  we need a centralized management of configuration. We also need a configuration API that the microservices can use to fetch configuration information.  1. **Service Discovery server** Instead of manually keeping track of what microservices that are deployed currently and on what hosts and ports we need service discovery functionality that allows, through an API, microservices to self-register at startup.  1. **Dynamic Routing and Load Balancer** Given a service discovery function, routing components can use the discovery API to lookup where the requested microservice is deployed and load balancing components can decide what instance to route the request to if multiple instances are deployed for the requested service.  1. **Circuit Breaker** To avoid the chain of failures problem we need to apply the Circuit Breaker pattern, for details see the book [Release It!].  1. **Monitoring** Given that we have circuit breakers in place we can start to monitor their state and also collect run time statistics from them to get a picture of the health status of the system landscape and its current usage. This information can be collected and displayed on dashboards with possibilities for setting up automatic alarms for configurable thresholds.  1. **Centralized log analysis** To be able to track messages and detect when they got stuck we need a centralized log analysis function that is capable to reaching out to the servers and collect the log-files that each microservice produce. The log analysis function stores this log information in a central database and provide search and dashboard capabilities. **Note**: To be able to find related messages it is very important that all microservices use correlation id's in the log messages.  1. **Edge Server** To expose the API services externally and to prevent unauthorized access to the internal microservices we need an edge server that all external traffic goes through. An edge server can reuse the dynamic routing and load balancing capabilities based on the service discovery component described above. The edge server will act as a dynamic and active reverse proxy that don't need to be manually updated whenever the internal system landscape is changed.  1. **OAuth 2.0 protected API’s** To protect the exposed API services the [OAuth 2.0] standard is recommended. Applying OAuth 2.0 to the suggested solution results in:  * A new component that can act as a *OAuth Authorization Server* * The API services will act as *OAuth Resource Server* * The external API consumers will act as *OAuth Clients* * The edge server will act as a *OAuth Token Relay* meaning: * It will act as a *OAuth Resource Server* * It will pass through the *OAuth Access Tokens* that comes in the extern request to the API services  **Note:** Over time the OAuth 2.0 standard will most probably be complemented with the [OpenID Connect] standard to provide improved authorization functionality.   All together this means that the microservices need a infrastructure with a number of supporting services as described above that the microservices interact with using their API's. This is visualized by the following picture:  ![P3]  **Note:** To reduce complexity in the picture interactions between the microservices and the supporting services are not visualized.   In upcoming blog posts we will describe and demonstrate how the suggested reference model can be implemented, see the [Blog Series - Building Microservices].
 WebSphere Message Broker has since version 6 sported a Java API that can be used to perform various actions, primarily message transformations. Users of the API will quickly find that it consists of a low level object model, much like the W3C XML DOM. After some additional use, one will find that the API has some odd inconsistencies, for example will an object model built with the MRM and XML domains differ in how value nodes are created. After a while, most users will grow increasingly frustrated. As programmers we know that adding another level of indirection solves any problem. Thus we'll create a high level abstraction on top of the WMB API. All WMB shops I'm familiar with has done this. All solving their immediate needs.  Enter [wmb-util]. It's yet another such API, but this time it's open source and freely available for anyone to use under a commercially friendly license. It sprung out of a project we did for a customer and together with the customer we opted for opening up the project rather than creating another proprietary version. For us this means we can reuse it in other customer projects. For the customer it means they'll get fixes for free, either from our other customer projects or from other users of the library. Hopefully all wins.  So, what does the library contain? The most important part, and the part I'll cover today, is that [very abstraction of the WMB API]-level classes for a few, typically message types:  - Headers (MQMD, RFH2, Properties, HTTPInput - XML messages - SOAP messages - Simple record based flat files  - Blobs  More messages can be added as need be. Each of the header and payload classes contains a set of static factory methods, used to create, wrap or remove messages. The classes work directly on the underlying WMB object model, meaning you can mix wmb-util code with use of the WMB API directly as you like. For example if you like to perform an XPath query and then use the TdsRecord to perform actions on the resulting elements.  The API aims to make the client code completely portable between the different message domains. More specifically, it should not have any imptact on your code if you use MRM, XML, XMLNS or XMLNSC.  Enough talk, let's look at some examples:  In this case we create a MQ message containing a simple XML payload from scratch   As seen above, you won't have to worry about things like domains, node types, weird ways of setting up namespace prefixes or that WMB will in some cases use an at-sign in front of attribute names. And you don't have to look up what the minimal attributes needed to create an MQMD header are. You have been copying the input message instead, haven't you?  We invite you all to use and participate in the project. All feedback is welcome, and patches double so.
In this 2-part blog we'll take a look at how I used the go profiling and built-in benchmarking tools to optimize a naive ray-tracer written in Go.  [Click here for Part 2].  1. Introduction 2. Why Go? 3. Multi-threading 4. Using pprof to identify bottlenecks 5. Reducing allocations & Caching 6. Conclusion of part 1  __  The full source code for my little ray-tracer can be found here: [https://github.com/eriklupander/rt]  Sometime late 2019 I stumbled upon a book - ["The Ray Tracer Challenge"]:  ![book]  What is ray-tracing? Let's quote wikipedia:  _Source: https://en.wikipedia.org/wiki/Ray_tracing__  Professional ray-tracing can produce quite photo-realistic results: ![glasses] _Source: https://en.wikipedia.org/wiki/Ray_tracing_#/media/File:Glasses_800_edit.png_  While ray-tracing has approximately 0% relation to my daily work, I've been interested in computer graphics ever since the mid-late 80's when I wrote my first GW BASIC graphics code and played any computer game on the family 286 I could lay my hands upon.  Ray tracing has always been something of my personal holy grail of computer graphics. I've toyed around with rasterization in the past, doing some OpenGL and shader stuff just for fun, but given the mathematical nature of ray tracing, I've never really even tried to wrap my head around it. You can never guess what happened next when I discovered the book above...  A few words on the book that launched me onto the path of writing my very own ray-tracer. The book in question takes a fully language-agnostic approach to building your very own ray-tracer from scratch. The book does a really good job expaining the physics and mathematics involved without drowning the reader in mathematical formulas or equations. While a number of key algorithms are explained both using plain text and imperative psuedo-code, the heart of the book is its test-driven approach where concepts are first explain in text, and then defined as cucumber-like Given - When - Then test cases. Example:   Everything from intersection math, rotations, linear algebra, shading to lighting calculations has these test cases to keep you as the reader of the book on the correct path. Which is enormously important, best stated in the book in regard to getting the basics wrong early on:  "... it will cause you a never-ending series of headaches"  _- Jamis Buck, The Ray Tracer Challenge_  Indeed - if you'd get some fundamental piece of the core math or intersection algorithms wrong, later use of those functions  will be off and very difficult to troubleshoot.  So, the book takes you step by step from building your core math library, rendering your first pixels and grasping the core concepts - to actually rendering various shapes including lighting, reflections, refraction, shadows etc. By the end of the book, your renderer is capable of creating images such as the one below: ![alt] _Gopher model by [Takuya Ueda]_  That said - the purpose of this blog post is _not_ teaching you how to create a ray-tracer - buy the book instead. This blog post is about Golang and my journey after finishing the book making my renderer perform better. As it turned out, my performance optimizations cut the time for a single render by several orders of magnitude!   ![alt] While not very impressive given 2020 graphical standards, this "reference image" of mine showcases a number of different object primitives, shadows, reflections, refraction, lighting, patterns and transparency. This is a high-res render, while the resolution used for all performance comparisons were 640x480.  Why not? Ok - after doing Java for 15+ years, I've been working full-time with Go projects for the last year or so. And I simply love the simplicity of the language, the fast and efficient tooling and how the language manages to make our team able to efficiently write stable, performant and correct services without having the language or its abstractions getting in our way very often.  I'll also admit - if I had wanted to write the most performant ray-tracer from the very beginning, I probably should have done this in C, C++ or Rust. However, given that _my_ challenge was learning how to do simple ray-tracing, I'd rather not have to deal with learning a programming language I'm not comfortable with. This point is actually rather significant, as it turned out I've probably spent more time on my ray-tracer _after_ finishing the book doing the performance optimizations this blog post is about than I spent with the book. Nevertheless, it's quite probable I'd run into many or most of the issues in whatever language I'd picked.  The _cucumber_-like tests from the book was implemented as standard Go unit tests. My implementation using Go 1.13 used no 3rd party libraries at the time I completed the book.  This blog post is about improving the Go-related code of the renderer. However, I must clarify that a huge part of creating a performant ray-tracer is actually about making it "do less work" through optimization of the actual logic of the renderer. Ray-tracing is mainly about casting rays and checking if they intersect something. If you can decrease the number of rays to cast without sacrificing image quality, that is probably the greatest improvement you can do.  An example of such an improvement is using something called [Bounding Boxes], where a complex object is encapsulated inside a "cheap" primitive, which can speed up rendering of complex meshes by several orders of magnitude:  ![alt]  __  A complex mesh such as the head in the image above may consist of hundreds of thousands of triangles. Without optimizations, a naive ray tracer would need to do an intersection test for every single triangle for for every ray cast in the scene, including shadow and reflection rays. That would make rendering of complex scenes quite infeasible on commodity hardware, where a single render could probably stretch into days or weeks to finish. Instead, by putting the complex head mesh inside a virtual "box", a simple ray-box intersection test for each ray cast will tell us whether we need to test all the triangles of the head or not. I.e. - if the ray doesn't intersect the box, it won't intersect any of the thousands of triangles either so we can skip them.  Further, we can subdivide the head into many smaller boxes in a tree-like structure we can traverse and only do intersection tests for the triangles bounded by the leaf "box". This is known as [bounding volume hierarchies].  ![bvh] _Source: Wikimedia: https://commons.wikimedia.org/wiki/File:Example_of_bounding_volume_hierarchy.svg)_  As said above - these optimizations are tremendously important for complex scenes with many and/or complex objects. While I've implemented Bounding Boxes and BVHs in my renderer based on one of the online [bonus chapters] of the book, for the simple "reference image" used for benchmarking in this blog post, the BVH is ineffectual since the reference scene does not contain any grouped primitives.  Once I was done with the book, I could render simple scenes such as the "reference image" on my MacBook Pro 2014  in 640x480 resolution in about 3 minutes and 15 seconds. This made further experimentations with multi-sampling, textures or soft shadows a very slow process, leading to me embarking upon my journey of optimizations.  At a glance, ray-tracing is more or less about casting a ray through each would-be pixel through the image plane, and then applying the exact same algorithms for determining the output color of that given pixel. In theory, this sounds like an [embarrassingly parallell problem].  Therefore, my first optimization was to move from a purely single-threaded rendering process to something that would utilize all available CPU threads.  So, this is where the Go fun starts. I decided to apply the simple [worker pool pattern] that I've touched on before on this blog.  In terms of Go code, the renderer treats each horizontal line of pixels as a unit of work that one of the workers can consume from a channel. The number of workers is set to GOMAXPROCS, matching the number of virtual CPU cores, 4/8 in my case.  The actual rendering code  just picks one "job" at a time from the channel and passes it to our  method:   Why use one line as the unit of work? Well, actually, I started passing one pixel at a time. However, if you're rendering a 1920x1080 image, that's approx 2 million "jobs" to pass over that channel. I noticed in a CPU [profile run] that a large portion of time was being spent in this  method:  ![alt] __  I couldn't quite make sense of this, other than the profiling tools seemed to think a ridiculous amount of time was being spent waiting for something. A block profile shed some additional light on the issue:  ![alt] __  Seems the  method spent a whole lot of time being blocked. By performing the tiny change of passing a line at a time instead of a single pixel as the unit of work, things started looking much better:  ![alt] __  In this particular example, the block in  went from 34.3 seconds down to 0.6, while the  actually increased from 4 to 8 seconds, clearly indicating the we improved things a lot while simultaneously moving the bottleneck to the sender side. The CPU profile also looked much better now:  ![alt] __  Overall - one should notice that this change didn't cut the total render time in half, it yielded an overall improvement of perhaps 5-7% - but it's a good example on how an issue could be identified using the Golang profiling tools not directly related to mathematics or ray-tracing algorithms, rather a problem with the internal app architecture and how "jobs" were distributed to the "workers".  As I quite quickly noticed, moving the codebase from strict single-threaded operation to using a worker pool introduced a whole new slew of problems to solve. State and shared object structures was definitely one of the more challenging ones.  What does the code that renders a single pixel need to do its job? Well, it needs to access all geometry and light sources in the scene, as well as knowing where the "camera" is. All that stuff was implemented as plain Go structs, including quite a bit of state that at times was being mutated by the renderer in order to avoid allocating memory for intermediate calculations.  This was actually a bit harder nut to crack than I initially thought. Since I didn't fancy rewriting everything from scratch, and using mutexes everywhere would both be error-prone and probably wouldn't scale all that well - I decided to let each "render thread" get a full copy of the entire scene to work with. As long as the scene doesn't contain a lot of really complex 3D models, in this day of 16GB laptops keeping N number of "copies" of some structs in-memory isn't a big issue.  The final declaration of a "Render Context" is implemented as a Go struct and looks like this:   The renderer instantiates GOMAXPROCS number of these "render contexts" that can autonomously render any pixel in the scene. Perhaps not the most elegant of solutions, but it provided safety and did definitely solve some of the rather strange render errors and other problems I did get before moving away from "shared memory".  Whenever a pixel had been completed, it was directly written to the  through a mutex-protected method. This mutex did not become any kind of bottleneck.  So, how did going from 1 to 8 "workers" affect performance? While I didn't expect my renders to become 8 times faster, I was kind of expecting maybe a 5-7x speedup.  I got about **2.5x**.  The reference image was down to about **1m30s** from **3m15s**. This was a huge disappointment! But on the flip side, it really got me intrigued about what was holding the CPU back. I'll revisit this in the final section of the second blog post of this mini-series.  What do you do when your code doesn't perform like you expect? You profile it! So, I added the requisite boilerplate to enable profiling:   I guess the finer details of [pprof] is better explained elsewhere, so let's jump directly to what the CPU profile showed after enabling multi-threading:  ![alt]  These profile PNGs are awesome for seeing not only where the most time is being spent  but also the call hierarchies. Seems about 25% of the CPU time was being spent on , which in plain english means "stuff the Garbage Collector is doing". While my naive ray-tracer definitely wasn't written with memory re-use in mind, this did seem a bit excessive.  Luckily, the profiling tools also provides heap profiling capable of both tracking the _amount_ of memory being allocated down to LoC level, as well as showing _the number_ of allocations occurring in any given function or LoC.  Time for a big image:  ![alt]  Yes, look at that  function. Over 95% of all memory allocations are happening in it's descendants! A closer look:  ![alt]  Yup - someone down that call hierarchy is performing about 3.7 billion allocations for rendering a 640x480 image!! That's more than 12000 allocations per pixel being rendered. Something was definitely rotten in my own little kingdom of ray-tracing.  While "inverse the transformation matrix" sounds like something Picard could say in Star Trek, it's one of those fundamental pieces of math that our ray-tracer needs to happen when moving from "world space" to "object space" when performing a ray intersection test on an object. Since we're testing all objects in the scene on every ray being cast, including reflections and shadows, this adds up to an enormous amount of matrix math that needs to happen.  __  The good news is that we actually needed to calculate the inverse transformation matrix for our scene objects exactly **once**. So once we know where in world space our camera and scene objects are, we can calculate their inverse transformation matrices once and store the result in their structs:   In practice,  above is only called once when we're setting up the scene.  This was a trivial optimization to code, but it made a _huge_ difference: The time needed to render the reference image went from **1m30** to about **4.5** seconds!! A single threaded render went from **3m15s** to about **10s**.  Looking at allocations, we went from 3.8 billion allocations to "just" 180 million and the total amount of memory allocated went from 154 GB to 5.9 GB. While we certainly saved a significant number of CPU cycles for the actual math of calculating the inverse, the big part of this optimization definitely was easing the pressure on the memory subsystem and the garbage collector since calculating the inverse required allocating memory on each invocation. The  function is now used so sparingly it doesn't even show up when doing a heap profile.  3.7 billion down, 180 million to go? If I had the time and will  I'd probably rewrite everything from scratch with a "zero-allocation" goal in mind. Keep things on the stack, pre-allocate every piece of memory needed to hold results, check inlining etc.  I should perhaps mention that when I begun implementing things based on the book, I used a "everything should be immutable" architecture. While that did really help with correctness and avoiding nasty bugs due to  mutation, allocating new slices or structs on every single computation turned out to be a quite horrible idea from a performance point of view.  Since I didn't want to start over , I took an approach of trying to identify one big "allocator" at a time in order to see if I could make it re-use memory or keep its allocations on the stack. Heap profiler to the rescue!  ![alt]  Looking at this new heap profile, the biggest culprit seems to be this  function responsible for almost 50% of the remaining allocations. It's a quite central part of the ray-object intersection code so it's being called often. What does it do?   Yes, it's just a simple function that multiplies a 4x4 matrix with a 1x4 tuple and returns the resulting tuple. The problem is the :   Oops. It's creating a new slice on _every_ invocation. That's not always desirable in a performance-critical context.   What we would like to do is to have the calling code pass a pointer to a struct in which we can store the result, and hopefully the calling code can reuse the allocated memory or keep it on the stack.   Easy peasy - a new third parameter passes a pointer to a  and we're down to zero allocations. However, there's a bit more to it. The most typical usage is in this code:  In this snippet, we're creating a new transformed Ray ` call here won't help us at all. No, this requires a bit more refactoring so the whole call chain uses the C-style pattern of passing the result as a pointer.  This implementation passes pointers to the underlying  directly to the , which stores the results directly in the passed  pointer. As long as the code calling  has allocated `out *Ray` in a sensible way, we've probably been able to cut down the number of allocations in a really significant way. The Ray in this case is something that can be safely allocated once _per render context recursion_ and can be pre-allocated.  I won't go into the exact details on how pre-allocating memory for that  works, but on a high level each render goroutine has this "render context" mentioned before, and each render context only deals with a single pixel at a time. The final color of a single pixel depends on many things, in particular the number of extra raycasts that needs to happen to follow reflections, refractions and testing if the point in space is being illuminated. Luckily, we can treat each new "bounce" as a new ray so as long as we have allocated enough "rays" in the context to support our maximum recursion depth, we're fine with this approach.  We can also use Go's built-in support for microbenchmarking. Consider these two benchmarks:  Output:  In this microbenchmark, the latter is more than twice as fast. __  The results obtained in this section clearly indicated that in order to improve efficiency through reuse of memory, pre-allocating data structures and reusing them and caching things whenever possible was a key component.  A few more examples where pre-allocating memory gave really nice benefits:  There's a lot of intersection lists to keep track of "hits" when casting rays through a scene. It turned out that the intersection code for each primitive  was creating a new  slice on each invocation:  This is the intersection code for a . Note how a fresh slice is created and populated at the end of the method if there were an intersection. This pattern repeats itself for all primitive types.  However, for each render context we also know that once we have "used" the intersection of a given primitive, we can safely re-use the same slice for that primitive for any subsequent ray / primitive intersection test. Therefore, a simple pre-allocated slice of sufficient length on the primitive created a large saving in allocations:  Why a slice of size 1? A  can only be intersected exactly once by a ray. Other primitive shapes may have more intersects by a ray. A  may be intersected twice , while  are worst with up to 4 possible intersections.  Another nice optimization was to look at the top-level render context - what was being created anew for each new pixel being rendered? A lot, it turned out. In its final form, these were things we could pre-allocate and re-use:  Each  contains pre-allocated structs needed for a single recursion.. I won't go into more implementation details, but as one can see there's quite a few of these "pre-allocated" structs or slices and the complexity of the solution did indeed become significant once I realized how the recursive nature of the ray-tracer affected things.  There's more of these, but in order to not make this section even longer, the key take-away is that while immutability is a really nice thing to strive for in general computer programs - for really performance critical software, re-using memory, caching and avoiding allocations might be a good thing...  This is a simple one. Many intersection lists were being re-used after a while by explicitly setting them to  or even doing  on them again .  Instead, slicing a slice to zero length retains the memory previously allocated for its contents, but allows adding items to it from its 0 index again:    The changes described above was just a few examples of many optimizations where allocations in computational functions was replaced by letting the caller pass a pointer to the result, memory was pre-allocated and slice-memory was being reused. In the end, the results was approximately the following:  * Duration dropped from **~4.5** to **~1.9** seconds * Allocation count dropped from **180** million to about **33** million * Total memory allocated went from **5.9** GB to about **1.3** GB  This sums up the **first part** of this 2-part blog series on my adventures optimizing my little ray-tracer. In the [next and final part], I'll continue with more optimizations.  Feel free to share this blog post using your favorite social media platform! There should be a few icons below to get you started.
 We are often asked to define WSDL- and schema design guidelines .
This autumn I attended a course called IBM WebSphere Message Broker V6 Application Programmer Workshop. Just the name is a bit too long to cope with :-). The course was an introduction of WebSphere Message Broker integration platform and the belonging development toolkit. The course introduced a lot of concept and functions with lab exercises. The likeness to be using all these functions is small, but of course it's good to get an understanding of the range of functions in the toolkit. An understanding how to use the toolkit and discussions about areas that could be a problem for organizations using any integration platform would had been more helpful and interesting.  The debugging tool is necessary in the development for message flows; during the course you get an introduction on how to use it but no tip on how to avoid problems during the debugging session. When you are using it frequently, and I promise you will, the debugging tool will eventually NOT trigger on breakpoints, it will hang and not start up properly. The deployment is running in a so called execution group, most of the times it helps to just restart this. But every once and twice you need to restart the whole environment, including the toolkit and the IBM Agent Controller, the debugging event service process running in windows. A redeploy of the message flows will also be in order every time you need to start up the debugging tool proper.  A deployment file is called .bar file, Message Broker Archive. Each time you do a refresh on the deployment file after a rebuild the file size increase rapidly, caused by service and user log files that gets updated. If you don't delete these logs the deployment step will eventually take forever and ever. This is a small thing to do but is not obvious and should be mentioned in the course.  In large organizations there is also another level of complexity - do we speak the same language? If I'm talking about a red ball - is that the same red ball that you are referring to? To solve the problem in the projects I've been working in, major work has been done developing a common message format to secure that the same language is used on the platform. This is not even a subject in the course and I think that this could be a large problem for organizations that is about to use any integration platform where there exist a mix of legacy systems, COTS applications and java applications.  My point is that it's hard to find a good course that is really useful in the terms of handle tools and use it efficient. I guess it's hard for suppliers to talk bad about there own products, but somehow it get obvious anyhow sooner or later - so why not be open and talk about the problems and have a open mind about it.
In this part of the Go microservices [blog series].  1. Building with Makefiles 2. Deploying with Docker Stack 3. Integration testing 4. Summary   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P17  While most compiled languages provides a compiler, developers usually rely on some kind of framework or library to streamline their build process, manage dependencies, run tests and so on. For example, in the Java world, we've been using ant, maven and more recently gradle for our building needs.  In Go we have a rich set of built-in tools for compiling, dependency management, linting, formatting and testing our code etc, but as far as I know, the most popular choice for build automation among Go developers is plain old [Make] originating from 1976!  I won't go into details about the inner workings of Make  to automate my most common Go build-related tasks.  A Makefile in it's simplest forms allows us to define a number of variables and a number of _tasks_ such as "test" for running your unit tests, "build" for building or "lint" for running your linter.  I typically start each of my Makefiles by defining a variable name of my executable:  executable := dataservice  This variable can then be used in my tasks.  As an example of a really simple task, this one runs all my Go unit tests:  test: go test ./...  I.e - it will just execute "go test ./...". For the "dataservice" it looks like:  $ make test go test ./... ? github.com/callistaenterprise/goblog/dataservice/cmd [no test files] ? github.com/callistaenterprise/goblog/dataservice/cmd/dataservice [no test files] ? github.com/callistaenterprise/goblog/dataservice/internal/app/dbclient [no test files] ? github.com/callistaenterprise/goblog/dataservice/internal/app/dbclient/mock_dbclient [no test files] ok github.com/callistaenterprise/goblog/dataservice/internal/app/service 0.030s  A more advanced example is my "build" task that first builds a linux/AMD64 executable into the _bin/_ folder and then executes a _docker build_ command to build a local Docker images .  build: @echo Building $ GOOS=linux GO111MODULE=on go build -o bin/$/main.go docker build -t someprefix/$ -f Dockerfile .  One can also combine several tasks together:  all: clean fmt test $  clean: @rm -rf bin/*  $: @echo Building $ GO111MODULE=on go build -o bin/$@ cmd/$@/main.go  As you may notice, we're using the _$_ variable and a task named after the variable in order to build a binary for the current OS/Arch.  Ultimately, Makefiles provides a really simple way to group one or several commands into tasks, who then also can be grouped together as prerequisites for running other tasks etc.  The source code for this blog series currently consists of 4 discrete Go-based microservices: accountservice, imageservice, dataservice and vipservice. Naturally, we want to build them as uniformly as possible and we would also like to build them all from a parent Makefile.  I've placed this "parent" Makefile in the _/goblog_ root folder. For example, the task for building all microservices looks like this:  build: $ -C accountservice/ build $ -C dataservice/ build $ -C imageservice/ build $ -C vipservice/ build  This will execute the _build_ task in each of the specified  variable makes sure all "sub-tasks" are executed using the same make binary as the top-level invocation.  Make is quite powerful, but arguably some of the more advanced features are not that easy to grasp. So for more advanced tasks or scripting needs, I usually resort to using bash shell scripts invoked from make tasks rather than trying to unleash the full power of Make.  Most projects - if not all - tests their code and services in some manner before going into production. Unit-tests running as part of local builds has been around for a really long time, a topic visited in [part 4] supporting services being available during the test. With the automation and Continuous Integration aspect of integration testing, you end up in a situation where you need to both run your actual microservice under test, but also need to run whatever services your microservice depends upon. For example, the dataservice in this blog series rely on CockroachDB to be available, and it will also need a configuration server in order to fetch it's configuration on startup.  Managing all of this was quite challenging before the advent of  network, where your test would seed/cleanup data for every test suite executed for a given application.  However, with docker containers, it has become relatively straightforward to provision a full isolated "stack" of services for a given test suite by declaring and running everything in docker containers, orchestrated by some container orchestration mechanism.  In this blog post, we'll use docker-compose in order to integration-test the HTTP endpoints of our "dataservice" component. We will also use go's built-in test functionality to write the actual tests, including seeding of test data and asserting results. We'll also use [mmock] for creating an HTTP mock that can be programmed to response with a pre-baked HTTP response for a given HTTP request such as the HTTP request for getting its configuration.  ![test landscape]  // Erik
What do you get when you try to say "Unit Test" and "Utilities" very fast? Unitils, of course! This new Open Source project gathers most of the productivity utilities and refactorings of typical JUnit/DbUnit/EasyMock code that most projects develop for internal use, over and over again .  What about reflection-based assertions, to avoid the need for equals implementations that only makes sense for unit tests:  Or DbUnit-based prepopulation of test data using annotations:  Or creating an EasyMock mock object for a dependee and inject it into the unit under test, also using annotations:  Its funny that we tend to solve those minor, small inconveniences over and over again in an ad-hoc, project-specific manner, not always realizing that exactly the same inconveniencies will bother us again in the next project. Hence even if they're minor, they are real time theives, and deserve a well thought out solution. Unitils is such a well thought out solution.
  The [Spring2GX 2015] mantra is Get Cloud Native, but what does it mean?  -[readmore]-  Many of the talks at [Spring2GX 2015] is a collection of characteristics that describes elements of a cloud native application. These characteristics should meet the requirements of a modern application with demands like:  * variable workloads * servers coming in and out of existence * zero downtime deploys * short time to production for new features * expectations are more dynamic * high cohesion and loosely coupling  For **cloud platform providers** these [twelve-factor] characteristics helps to build mechanisms that is needed for cloud native applications, such as  * consistent provisioning * high visibility of health and logs * making the system handle well under stress  For [DevOps] these characteristics helps to build, test and run applications in isolation with explicit isolated dependencies. It also enables to build systems that:  * is easy to monitor and trace * is possible to get to production faster * makes production the happiest place to be  For **third party providers** It also gives possibilities to build tools like application monitoring and log monitoring to be integrated through the published and versioned API’s.  When it comes to **security** though, the [twelve-factor] comes a little bit short handed. Only in two places can you read some information that can be related to security, in chapter III. Config and IV. Backing Services, but very vague. In a collection of cloud native application patterns describing characteristics for applications in such a heterogeneous environment as the cloud, some more explicit information in security would be expected.
XMPP, aka Jabber, is making great strides into the world of instant messaging. Since Jeremie Miller released the first version in 1998, it has been the obvious alternative for those preferring open protocols over the proprietary networks pushed by ICQ, AOL, MSN and others. With big guys like Apple and Google using XMPP it began making noise in the corporate world. Now, with AOL  looking at XMPP, the road to world dominance seems clear ahead.  So what's in it for us, the application developers? One example is keeping an eye on our applications. XMPP provides two perfectly fitting concepts:  - Presence - what modules are up and running - Messages - communicate with your application  Let's have a look at a simple example. Note this is a demo code, quality code might use things like clever things like exception handling and external configuration. But for now, this will do.  First of all, you need two Jabber/GTalk accounts. Go create them , make sure they are buddies and get back here... back already? That was fast.  Now, let's write a simple OSGi bundle that sends its presence based on the state in its life cycle. Feel free to imagining how this would work with Spring or EJB life cycle if that's tickles you more. OSGi has what is called a BundleActivator that keeps track of when a bundle is started and stopped . Let's create an implementation.  Choose one of many XMPP clients available in Java. One of the easier is the Smack client from Jive^H^H^H^H Ignite Realtime, also available for all your Maven needs. Let's create and connect one when our BundleActivator is created:  I did tell you this is demo-quality code, right. If this were production code you would certainly want to inject that connection into your code rather than hard coding it. Fine, now, let's show when the bundle goes active or is stopped by indicating our presence:   We're almost done. Why not get a message that tells us that the bundle has started and all is good. This is the complete code listing:  We're done with coding. An OSGi bundle requires some extra attributes in the manifest but I won't describe them in any detail here, go read any of the OSGi tutorials or the surprisingly readable specification if you want to understand them.  Now package all of that into a JAR, including the Smack JAR, and fire up your favorite OSGi implementation. You do have a a favorite one, right?  For me, that means the Apache Felix shell:  Start the bundle by running "start 4" and you should see the Jabber user getting online and get a message saying that everything is okay. Stop it with "stop 4" and you should get a message and see it go offline. Imagining having that with all your components and not having to depend on that monitoring system or wading through log files.  For all you OSGi geeks, have a go at wrapping this code into a service that all your bundles can use. That's what I do.  Seems useful? Got any ideas as to where you could use XMPP communicating with you?
 This is not new for you guys that already know Scala. I attended a Scala tutorial last week and having worked exclusively with java 13 years it was a really good experience. My current project has a lot of code that deals with collections. Filtering, sorting, matching, manipulating and creating lookup caches is done en masse. Feels like this would have been much easier to accomplish if we could have used Scala. I think it is the combination of a richer api and the support for closures that make the difference.  It is not a problem to do any of these thing is Java but I think that the way you can do it in Scala is much more compact, readable and less likely to create errors.  -[readmore]-  Some very simple samples. 
So, what is an ontology? In the broadest sense, an ontology is knowledge representation, symbolically encoded as to allow for computerized reasoning. Simplified: to use the terminology [from the previous post], an ontology describes concepts and their relation to other concepts using a formalized language. This enables powerful computerized “thinking”, but creating a well-formed ontology is a big task.  -[readmore]-  There are a multitude of concepts that are similar: ontology, classification, taxonomy, nomenclature, thesaurus, controlled vocabulary, enumerated type and code system , to name a few. I won’t differentiate these, but instead I will try to pick them apart.    The first thing to notice is that I’ve . Ontologies are about semantics, so from this point on we’ll stick with the old Bill Gates adage: content is king.  The second thing to notice is that content in this case means a fixed set of concepts to choose from in a given situation. This is basically what we call a controlled vocabulary. These are used for catalogization of information and making data entry consistent in order to make information retrieval and reusage easier. In the simplest cases, like for most cases where enums are used, the controlled vocabulary is an ad hoc list, which is fine for  - single purpose uses  - where the content is well defined  - where the content is definitionally complete .  In these cases, the overhead of creating a full ontology is a waste of time. Please note that I’m not talking about the size of the content.   Vice versa: - The larger your scope of usage and the less control you have over the circumstances in which the information is used - The larger and more complex your subject area and the less well defined the domain - The more prone to change some set of information is the likelier it is that an ontology will be a good match for your needs.    These dimensions aren’t black and white, each one is a sliding scale. Again, let’s use a simple example to concretize - you run an online community, and one piece of information the members can divulge about themselves is their nationality. For this purpose, you need to create a list of all the countries in the world. This will be a list of some 200 countries.  ![people_world_map.png]   The task of creating a list of the nations of the world isn’t complex, it’s a one-dimensional list. As opposed to, say, a list where your users can enter information about their occupation .  Although the subject area of “all the nations of the world” is decently well defined, there are still some contested territories. This means you need to choose which nations to include with some care. Case in point: if you include Taiwan in your list, you will make many Chinese people angry. If you don’t include Taiwan in your list, you will make many Taiwanese people angry.  Lastly, this list will need to be maintained. Changes over time are small in this case, and the ramifications of not instantly keeping up to date are likely small. These are both arguments for an ad hoc list. Other types of data, where there is constant change, might benefit from an ontology approach.   If your scope is simply to let a user enter nationality to display on a member profile page, this is what I call single purpose use. Enter once, display in one place. If you also want to display a flag next to this information, you’re still close to single purpose use - enter once, use twice.  When you want to view aggregate user statistics or personalize , you get further away yet. You are still in control over how your information is used. Any problems that may arise in integrations, you yourself can manage, so this is still some kind of middle ground.  Finally, when your small community is no longer that small, and you release API:s for outside app developers to use aggregate information about your members, you’ve deep into multipurpose use territory.
 According to my colleges that work with [frontend technologies] when it comes to communicating with backend server applications.  WebSockets simplifies much of the complexity in HTTP for bi-directional communication required by single page web apps. With its support for full duplex communication over a single socket it enables both client initiated [request/response] in a natural and simple way. Added to this the WebSocket protocol have a much lower overhead compared to the HTTP protocol resulting in lower latency. With its support for full duplex and low latency communication WebSocket opens up for true realtime communication, not feasible with HTTP.  With the growing support for [WebSocket in web browsers] it is now becoming more and more interesting to look into how to start to use WebSocket instead of HTTP.  In a previous [blog post], like:  ![]  In this blog we will focus on the case where we  don't want to invest in a WebSocket enabled integration platform or ESB but just want to WebSocket-enable a single Java based server application:  ![]  The blog is divided in the following sections:  - Requirements - Meeting the requirements - Source code - A test run - Summary - Next step  Requirements for the scope of this blog entry are:  Web apps must be able to send requests asynchronously allowing the user to do other things while the server process the request and also allow the server to send information to the web app without the web app asking for it in advance. To support remote mobile devices with potential limited bandwidth it is also important that the communication have minimal overhead built into the protocol to be able to communicate in realtime.  Added to the traditional request/response model we also want to be able to notify users as soon as any interesting happens on the server side without having to wait for polling solutions or forcing the user to refresh the user interface to see if any interesting have happened since the previous refresh...  Exposing enterprise applications to large user groups on their mobile devices is expected to raise requirements of handling much larger numbers of concurrent users on the server side then before.  Given that we don't want to bring in a full fledged integration platform or ESB in between the server and its clients we need to ensure that the server application itself can be enhanced with communication capabilities that meets the requirements above without draining the server hardware or requiring extra server processes to be setup, managed and monitored separately from the application itself. Otherwise, in the end, that could easily lead to a solution that is equally complex as introducing a separate integration platform!  Added to these requirements we have defined the following constraints:  1. Target application technology on the client side are HTML5 based single page web applications.  It could actually be any type of WebSocket enabled technology on the client side, such as native iOS or Android apps, but we have limited this blog to HTML5 web apps. 2. Server applications are expected to be based on Java SE. It should however be very interesting to cover other alternatives, such as [node.js], in a future blog.   Historically HTTP has been used with various success to meet these requirements, e.g. using AJAX, Reverse AJAX and Comet technologies. But HTTP was never designed for this purpose, HTTP is based on a half-duplex model initially targeted for fetching documents and added to that with a substantial communication overhead. WebSocket on the other hand is explicitly designed for full duplex communication with a very low overhead enabling realtime communication even over wireless connections with very limited bandwidth. WebSocket is clearly the technology for the future to meet this requirement!  > **Decision #1:** Use WebSocket for realtime full duplex communication.  WebSocket, with its fully asynchronous and duplex communication over a single socket, however neither understand nor support message exchange patterns such as request/response or publish/subscribe. An asynchronous version of the request/response model is however very simple to implement using WebSocket. One concern could be how to correlate incoming responses on the client side to the originating requests in the case where more than one request can submitted concurrently from one and the same client. Using some kind of correlation id in the message payload however solves the problem in most cases.  When it comes to the publish/subscribe model it becomes tricker since WebSocket have no understanding of critical concepts such as subscribers, publishers and topics. We clearly need a protocol on top of the WebSocket protocol to handle publish/subscribe!  A good thing is that WebSocket is prepared for these types of extensions with its support for subprotocols. Clients and servers can agree upon use of subprotocols during the connection phase . Even tough we here have the opportunity to create our own publish/subscribe subprotocol we prefer to use something already existing and well established!  Commonly used open  while MQTT is a more advanced binary protocol with more features and potentially higher performance due to its binary nature. Which one to prefer is hard to say and will depend on case specific requirements, STOMP being very simple and easy to learn while MQTT being more functional and potentially better performing but with a steeper learning curve. Let's look into both!  > **Decision #2:** Complement the WebSocket protocol with the subprotocols STOMP and MQTT to support the publish/subscribe model.  When it comes to handling large number of users, e.g. thousands of concurrently connected user, it has historically been quite complex requiring lots of server hardware and carefully configured communication software. This is mostly depending on the fact that a blocking I/O model was used, locking a separate thread per connection or request. This quickly drain the server hardware resources when the number of concurrent requests rises. A much better approach to handle large number of concurrent users are a non blocking I/O model allowing one thread to handle many concurrent requests. For more background information see [the C10K problem].  The Java Runtime Environment supports non blocking I/O since J2SE 1.4 with a library called "New I/O" or "NIO", later enhanced in Java SE 7 with "NIO.2". But we also need a framework on top of it that handles WebSocket based on Java NIO.  > **Decision #3:** Base server solution on Java NIO.  As described above we don't want to bring in a complex and resource demanding solution on the server side. In that case we actually should take the integration platform/ESB path from the start enabling other server applications to benefit from it as well.  A number of Java based web servers and frameworks with support for WebSockets exists today. The two most widely used in my mind are the open source projects [Jetty] and Netty a lightweight framework optimized for supporting development of asynchronous event-driven network applications. For the purpose of this blog, i.e. setting up a embeddable lightweight but highly scalable Java server for asynchronous WebSocket communication, Netty became a natural choice.  Netty supports however neither of the messaging protocols STOMP and MQTT. Seems like we need a messaging product to complement Netty for the publish/subscribe part!  [Apache ActiveMQ] but it can be configured for a very slimmed down execution where it runs embedded inside a server application using the internal VM protocol instead of TCP. If not required you can also disable resource demanding features such as transactions, persistent messages and durable subscribers to make the configuration even more lightweight.  > **Decision #4:** Use Netty and ActiveMQ as embeddable frameworks in the server application.  This leads to the following architecture:  ![]  Well, that was a lot of theory! Let's look into some code examples of how this can be implemented instead.  The full source code is available at github: [https://github.com/callistaenterprise/websocket-labs/tree/master/ws-one].  The source code uses Maven as build tool so to get the code and build it simply perform:   After the Maven build is done a server application based on [Java Service Wrapper] is created under the folder .  If you want to run the server application in the development environment without running it as a Java Service Wrapper simply perform:   After the server application print out a log message like:  ...you can visit the url [http://localhost:8081/target/classes/web/index.html] in a WebWocket enabled web browser to see the web client. Try to send requests with the commands ,  and . More about what happens in response to these requests is explained below, so read on!  The server application is based on a simple Main-class demonstrating how Netty and ActiveMQ can be embedded in an existing Java application. The Main-class also creates a JMS publisher for the server application to use whenever it wants to push information to the WebSocket clients over STOMP and MQTT. Finally the Main-class also use Netty to provide a HTTP based static file server to host the HTML and Javascript files used by the web app .   Initiating ActiveMQ with VM-transport for internal communication and WebSocket for external communication is as simple as:   **Note #1:** We don't need to initiate the VM-transport, it is enabled by default.  **Note #2:** Support for STOMP and MQTT over WebSocket comes out of the box with the WebSocket - transport.  Initiating Netty is a bit more involved compared to ActiveMQ but follows a standard pattern that is very simple to reuse:   1.  creates a Java NIO based channel. 2. `new WebSocketServerProtocolHandler` initiates WebSocket on the Java NIO based channel. 3. `new CustomTextFrameHandler` adds an application specific request handler.  The -class picks up a WebSocket request as a string and hands it over to a request handler method, , that that is totally unaware of Netty. The request handler returns the response as a string and -class writes the response back to the WebSocket client:   **Note:** We also clearly need some mechanism for configuration, like [dependency injection] in Spring Framework, to separate the application logic in the request handler from the Netty-plumbing seen above but that has not been addressed in this blog.  The client is a HTML5 based web app . It has an input field for text based WebSocket requests and a number of read-only text areas for displaying:  1. Asynchronous WebSocket responses 2. Asynchronous notifications from the server using STOMP over WebSocket 3. Asynchronous notifications from the server using MQTT over WebSocket 4. Log messages for tracking what is going on under the hood  **Note:** This test web app subscribes using both STOMP and MQTT, a real world web app of course use only one of the subprotocols.  After initialization the web page looks like:  ![]  In the **Log messages** test area you can see how the communication is initiated for a WebSocket and adding a subscriber using both STOMP and MQTT.  The HTML page only contains layout:   The Javascript code is separated in a number of source files where  initiate the web app and the other three source files handles the communication, one source file per protocol:      That's all the code we need to write!  Let's deploy it to a small server  to prove that the Java server solution really is lightweight!  The smallest server I can think of is the [Raspberry Pi], a credit card sized computer at $35!  It is equipped with a single core 700 MHz 32 bit Arm processor, 512 MB memory and a SD card as disk device, i.e. it has hardware equivalent of a simple entry level smartphone. If our Java server runs on this tiny little server it has to be lightweight!  In the photo below we can see my Raspberry Pi  on top of its battery pack, powered up and waiting for something useful to do...  ![]  My Raspberry Pi runs [Raspbian]. Logging in to it over SSH reveals the following version information:  ![]  Ok, so now we need a Java server application that can perform some useful work that we can control via WebSocket commands and monitor via WebSocket notifications. The Raspberry Pi comes with a set of [GPIO pins]. So I wrote a very simple Java application that controls a LED light exposing three commands over a WebSocket: ,  and . Each time a command is executed the result returned to the caller is also notified over WebSocket to connected STOMP and MQTT subscribers.  **Note:** When the Java application runs on a non Raspberry Pi hardware  of the, in this case, imaginary LED light making it possible to run the server application on other hardware than a Raspberry Pi.  The source code for the request handler looks like:   Using the web app above to send the commands  and  results in the following output on the HTML page:  ![]  **Note:** messages are written in reverse order to the text areas to always have the latest message in the top of each text area.  ...and of course the LED light  is turned on:  ![]  As you can see from the log messages in the HTML page the server application response to the last  - command in 13 ms  are received 19 ms and 24 ms after the request was sent.  That is what I call realtime responses !  Coming from a $35 server indicates that our solution indeed is lightweight!  Monitoring the server application from another device :  ![]  Finally lets take a quick look using JConsole of what resources the server app requires:  ![]  I tried to put some manual load on the server application sending 4 reqs/sec and as you can see above the heap size is never exceeding 15 Mb, that is again what I call lightweight!  The CPU was working at 10% during my minimal 4 reqs/sec load test .  Now it should be very interesting to take on an appropriate load test tool and perform some serious scalability tests with a couple of thousands connected test clients but I have to leave that exercise to a follow up blog.  In this blog we have demonstrated how popular Java based open source products such as Netty and ActiveMQ can be embedded in an existing server application to enable it for full duplex communicating with modern HTML5 web apps using WebSocket, both for client initiated [request/response]. To support publish/subscribe over WebSocket we however need to use a protocol on top of WebSocket such as STOMP or MQTT as demonstrated. The proposed solution is straightforward to implement and provides a lightweight solution with low latency enabling realtime communication. The solution is based on the Java SE library for non blocking I/O, "_New I/O_", that provides capabilities for handling large numbers of concurrently connected clients. An actual test that demonstrates the scalability capabilities of this solution however has to be left for a follow up blog.  There are a number of interesting follow up questions worth their own blog posts in the future:  1. Will this Java server solution scale? 1. Within a single server. 2. Over many servers, also providing high availability. 2. How will [Java EE 7 and JSR 356] affect how we develop WebSocket enabled Java applications in the future? 3. What if the server application is not written in Java, e.g. how can we achieve this using node.js? 4. What if we not only want to enable a single server application with WebSocket communication but a whole system landscape without being forced to update each and every server application, e.g. how can a WebSocket enabled ESB help out in that case ? 5. Security aspects, as always...  So stay tuned for follow up blog entries on these subjects!
 Handling user registration and authentication on a website is hard, both for the users required to remember yet another pair of username/password and for developers implementing a secure handling of the user credentials. With requirements of more sophisticated login-methods such as [two-factor authentication] such as Facebook, Twitter and Google+ allowing users to sign in to the website using their social network accounts.  Initially [social login] project.  This blog will describe how [Spring Social] can be used to setup social login for a sample website. The login page looks like:  -[readmore]-  ![]  Before we delve into the details let's go through some background on the subject…  The benefits of social login are two-folded.  1. Website users can benefit from single sign-on using their social networking accounts to identify themselves for the website. 2. Website developers can use social login to automatically create local account for the user in the website based on the login information from the selected social network. As a website developer you can delegate the authentication process to the social network services.  To enable social login an open standard, [OAuth] to access server resources on behalf of the user, typically through an API that the service provide. But since _authorization_ requires _authentication_, authentication is also provided by the social network services in various forms.  Going through the details of the OAuth standard is out of scope for this blog but Jacob Jenkov's [tutorial on OAuth 2.0] is recommend for the interested reader, where Jacob explains the steps in "_the OAuth dance_" illustrated in his tutorial as:  <img src="http://tutorials.jenkov.com/images/oauth2/overview-1.png" alt="OAuth 2.0" style="width: 387px; "/>  In 2011 an open source based project, [Spring Social], with the target to simplify access to social networking services from Java. In April 2014 v1.1 was released with support for Twitter, Facebook and LinkedIn and with, as of today, support for some 30 more social network services as either incubator or community projects.  The Spring Social project provides:  * A standard way to get access to the social network specific API's with Java bindings to popular service provider APIs such as Facebook, Twitter, LinkedIn and GitHub. * An extensible service provider framework that greatly simplifies the process of connecting local user accounts to social network provider accounts. * Integration with Spring Security and Spring MVC * A connect controller that handles the authorization flow between your Java/Spring web application, a service provider, and your users. * A sign-in controller that enables users to authenticate with your application by signing in through a service provider.  For more information regarding the Spring Social project and how it works see its [reference documentation].  The sample website is designed to be as simple as possible providing:  * a _login page_, allowing the user to login using his account at Facebook, Twitter, LinkedIn, Google+, Windows Live or GitHub * a _main page_, where the user can see some basic profile information and enter some data in the website's database that is connected to the login * an _error page_, in case of that anything goes wrong...  When the user clicks on one of the login buttons in the login page  and if this is the first time the website is used the social network service will also ask for permissions to share the users login information with the website. This is done in a different ways, for example Twitter displays a combined web page where the user both can login and give its consent:  ![]  Once the user is signed in the main page will display some profile information in the upper right corner and present a form where the user can store some data in the website. When using Facebook it looks like: ![]  ...and when a user is signed in using LinkedIn the main page looks like: ![]  Clicking on the username in the upper right corner takes you to the profile page of the selected social network service, e.g. for LinkedIn: ![]  If you want to check out the source code  you need to have Git installed. Then perform:    The sample website use Spring Social to login users from a list of social networks and automatically create an internal user-account that is connected to the social network login. The sample website also demonstrates how a user can store user specific data and that the user can continue to work with the data the next time the user sing in using the same social network login.  The sample website is based on [Spring Boot].   To bring in Spring Social with the providers for the selected social networks into the project we only have to add the following to our Gradle build file, :    Once we have the dependencies in place we can configure Spring Social. We need to first specify a security filter and next setup the selected social network providers.  First we have to setup a security filter, , provided by Spring Social to integrate with Spring Security so that a social network provider can be activated when a user needs to be authenticated. The security filer will listen to URL's that start with  and route incoming requests to the corresponding social network provider. E.g. a request sent to  will be redirected to the Facebook provider. The security filter is configured by an -instance in the class  like:   Next we have to configure each provider with the standard OAuth Client Id and Client Secret properties. This is done in the class  where we add a connection factory implementation for each social network service :   We specify the properties in the  - file:   See below for how to get access to you own Client Id's and Client Secrets.  When it comes to creating a local user account connected to the social network service account most of the machinery is handled by the Spring Social project but we have to supply some logic to define how to:  * create an internal userId for a new user in the local account * load user information when a user logs in using a social network provider  Se the classes in the  package for details.   With the configuration in place we can now login a user using social login by declaring a set of login buttons in the  page like:   We are using [Bootstrap Social Buttons] for the login buttons. See the source code for details on CSS usage to render the buttons.  The information for each users connection to his social network service account is stored in a set of tables defined by Spring Security and Spring Social: ![]  Some notes on the data model:  * No real username nor password is stored in the -table. The table is defined by Spring Security for general purpose usage and not specific for social login use-cases. * The table  is the only table that is specific to our sample website and it holds the data that the user enters in the main page. * The profile information displayed on the main page comes from the -table and is stored in a model-element by the Spring MVC controller, , so that the Thymeleaf template engine can render it on the HTML pages using a html-framgent, , as:    To be able to build and run the sample code you need: Java SE 7, Gradle and Maven 3.  Before you can build the source code you checked out earlier you first have to build one of the providers, the Windows Live provider, since it is yet in its early days:   Now you can build the sample project:   Before you actually can run the project you have to register your own applications at the social network services, since I don't want to share my OAuth 2 Client Secrets with you . You can register your own application using the following links:  * [Facebook]  Some of the social network services don't accept  as a hostname or  as part of the hostname in the Redirect URI that you need to register for your application. Instead of setting up a fully-fledged DNS-hostname for my PC I created a made-up hostname, e.g.  that I used as the Redirect URI. I also added it to my  -file like:   The Redirect URI is only resolved by the web browser, not by the social network services, so it is ok that only your web browser knows how to translate the made-up hostname.  When you deploy it for real use you should of course replace it with a real DNS hostname!  **Note**. Microsoft has chosen to add an extra security check on the Redirect URI so they will not accept a made-up hostname. However they do accept localhost as a hostname so if you look carefully in the  page you can see that I have overridden the Redirect URI for the Windows Live provider as:   Now you can start the website with:   Go to a web browser on a machine that can translate the hostname   and enter the URL  and you should see the login page as displayed above.  Have fun playing with the social login using the various social network services!
  First impressions from the Spring XD tutorial  -[readmore]-  Some of us from Callista are currently on-site in Washington D.C. to participate in the [SpringOne2GX] tutorial. In this blog post I'll summarize my personal impressions after the first day with Spring XD.  First off, there little doubt Spring XD is an interesting piece of technology, but I think the tutorial this far is a bit of a mixed experience.  The the basic presentation about core XD concepts was quite good - their so-called “Lambda Architecture”  are built of Input Sources, Processors and Sinks. I felt some more time could have been spent on the problems it is trying to solve, and especially how it relates to Integration Platforms which also provides composable pipelines made out of EIP components. Another potential topic of interest are those cases where XD may prove to be the wrong tool for the job while first seeming to be so at a first glance.  Further, the hands-on exercises were on a too basic level in my humble opinion. While there is a day left, I would have liked more focus on potential application of XD from an architect's point of view rather than basics of the [Spring XD DSL] or the trivial task of how to install the product. I had expected more advanced application of XD concepts on real problems as labs rather than various Hello World scenarios. Perhaps building of an at least semi-complex solution over the two days in increments which would eventually cover the core concepts, DSL use, horizontal scaling, custom processors etc could have been an option.  The most interesting part were actually the coffee-break discussions with my Callista colleagues about Spring XD and what uses we could see in current and past projects. Realization such as _“had we only had this piece of technology when we…”_ can be both uplifting and frustrating. Technologies evolve, making the silver bullets of yesterday seem weathered and spent today.  Today we’ll hook up XD to hadoop, run XD in distributed mode and hopefully cover more complex topics.  More on Spring XD will follow. Stay tuned.
 Since people are using more and more devices to manage communication and planning data in their daily life, both for personal and professional use, there seems to be a growing challenge to keep the data on all those devices up to date and synchronized.  -[readmore]-  As most devices  are online these days it might obviously be a good idea to let them work with the same online master data, such as a for example a Google Apps or a Gmail account, for synchronizing calendars, mail and contact information.  The longer title of this article would be **Synchronizing Mail, Calendar and Contacts between Google Apps or Gmail accounts and iOS devices .  First of all you need to have a Google Apps or Gmail account, which you probably already have if you are reading this. Otherwise you can create one here: [http://apps.google.com]  It is a good idea to follow Google's and Apple's recommendations to backup your existing data before setting up synchronization to be on the safe side if something unexpected should happen during the synchronization process.  The following has to be done on each device for which you want to configure Google Sync. And also once for each account, if you have more than one.  1. At your device, open up a browser pointing at [http://m.google.com] 2. Tap in the square near the bottom saying something like **Are you using Google Apps? Click Here to configure your domain** and a dialog will appear. 3. In the dialog box saying **Enter your Google Apps-domain** enter your Google Apps domain e.g.  and press the button to save your domain. 4. Tap on one of your Google Apps, for example the Calendar. Just to get to your Google Apps login page. 5. Login with your Google Apps credentials. 6. Now, while you are logged in to your account, open up the URL [http://m.google.com/sync]  > If you get an error messages saying that your device is not supported, try tapping the **Change language** button and change language to English   7. You should now have got a list of your devices that are registered for synchronization. 8. Tap on the device you want to manage synchronization for, e.g. iPad. 9. In the opened view you can choose which calendars you want to be synchronized to your device.  1. Start up the Settings app and open the **E-mail, contacts, calendars** section. 2. Tap the **Add new account…**, choose **Microsoft Exchange** and enter the following: 1. E-mail; enter your e-mail address e.g.  . 2. Domain; Leave blank 3. Username; enter your e-mail address e.g.   4. Password; enter your Google Apps/Gmail password 5. Description: Give a short description of the account e.g. `My Company` 3. Tap **Next** and a Server field will appear 1. Server; enter  4. Choose what you want to synchronize   Your account is now configured and your device will start to synchronize mail, contacts and your chosen calendars in the background .
Recently I joined my first portal project. Almost immediately we ran into the problem of inter-portlet communication  all about IPC, with major new features including:  - Event handling - portlets will be able to send and receive events. - Public render parameters - portlets will be able to share parameters with other portlets. - Resource serving - The ability for a portlet to serve a resource.  JSR 286 is currently in the state _Public Final Draft_, so hopefully it will soon be finalized. After that it will take some time before the portal servers start supporting it.  In the meantime almost every portal vendor offers their own implementation. The question, off course, is how to solve the problem without vendor lock-in .  A common solution to the most basic IPC cases is to let one portlet store data in the PortletSession during the action phase. If the data is stored in the application scope other portlets within the same web application can read the data during the render phase. This means that the solution only works for portlets that are packaged and deployed in the same web application. In most cases this won't be a problem since we probably don't want unrelated portlets to depend on each other anyway.  The solution is perhaps not the most elegant but it does the job, with the advantage of doing it in a vendor independent way. We will probably want to minimize the use of IPC altogether but when it's absolutely necessary this solution will be sufficient. Another advantage, compared to the vendor specific implementation , is that it doesn't rely on IDE wizards that changes and updates files all over the place.   - Inter-portlet communication tip on java.net - JSR 168 - JSR 286
 I just read Boris Lublinsky's article on [Is REST the future for SOA?].  I think it makes a good job in clarifying that REST and SOA are different architecture styles, rather than merely a choice of protocol. I also like that he discusses the common case where people talk about REST when they actually mean http services without SOAP and WSDL. Lublinsky names this no mans land "REST Web Services". I think it is unfurtunate to include "REST" in the name. It kind of preserves the fuzziness. I would prefer "HTTP services". I was tempted to say "HTTP XML Services" but I guess JSON would be the typical payload representation.  -[readmore]-  Beyond that, I don't agree with his conclusions on how "REST Web Services" compare to WS-* web services. In contrast, I think it is more lightweight to utilize the HTTP protocol. Today, WS-* is WS-I Basic Profile with transport layer security. So there is no "feature" of WS-* that is not available in the http protocol, while there are actually several examples of the opposite  that are lost by layering SOAP on top of http.  Moreover, http as a protocol is simpler because developers today are born with http.  So what about contracts? I agree with Lublinsky's take on contracts. With "REST Web Services" we still need contracts. And the XML schemas could be used regardless whether WS-* or "REST Web Services" are used. Generally, the payload of well-designed contract first WS-* web services can be used out of the box as contracts for "REST Web Services" as well. But since XML schemas used for services based on canonical business models rarely express a majority of the constraints of the service, we still depend on a document to be interpreted by developers in design time  as if java- or .net objects were actually marshaled between service consumer and producer. As a contract language, XML schema isn't close to the importance expressed by Lublinsky.  For the service consumer of a contract-first service, an http XML service is likely the most straight-forward protocol to consume in case the consumer is a "system" rather than a web browser. Trough content negotiation it is also easy to provide multiple representations from the same endpoint. Web browser applications need for JSON payload could then be supported in parallell, through mechanisms architected into the http protocol itself .  But most importantly – as stated by the article – this is not REST.
 In this blog post we will create a secure API for external access, using [OAuth 2.0].  -[readmore]-  For information about OAuth 2.0 either see introductory material [Parecki - OAuth 2 Simplified].  We will add a new microservice, , that will act as the external API  acting as a *token relay*, i.e. forwarding OAuth access tokens from the client to the resource server. We will also add an *OAuth Authorization Server* and an *OAuth client*, i.e. service consumer, we will continue to use .  The system landscape from [Part 2]:  ![system-landscape]  We will demonstrate how a client can use any of the four standard authorization grant flows to get an access token from the authorization server and then use the access token to make a secure request to the resource server, i.e. the API.  > **NOTES:** > > 1. Protecting external API's is nothing specific to microservices, so this blog post is applicable to any architecture where there is a need to secure external API's using OAuth 2.0! > > 1. We will provide a lightweight OAuth authorization server only useful for development and testing. In a real world usage it needs to be replaced, e.g. by an API platform or by delegating the sign in and authorization process to social networks such as Facebook or Twitter. > > 1. We are on purpose only using HTTP in this blog post to reduce complexity. In any real world usage of OAuth all traffic should be protected using TLS, i.e. HTTPS! > > 1. As in the previous posts we emphasize the differences between microservices and monolithic applications by running each service in a separate microservice, i.e. in separate processes.   As in [Part 2] we use Java SE 8, Git and Gradle. So, to access the source code and build it perform:   > If you are on **Windows** you can execute the corresponding bat-file !  Two new source code components have been added since [Part 2], an *OAuth Authorization Server*, , and the *OAuth Resource Server*, :  ![source-code]  The build should result in ten log messages that all says:    Let's look into how the two new components are implemented and how the edge server is updated to be able to relay the OAuth access tokens. We will also change the URL for the API to make it a bit more convenient to use.   To be able to use OAuth 2.0 we will bring in the open source projects [] with the following dependencies.  For the :   For full source code see [auth-server/build.gradle].  For the :   For full source code see [product-api-service/build.gradle].   The implementation of the authorization server is straight forward. It is brought to life using an annotation, . Then we use a configuration class to register  the approved client applications, specifying client-id, client-secret, allowed grant flows and scopes:   This approach obviously only works for development and test to simulate a client application registration process provided by real world OAuth Authorization Servers, e.g. [LinkedIn].  For full source code see [AuthserverApplication.java].  Registration of users , is done by adding one line per user in the file , e.g.:   For full source code see [application.properties].  The implementation also comes with two simple web based user interfaces for user authentication and user consent, see the [source code] for details.   To make our API-implementation act as a *OAuth Resource Server* we only need to annotate the -method with the -annotation:   For full source code see [ProductApiServiceApplication.java].  The implementation of the API-service is very similar to the composite service in [Part 2]. To be able to verify that OAuth works we have added logging of the user-id and the access token:   > **NOTES:** > > 1. Spring MVC will fill in the extra parameters for the current user and the authorization header automatically. > > 2. We have removed the uri  from the  to be able to get a more compact URL when using the edge server since it will add a  prefix to the url to be able to route the request to the correct service. > > 3. Writing access tokens to the log is probably not to recommend from a security perspective in a real world application.   Finally we need to make the edge server forward the OAuth access tokens down to the API-service. Fortunately this is actually already the default behavior, so we don't need to do anything in this case :-)  To make the URL's a bit more compact we have modified the route configuration slightly from [Part 2] to:   This makes it possible to use URL's like:  instead of  as we used in the previous posts.  We have also replaced the route to the  with a route to the .  For full source code see [application.yml].   > As in [Part 2] for details on how to start up the system landscape.  First start RabbitMQ:   > If you are on **Windows** use Windows Services to ensure that the RabbitMQ service is started!  Then start the infrastructure microservices, e.g.:   Finally launch the business microservices:   > If you are on **Windows** you can execute the corresponding bat-file !  Once the microservices are started up and registered with the service discovery server they should write the following in the log:   We are now ready to try out obtaining an access token and then use it to call the API in a secure way!   The OAuth 2.0 specification defines four grant flows to obtain an access token:  <img src="https://callistaenterprise.se/assets/blogg/build-microservices-part-3/grant-flows.png" width="500" />  See [Jenkov - OAuth 2.0 Authorization] for more details regarding the grant flows.  > **NOTE**: The grant flows *Authorization Code* and *Implicit* are the most frequently used and the two remaining are considered to cover corner cases where the first two does not apply.  Let's go through each grant flow and see how it can be used to obtain an access token!   First we need to get a `code grant`  using a web browser. Go to:  [http://localhost:9999/uaa/oauth/authorize? response_type=code& client_id=acme& redirect_uri=http://example.com& scope=webshop& state=97536]  Login  and give your consent in the web pages that are displayed. The web browser should redirect you to a URL like:   **NOTE:** The  parameter should be set to a random value in the request and checked on the response for preventing cross-site request forgery  Take the  parameter from the redirect URL you got as the response and store it in an environment variable:   Now act as the secure web server and use the `code grant` to get the access token:   Save the access token in an environment variable for later use when we access the API:   Let's make a second attempt to get an access token for the same code. It should fail, e.g. the code is actually working as a one time password:    With implicit grant we skip the code grant, instead we request the access token directly from the web browser . Use the following URL in a web browser:  [http://localhost:9999/uaa/oauth/authorize? response_type=token& client_id=acme& redirect_uri=http://example.com& scope=webshop& state=48532]  Login  and give your consent, if required, in the web pages that are displayed. The web browser should redirect you to a URL like:   **NOTE:** The  parameter should be set to a random value in the request and checked on the response for preventing cross-site request forgery  Save the access token in an environment variable for later use when we access the API:    In this case the user typically don't have access to a web browser, instead the user have to give his/her credentials to the client application that use them to obtain an access token :   Save the access token in an environment variable for later use when we access the API:    In the last case we assume that there is no need of a user consent to access the API. The client application can, in this case, authenticate it self to the authorization server and obtain an access token:   Save the access token in an environment variable for later use when we access the API:   > Note that the access token in this case represent the client application, not the resource owner ` will return  and not  when called from the .   Now, when we have an access token, we can start to access the actual API.  First try to access the API without an access token, it should fail:   Great, we are stopped as expected!  Next, try with a invalid access token, it should fail as well:   Again, we are denied access as expected!  Now, let's try to perform a correct request supplying one of the access tokens we received from the grant flows above:   Great, it works!  Also take a look at the log events printed by the api-service, :   We can see that the API contacts the Authorization Server to get info about the user and then prints out the username and the access token in the log!  Finally, let's try to invalidate the access token, e.g. simulating that it has expired. One way to do that is to restart the   and then retry the request from above:   The previously accepted access token is now rejected, as expected!   We have seen how we, thanks to the open source projects *spring-cloud-security* and *spring-security-auth*, easily can set up a secure API based on OAuth 2.0. Remember, however, that the Authorization Server we used only is useful for development and testing!   Next up in the [Blog Series - Building Microservices] is centralized log management using the ELK stack, i.e. Elasticsearch, LogStash and Kibana.  Stay tuned!
 As Martin Fowler states in his article [Mocks Aren't Stubs]. It is understandable but there are some distinctions. A common interpretation is that stubs are static classes and mocks are dynamically generated classes by using some mocking framework. But the real difference between them is in the style of how you use them, i.e. state-based versus interaction-based unit testing.   A stub is a class supposed to return data from its methods and functions. The return value is hard-coded. Stubs are used inside unit tests when we are testing that a class or method delivers expected output for a known input. They are easy to use in testing, and involve no extra dependencies for unit testing. The basic idea is to implement the dependencies as concrete classes, which reveal only a small part of the overall behavior of the dependent class, which is needed by the class under test.  First, the interface...   Class with a dependency to the interface...   Testing the implementation...   The stub does nothing more or less than returning the value that we need for the test. It is common to see such stubs implemented as an anonymous inner classes in Java...   This saves a lot of time maintaining stub classes as separate declarations, and also helps avoiding the common pitfalls of stub implementations, i.e. reusing stubs across unit tests.  Implementing stubs in this way  requires dozens of lines of empty declarations of methods that are not used in the service. Also, if the dependent interface changes, we have to manually change all the closure stub implementations in all the test cases. Which can be a hard and tedious work. The example here is simple. In real world there would be a lot more interface methods declared.  A solutions to this problem is to use a base class, and instead of implementing the interface afresh for each test case, we extend that base class. If the interface change, we only need to change the base class. Usually the base class would be stored in the unit test directory of the project, not in the production or main source directory.  A base class implementing the interface...   And the new test case will look like this...    Mocks are used to record and verify the interaction between two classes. Using mock objects gives a high level control over testing the internals of the implementation of the unit under test. Mocks are beneficial to use at the I/O boundaries - database, networks, XML-RPC servers etc - of the application, so that the interactions of these external resources can be implemented when they are not in the application's control.   __  Another advantage to the mocking approach is that it gives a more development process when working within a team. If one person is responsible for writing one chunk of code and another person within the team is responsible for some other piece of dependent code, it may not be feasible for this person to write a stubby implementation of the dependency, when the first person is still working on it. However, by using mock objects anyone can test this piece of code independent of the dependencies that may be outside that persons responsibility.  Advantages with mock objects:  - Allow testing a specific unit of code with few line of code - Isolated and autonomous tests - Fairly easy to set up - Fast test executions   Stubs and mocks may seem the same but the flow of information from each is very different:  - Stubs provide input for the application under test so that the test can be performed on something else. - Mocks provide input to the test to decide on pass or fail.  A stub is application facing, and a mock is test facing. It's important to know and distinguish the two since many frameworks for mocking use these terms for different kinds of objects.  The biggest distinction is that a stub you've already written with predetermined behavior. So you would have a class that implements the dependency  you are faking for testing purposes and the methods would just be stubbed out with set responses. They wouldn't do anything fancy and you would have already written the stubbed code for it outside of your test.  A mock is something that as part of your test you have to setup with your expectations. A mock is not setup in a predetermined way so you have code that does it in your test. Mocks in a way are determined at runtime since the code that sets the expectations has to run before they do anything.  Tests written with mocks usually follow:   While the pre-written stub would follow   However, the purpose of both is to eliminate testing all the dependencies of a class or function so your tests are more focused and simpler in what they are trying to prove.
 Earlier this year David Dossot announced the availability of [CRaSH for Mule] for more background info.  Let's try it out!  -[readmore]-  First we need to install it :-)  That's really a no-brainer, just download, extract and deploy as a Mule App!  1. Download [crash-1.2.6-mule-app.tar.gz]. 2. Extract its content 3. Locate the app, , in the extracted content and copy it to your running Mule instance apps-folder and it will auto-deploy   Since I prefer ssh over telnet I'll go with that :   To see the available commands simply submit the command "mule":   Get info about the Mule ESB instance:  Mule Version 3.3.1 Build Number 25116 Build Date 2012-Dec-10 12:56:47 Host IP 127.0.0.1 Hostname singleserver.local OS Linux  JDK 1.7.0  Launched As Service true Debug Enabled false Java PID 14012 JVM Id 1  % % mule apps name start time initialized stopped  List what flows does the `vp-services-2.1.0 app` have:   GetSupportedServiceContracts-flow Flow PingForConfiguration-rivtabp20-flow Flow PingForConfiguration-rivtabp21-flow Flow PingService-flow Flow ServiceGroups-flow Flow crm-scheduling-GetSubjectOfCareScheduleInteraction-virtualisering-flow Flow htmlDashboardService-flow Flow itinfra-tp-ping-virtualisering-1.2-SNAPSHOT-flow Flow log-error-receiver-flow Flow log-info-receiver-flow Flow log-publisher-flow Flow log-store-receiver-flow Flow resetHsaCache-flow Flow resetVagvalCache-flow Flow vagval-dynamic-routing-flow Flow  % % mule endpoints -a vp-services-2.1.0 name address  Some runtime statistics from a specific flow:  SyncEventsReceived 20 AsyncEventsReceived 0 TotalEventsReceived 20 ExecutionErrors 0 FatalErrors 0 ProcessedEvents 20 MinProcessingTime 198 MaxProcessingTime 3467 AverageProcessingTime 486 TotalProcessingTime 9729 % % mule app -a vp-services-2.1.0 stop Action stop successfully run. Application intialized: true, stopped: true  % % mule apps name start time initialized stopped  Start the application again:   Ensure it is back on-line :   crash-mule-app Tue Jun 11 19:24:54 UTC 2013 true false default Tue Jun 11 19:24:54 UTC 2013 true false vp-services-2.1.0 Wed Jun 12 09:11:54 UTC 2013 true false  % % mule broker restart Action restart successfully run.  %  % mule appsConnection to 33.33.33.33 closed. $ ssh -p 4022 -l root 33.33.33.33 root@33.33.33.33's password: ______ .~ ~. |, .'. ..'''' | | | |'''|''''' .''. .'' |_________| | | `. .' `. ..' | | `.______.' | `. .' `. ....'' | | 1.2.6  Follow and support the project on http://www.crashub.org Welcome to singleserver.local + ! It is Wed Jun 12 09:19:32 UTC 2013 now  % mule apps name start time initialized stopped  We have just seen some of the most common functions in the tool in action but I guess that you already agree with me that in environments where you don't have access to the Mule Management Console this is really a useful management tool, specially if you have multiple Mule ESB instances spread over a number of servers!  Give it a try your self!
 Imagine a service implementation having autowired dependencies like:   To test this above service in isolation and mock each dependency is not hard. Create a setter method for each dependency and mark it as "only used in test" in your service class and then in your test class you create a EasyMock object for each and set them on the service:   It feels like the "slim and slick code" of the annotated  is disappearing when you still need to create your setter methods.  I am a fan of writing "IntegrationTest" with junit. It means that I like to test all the way without mocking some of the parts that isn't needed like the database. It might not be the proper way of writing tests, but I like to test the code all the way to see that everything runs smoothly before I deploy it and runs it for real in a test environment. Some of you would probably say that if I write test in perfect isolation it is not needed to write integration tests. But until I am proved that there is a "perfect isolated unit test world" I will continue to write integration test for some parts of my code.  So my problem is now that if I in my above service class would like to mock the first manager but not the dao beans. I will use EasyMock and then combine it with dbunit to create some testdata in the database. The only way I could figure out how to do it is to create setter methods of all injected beans as above and then autowire the real beans into my testclass and then create the mock of the manager and set all on the service:   It's not pretty at all. I tried to configure a application context for the test context, but couldn't figure out how to get it to work to be able to autowire one bean but not the other ones.
 The more microservices we get in our system landscape, the harder it gets to perform configuration management. If each microservice have its own configuration, typically in a local property or yaml file, it can quickly become a nightmare to maintain. Let's add a Configuration Server to centralise management of all the configuration files!  -[readmore]-   In this blog post we will complement the system landscape in the [blog series - Building Microservices]:  ![Config Server]  ...and describe how it can be used to centralise the configuration management for a microservices based system landscape:  ![Config Server]  > We actually used the configuration server already in the [previous blog post]. Now it's time to explain how it works!  We will cover the following aspects of using [Spring Cloud Config]:  1. **Server setup** How to create a configuration server  1. **Client configuration** How to configure the clients, i.e. our microservices, to get their configuration from the config server  1. **Configuration structure** How a configuration repository can be structured  1. **Secure the configuration** How to secure the configuration, both at rest and in transit  1. **Configuration storage and change propagation** How to store the configuration and how changes in the configuration can be propagated to the configuration server and affected microservices  1. **Refreshable properties** How a microservice can react on an updated configuration in runtime without require a restart  Each section will explain the most central parts of the code and when applicable run some tests to prove that it's actually working!  But first, let's get it up and running without any further explanations, shall we?   For details on how to build and run the microservice landscape in this blog post series, see the [previous blog post].  > To be able to run some of the commands used below you need to have the tools [cURL] installed.  In summary:  1. Open a terminal, create a folder of your choice and  into it:  $ mkdir a-folder-of-your-choice $ cd a-folder-of-your-choice  1. Since we have externalised our configuration into a configuration repository we first need to get it from GitHub:  $ git clone https://github.com/callistaenterprise/blog-microservices-config.git  1. Next, we get the source code from GitHub and checkout the branch used for this blog post:  $ git clone https://github.com/callistaenterprise/blog-microservices.git $ cd blog-microservices $ git checkout -b B9 M9  1. Now we can build our microservices with:  $ ./build-all.sh  1. Finally we can bring up the dockerized microservice landscape and run a test:  $ . ./test-all.sh start  It should after a while result in a response from the API request like:  $ curl -ks https://localhost:443/api/product/123 -H "Authorization: Bearer $TOKEN" | jq . { "productId": 123, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ], "serviceAddresses":  }  > **Note #1:** We, don't shut down the microservice landscape . We will use it later on run some tests that demonstrates that the things we explain actually works.  > **Note #2:** The first in the command above is essential. It allows us to reuse the  environment variable that the script creates to store an OAuth Access Token, i.e. we don't need to acquire one ourselves.   Further details, not covered in this blog post, on how to use [Spring Cloud Config].   We have replaced the no longer maintained Docker image for a 32 bit embedded Java 8 JRE, used to keep memory consumption to a minimum, with the official 64 bit Open JDK Java 8 JRE. To minimise the increase of memory usage by going from 32 bit to 64 bit we have setup a max size of the heap for each microservice in the  - file:  environment: - JAVA_TOOL_OPTIONS=$  We have created a setup-file,  for the required environment variable :  export MY_JAVA_TOOL_OPTIONS=-Xmx256M  You need to execute it once before you run any commands:  $ . ./setup-env.sh  Then you can, for example, start up the system landscape with:  $ docker-compose up -d   Setting up a Spring Cloud Config server is very straight forward, i.e. very similar to how to setup the other Spring Cloud servers, e.g. Discovery, Edge and OAuth Authorization Servers:  1. The config server runs as a conventional Spring Boot application 1. The Gradle dependency that enables it to be a config server is:  compile  1. It only contains a single Java class:  and the only thing of interest in it is the following annotations on its `static main` - method:  @EnableConfigServer @EnableDiscoveryClient @SpringBootApplication  1.  is what makes it a config server 1.  enables it to register itself with Eureka, our discovery server,  1.  is, as usual, what makes our application an Spring Boot application.  The source code of the config server also contains:  1. Standard configuration files, its content will be explained below 1. An empty test-class, that at least ensure that the config server can start 1. A conventional Dockerfile   Clients, i.e. our microservices, that want's to access its configuration can reach the config server in two ways:  1. Bind directly to the config server using the configuration property  1. Lookup a config server using the discovery server, Eureka. This can be done by specifying the properties: 1. `spring.cloud.config.discovery.enabled: true`  1. `spring.cloud.config.discovery.serviceId: config-server`, i.e. the name of the config server as specified in the config server's  - file in the property .  Each client stores this connection configuration in its local -file.  > Clients can't store information on how to connect to the config server in the config servers repository, right?  Option no. 2 might seem better due to the looser coupling, e.g. you can move the config-server or have multiple config servers started to avoid a single point of failure. Option no. 1 however works fine as well given that you deploy your microservices in a container orchestrator, e.g. Docker in Swarm mode or Kubernetes, since they will provide a logical service name that the clients can use and the container orchestrator will forward the request to one of the running config server. More on that subject in a future blog post.  For the scope of this blog post we will use option no. 1, but the configuration files are prepared for option no. 2, just change the properties  from to .  To add some robustness to the solution I have also added connection configuration that allows the clients to retry connecting to the config server if they can't access the config server, e.g. during startup of all the services after a `docker-compose ut -d` - command:  spring: cloud: config: failFast: true retry: initialInterval: 3000 multiplier: 1.3 maxInterval: 5000 maxAttempts: 20   All local configuration files, e.g. , for each microservice will be moved to a central configuration repository . An obvious question is how to structure the files when they are placed together.  Two important aspects are:  1. How to share common configuration and how to allow for microservice specific configuration 2. How to handle environment specific configuration  All common configuration is placed in the file  and specific configuration per microservice is placed in a file with the name of the microservice .  Environment specific configuration is handled as before, i.e. in different Spring profiles. The  profile is used for running the microservices locally without containers and the  profile is used for deployment in Docker. This model can easily be extended by adding more profiles for specific environments, e.g. profiles for , and .  The config repository contains the following files:  application.yml auth-server.yml composite-service.yml edge-server.yml monitor-dashboard.yml review-service.yml  The config server expose a REST API for accessing the configuration.  To get access to the default configuration for all microservices you make a request like:  $ curl -ks https://$MY_CONFIG_USER:$MY_CONFIG_PWD@localhost:8888/application/default | jq  > The use of  will be explain below in the section regarding security.  ...and you will get a response like:  { "name": "application", "profiles": [ "default" ], "label": null, "version": null, "propertySources": [ { "name": "file:/config-repo/application.yml", "source": { "server.port": 0, "service.defaultMinMs": 5, "service.defaultMaxMs": 20, . ..  If you want to see the configuration for the review-service when it runs in Docker you ca submit a request like:  $ curl -ks https://$MY_CONFIG_USER:$MY_CONFIG_PWD@localhost:8888/review-service/docker | jq  ...and get a response like:  { "name": "review-service", "profiles": [ "docker" ], "label": null, "version": null, "propertySources": [ { "name": "file:/config-repo/application.yml#docker", "source": { "spring.profiles": "docker", "server.port": 8080, "spring.rabbitmq.host": "rabbitmq", "eureka.instance.preferIpAddress": true, "eureka.client.serviceUrl.defaultZone": "http://discovery:8761/eureka/" } }, { "name": "file:/config-repo/review-service.yml", "source": { "service": "", "my-secret-property": "my-secret-value" } }, { "name": "file:/config-repo/application.yml", "source": { "server.port": 0, "service.defaultMinMs": 5, . ..   Securing the configuration is of course vital in a production environment.  To me, security aspects can be divided in two main areas:  1. How to prevent unauthorised clients to access the configuration information when it is *in transit*, i.e. during an API call to the configuration server.  1. How to protect sensitive data  *at rest*, i.e on disk, from potential intruders that got access to the file system.   To protect the configuration information in transit there are two aspects to cover:  1. How to ensure that only authorised clients can make calls to the configuration server API.  2. How to ensure that a non authorised third parties can't eavesdrop the API calls, i.e. intercept the network traffic and get access to the configuration information.  To prevent eavesdropping we need to ensure that the traffic is encrypted in a sufficient way  and to authorise the clients we can apply a wide range of mechanisms e.g. HTTP Basic Authentication, HTTPS with mutual authentication or OAuth.  In this blog post we use HTTPS to encrypt the communication and HTTP Basic Auth to ensure that only authorised clients can make calls to the configuration server.  The username and password used for Basic Auth is externalised in the  - file:  export MY_CONFIG_USER=config_client export MY_CONFIG_PWD=config_client_pwd  The config server uses Spring Security to protect its API's and gets the approved username and password injected in the  - file as:  config: environment: - SECURITY_USER_NAME=$ - SECURITY_USER_PASSWORD=$  All config clients gets a pair of environment variables defined in  - file as:  environment: - MY_CONFIG_USER=$ - MY_CONFIG_PWD=$  That they use in their local  - file to setup the username and password in the URL to the config server as:  spring: cloud: config: uri: https://$@config:8888   > **Note:** In a Docker based production environment I strongly recommend you to use a container orchestrator such as Kubernetes or Docker in Swarm mode and then use the build in support in those tools for handling secrets such as usernames and passwords instead of passing them through environment files. I hope to be able to cover how that is done in a future blog post!  Try it out by asking for some configuration information!  First without supplying any credentials:  $ curl -ks https://localhost:8888/application/default | jq  Returns an expected error message:  { "timestamp": 1494318890659, "status": 401, "error": "Unauthorized", "message": "Full authentication is required to access this resource", "path": "/application/default" }  Next, add authentication info in the request:  $ curl -ks https://$MY_CONFIG_USER:$MY_CONFIG_PWD@localhost:8888/application/default | jq  ...and you will get the expected configuration information:  { "name": "application", "profiles": [ "default" ], "label": null, "version": null, "propertySources": [ { "name": "file:/config-repo/application.yml", "source": { "server.port": 0, . . .   The Spring Cloud Config server supports encryption of sensitive properties in the configuration files. To be able to encrypt sensitive properties it needs a encryption key to be configured. The key can either be symmetric or asymmetric.  Asymmetric keys are superior in terms of security, but symmetric keys are easier to setup so we will use a symmetric key in this blog post. For details see the reference documentation regarding [key_management].  The config server provides two endpoint for encrypting and decrypting sensitive properties: and . These two endpoints needs of course to be protected as well as described above.  We have, again, used the - file to externalise sensitive information, in this case the symmetric key:  export MY_CONFIG_ENCRYPT_KEY=my-very-secret-encryption-key  We setup an environment variable for the config server with the symmetric key in the  - file as:  config: environment: - MY_CONFIG_ENCRYPT_KEY=$  Finally is the config server using the environment variable in its local configuration file  as:  encrypt.key: $  Try it out, encrypt the text :  $ curl -ks https://$MY_CONFIG_USER:$MY_CONFIG_PWD@localhost:8888/encrypt -d my-secret cfdfe0ed3eeb9dc406508d4a5a7124e7192def5422a86bd5183ff00eb6fb1d77  ...and decrypt it back:  $ curl -ks https://$MY_CONFIG_USER:$MY_CONFIG_PWD@localhost:8888/decrypt -d cfdfe0ed3eeb9dc406508d4a5a7124e7192def5422a86bd5183ff00eb6fb1d77 my-secret  The encrypted value can be used in our config files in the config repo, e.g. in :  my-secret-property: 'cfdfe0ed3eeb9dc406508d4a5a7124e7192def5422a86bd5183ff00eb6fb1d77'  The review service will get the property in clear text from the config server, so it is totally unaware of that the property is protected at rest.  See :  public class ReviewService {  @Value private String mySecretProperty;  ...  @RequestMapping public List<Review> getReviews( @RequestParam {  LOG.info;  Try it out with:  $ curl -ks https://localhost:443/api/product/123 -H "Authorization: Bearer $TOKEN" | jq .  ...and you will see the decrypted value in the log output:  $ docker-compose logs --tail=10 rev ... rev_1 | 2017-05-12 13:34:30.185 INFO ... ReviewService : mySecretProperty: my-secret-value ...  > In general it is obviously not a very good idea to print out decrypted values in the log output...   Spring Cloud Config supports storing the configuration files in either:  1. A version control system, either Git or Subversion 2. The local file system 3. [HashiCorp Vault]  This blog post covers how to store the configuration files in ether a Git repo or in the local file system, checkout the [reference documentation] for information on the other options.  The configuration files are normally stored in a Git repo, providing version control of the configuration . For development, storing the configuration directly in the local file system is very handy.  In this section we will go through:  1. Git based storage 2. Push notifications from a Git server 3. Propagating configuration changes to the clients 4. File based storage 5. Usage in this blog post   The Git repo can be stored locally, e.g during development, but is normally hosted on a Git server.  For a local Git repo, the configuration can look like:  spring.cloud.config.server.git.uri: file:///$/../../../../blog-microservices-config/  For a git repo on GitHub, the configuration can look like:  spring.cloud.config.server.git: uri: https://github.com/callistaenterprise/blog-microservices-config.git username: your-username password: your-password   To be able to be notified on changes of the configuration in a Git repo on a Git server Spring Cloud Config supports webhook based push notifications from GitHub, GitLab and Bitbucket. By adding the following dependency in the config servers Gradle build file, a  endpoint is enabled:  compile  A webhook can now be registered in the Git repo on the Git server using a URL like:  http://dns-name-for-the-config-server:8888/monitor  To test push notifications locally you can simulate a webhook request from GitHub using  like:  $ curl -H "X-Github-Event: push" -H "Content-Type: application/json" -X POST -d '' -ks https://$MY_CONFIG_USER:$MY_CONFIG_PWD@localhost:8888/monitor ["review:service","review-service"]  What this request actually says is that the configuration file  is changed in the Git repo.    The configuration server will notify the affected microservices, the review-service in the example above, and it will pick up the changes. This can be seen in the log output from the affected microservices, e.g.:  $ docker-compose logs --tail=10 rev ... rev_1 | 2017-05-12 13:40:21.552 INFO [review-service, ... : Fetching config from server at: https://config:8888 rev_1 | 2017-05-12 13:40:21.776 INFO [review-service, ... : Located environment: name=review-service, profiles=[docker], label=null, version=null ...  To notify the affected clients Spring Cloud Config use the [Spring Cloud Bus]. Spring Cloud Bus supports a number of message brokers, e.g. RabbitMQ and Kafka.  In this blog post we use [RabbitMQ] by adding the following dependency in both the config server and config clients Gradle build files:  compile  ...and adding the following configuration for both the default profile and the docker profile:  spring.rabbitmq: host: localhost port: 5672  --- # For deployment in Docker containers spring: profiles: docker  spring.rabbitmq.host: rabbitmq  The configuration is added in the config repo file , so it applies to all microservices. Since the config server don't reads its own configuration from the config repo we also needs to repeat the configuration in the config servers local  - file, see   Finally to bring in RabbitMQ we simply run it as a Docker container added to our  file:  rabbitmq: image: rabbitmq:3-management restart: always ports: - "15672:15672"  > Running RabbitMQ in a single container like this is only useful for development and test, for production you need to setup a proper RabbitMQ cluster or use a cloud service like [CloudAMQP]  The RabbitMQ web admin GUI can be reached at: [http://localhost:15672] and you can login using the default credentials "guest/guest".  ![RabbitMQ]   Another option, that is very handy during development, is to configure the config server to use the native file system instead of Git. This enable the config server to pickup changes directly form the local file system and promote them immediately to the affected clients .  To setup the config server to use the native file system instead of Git you configure it to use the Spring profile  and then use the property  to specify where to find the configuration files.   In this blog post we are using a file based configuration repository. We specify the configuration for the config server in the - file like:  environment: - SPRING_PROFILES_ACTIVE=docker,native - SPRING_CLOUD_CONFIG_SERVER_NATIVE_SEARCH_LOCATIONS=file:/config-repo  ...and we have a Docker volume that connects the Docker container folder  to the config-repo we checked out from Git above:  volumes: - $PWD/../blog-microservices-config:/config-repo  If you want to try out Git based usage you can remove the  profile and the  environment variable from the  file.  I have setup the config server to use a local Git repo in the file:  for both the default Spring profile and the docker profile:  # Configuration of a local Git-repo for development locally without Docker # note that we are still using git; We just access the git-repo via file: protocol spring.cloud.config.server.git.uri: file:///$/../../../../blog-microservices-config/  --- # For deployment in Docker containers spring: profiles: docker  # when running docker locally on a dev pc the config-repo is mounted to a local git-repo in docker-compose file spring.cloud.config.server.git.uri: /config-repo    Ok, so the learned that the config server can get notifications from the config repo  when some configuration was changed and the config server can use Spring Cloud Bus to inform our microservices regarding the change...  But now what?  I.e. how can we write code to react on changed configurations in realtime, i.e. without perform a traditional restart of the service?  Spring Cloud comes with a bean annotation, , that is very useful for this purpose. It will instruct Spring to reinject all fields and setter methods that are annotated with the  - annotation if their configuration property is changed. See [http://cloud.spring.io/spring-cloud-static/docs/1.0.x/spring-cloud.html#_refresh_scope] for details.   Some built in properties are already using the  - annotation, e.g. logging. This means that changes of the log level settings in the config repo has immediate affect.  Try it by monitoring the logs and then change log level from INFO to DEBUG for the review-service. Edit  in the config repo and remove the comment mark from:  #logging.level.se.callista: DEBUG  to:  logging.level.se.callista: DEBUG  Try call the API:  $ curl -ks https://localhost:443/api/product/123 -H "Authorization: Bearer $TOKEN" | jq .  ...and you should start to see log output from the review service with  level like:  $ docker-compose logs --tail=10 rev ... rev_1 | 2017-05-12 13:56:15.662 DEBUG [review-service, ... : Return calculated processing time: 14 ms ...   As seen when playing with the circuit breaker in the [previous blog post] you can change the response time for our services with properties:  service: defaultMinMs: 6000 defaultMaxMs: 6000   The code to enable realtime updates of these properties looks like:   @RefreshScope @Component public class SetProcTimeBean {  @Value public void setMinMs { LOG.info; this.minMs = minMs; }  @Value public void setMaxMs { LOG.info; this.maxMs = maxMs; }  I.e. a Spring Bean that is annotated with  and a couple setter-methods that are annotated with .   Well, that's it!  It might look a bit overwhelming at a first glance. But when you get used to it I hope you will find it as convenient to work with as I did!  Next up in the [blog series - Building Microservices] is centralised logging and a pre-req for doing centralised logging, handling correlation ids, stay tuned...
 Imagine that you could write code like this:   This is a simple DSL that lets you create an order with specific attributes. Quite obvious isn't it? This does not really look like something from a programming language, but it is actually valid Scala code. Let me show you how that can be.  Let's start with the first part ` you can write `object say "hello"`. This makes the first part of our DSL simple to express. We create an object called 'create', gives it a method called 'an' that takes an object 'Order'. Scala let's you create singleton objects directly without requiring a class for them:   What is then that DO stuff? DO is a class  and the Order object is an instance of that class. The declaration of that little class hierarchy looks like this:   Don't bother so much about the match/case stuff. It lets you do pattern matching on the objects and classes. Pattern matching is like if/else statements on steroids. In the  method above we match on the parameter  and if it is an , what is substituted by the ... is executed. If it is not an  is thrown.  We now have two singleton objects, one named  and the other named . The  object is of type  and the  method on the  object takes any  type as parameter. If we send it an  object it will match on the first case clause and whatever goes after the  in the case clause gets executed. Let's define what should go there then.  We begin by creating a class and give that class a method called . The  method should take some list of arguments of some type  on our new class  looks like this. We wait a little with the definition of what the method is actually supposed to do.   The asterisk after the type in the method declaration turns it into a varargs.  Ok, so we have an  class that has a where method and the where method takes a number of  parameters. The pattern match in the create object's  method above should return an instance of this type. The declaration of the  object is now complete and will be:   What is then the  parameter? Let's define a class hierarchy like this:   An  is either an  or an . Sounds close to what we are after, but we are not really there yet. We have to get from  is 1,  is 2 to the  and  objects. That turns out to be simple and if you have followed along you know how to do it. Define objects  and  and give them  methods that returns the type of objects you want. In this case it is instances of  and .   A little strange notation maybe, but case classes come with companion objects  that allows creation of the object without the  keyword. When we now call:   we get an  object that encapsulate the number 1 in the  property.  We still have not defined the supposed return value from the  method on . I guess that we should return something that is some kind of order object. Suppose that we have an order class defined like an ordinary Java bean class. In Scala we can import it under a new name, so that we do not get a name conflict. Remember we already have an Order object.   For now on  is just an alias for the  class and that is an ordinary Java object written in the Java language.  Now we need to be able to create  objects somehow. It would be nice if we just could write `new MyOrder. We create a functor or function object. That is an object that can be called just like a function. It is done in Scala by creating an object with an apply method, in this case like this:   The apply method takes a varargs of  and produces a  instance . It will iterate over the varargs using the foreach method and it will pattern match each  against the actual class and extract the value it is holding. When all values have been extracted it will create the  instance and return it.  When we make a call using the functor the apply method is not visible. We just make the call like this.   The Scala compiler will look for an apply method that takes a vararg of  on the  object and call it. Let's fill in that last ... in the OrderDSLs 'where' method to finish up that class.   That strange  after the op parameter ensures that the vararg is passed along to the  functor.  The code that we have produced so far let's us write   and get one of our domain objects  back, filled in with  and . But we have to put the returned object somewhere. Normally we would put it in a variable, but let's not bother our domain-expert-DSL-writers with that technicality. Let's just put it in a  under a name. This is where the last part with the call to the  method comes in. But the  class does not have a  method, so we are stuck aren't we?  Turns out we are not . Scala allows you to do a trick that looks like you are dynamically adding new methods to an existing class. The trick is to use Scala implicit conversions. That is a really powerful feature that I guess could turn any program into a mess if it is used the wrong way. Implicit conversions allows you to instruct the compiler to convert one object to another if it is required in the context. Sounds strange? In this particular case it would mean that when the compiler sees a call to the  method on the  class and it finds that the  class does not have such a method, it will start looking for an implicit method that can convert the  to an object of some other class that has a  method taking a String as argument.  Let's define that class, , and the convert method, :   This will allow the compiler to convert the  object to an , since it finds the convert method,  and has a  method that takes a String. When the  method is called, the  object is put into a hash map  under the name n. The Scala notation for putting stuff in hash maps is   Quite intuitive actually. Key points to the value and append  key/value pair to the map.  Now we are finished!  Scala is a very powerful language as you can see and it let's you derive your own language from it. As you saw when we follow the pattern 'object' 'method' 'object' 'method' etc. we can define a near-English-looking-DSL. Nice isn't it? I have recently used exactly this technique for developing a simple DSL that let's you define test case scenarios for a web service client application. The bonus here is that since Scala is statically typed, the statements defined by the DSL are type safe and the compiler will help you getting them right!
 This is the second part of my [blog series on reactive programming].  -[readmore]-    Reactive programming is supported by Spring Framework since version 5. That support is built on top of Project Reactor.  Project Reactor  is a Reactive library for building non-blocking applications on the JVM and is based on the Reactive Streams Specification. Reactor is the foundation of the reactive stack in the Spring ecosystem and it is being developed in close collaboration with Spring. WebFlux, Spring's reactive-stack web framework, requires Reactor as a core dependency.   Project Reactor consist of a set of modules as listed in the [Reactor documentation]. The modules are embeddable and interoperable. The main artifact is `Reactor Core` which holds the reactive types Flux and Mono, that implement the Reactive Stream's Publisher interface  and a set of operators that can be applied on these.  Some other modules are: - `Reactor Test` - which provides some utilities for testing reactive streams - `Reactor Extra` - that provides some additional Flux operators - `Reactor Netty` - non-blocking and backpressure-ready TCP, HTTP, and UDP clients and servers - based on the Netty framework - `Reactor Adapter` - for adapting to/from other reactive libraries such as RxJava2 and Akka Streams - `Reactor Kafka` - a reactive API for Kafka which enables messages to be published to and consumed from Kafka  Before we continue, if you want to set up a project and run some of the code samples below, generate a new Spring Boot application using [Spring Initializr]. As dependency select Spring Reactive Web. After importing the project in your IDE have a look at the POM file and you will see that the spring-boot-starter-webflux dependency is added which will also bring in the reactor-core dependeny. Also reactor-test has been added as a dependency. Now you are ready to run the coming code examples.   Reactor Core defines the reactive types Flux and Mono.  A Flux is a Publisher that can emit 0 to N elements, while a Mono can emit 0 to 1 element. They are both terminated either by a completion signal or an error and they call a downstream Subscriber’s onNext, onComplete and onError methods. Besides implementing the functionality described by the Reactive Streams specification, Flux and Mono provide a set of operators to support transformations, filtering and error handling.  As a first exercise, go to the test class generated in your new project, add the following example and run it:  The just method creates a Flux that emits the provided elements and then completes. Nothing is emitted until someone subscribes to it. To subscribe to it, we invoke the subscribe method and in this case we just print out the emitted items. Creating a Mono can also be done with the just method, the only difference being that only one parameter is allowed.  Take a look at the [Flux API] and you will see that almost all methods return a Flux or a Mono, meaning that operators can be chained. Each operator adds behavior to a Publisher  and wraps the previous step’s Publisher into a new instance. Data originates from the first Publisher and moves down the chain, transformed by each operator. Eventually, a Subscriber finishes the process. Note that nothing happens until a Subscriber actually subscribes to a Publisher.  There is an operator called log which provides logging of all Reactive Streams signals taking place behind the scenes. Just change the last line of the above example to  and rerun the test. You will now see the following being added to the output:   Now, to see what happens if you exclude the call to subscribe, again modify the last code line to the following and rerun the test:  As you will see from the log output, no items are now emitted - since there is no Subscriber initiating the process.   Reactor provides a long list of operators and as a help to find the right one for a given use case there is a dedicated [appendix] in the Reactor reference documentation. It is divided into different categories as shown in the table below.  | Operator category | Examples | |---|---| | Creating a new sequence | just, fromArray, fromIterable, fromStream | | Transforming an existing sequence | map, flatMap, startWith, concatWith | | Peeking into a sequence | doOnNext, doOnComplete, doOnError, doOnCancel | | Filtering a sequence | filter, ignoreElements, distinct, elementAt, takeLast | | Handling errors | onErrorReturn, onErrorResume, retry | | Working with time | elapsed, interval, timestamp, timeout | | Splitting a Flux | buffer, groupBy, window | | Going back to the synchronous world | block, blockFirst, blockLast, toIterable, toStream | | Multicasting a Flux to several Subscribers | publish, cache, replay |  Now feel free to go ahead and create some small examples that use some of these operators and see what happens when you run them. For example using the map operator :   Or the zip operator, which zips multiple sources togheter :   As described in my previous blog post, in Reactive Streams errors are terminal events. When an error occurs, it stops the whole sequence and the error gets propagated to the Subscriber's onError method, which should always be defined. If not defined, onError will throw an UnsupportedOperationException.  As you see running the following example, the third value is never emitted, since the second value results in an error:  The output will look like:   It is also possible to deal with errors in the middle of a reactive chain, using error-handling operators:  The  method will emit a fallback value when an error of the specified type is observed. It can be compared to catching an Exception and returning a static fallback value in imperative programming. See the example below:   and the resulting output:  As you can see, using an error-handling operator this way still does not let the original reactive sequence continue , it rather substitutes it. If it's not enough to just return some default value, you can use the  method, to subscribe to a fallback Publisher when an error occurs. This could be compared to catching an exception and invoking a fallback method in imperative programming. If for example a call to an external service fails, the onErrorResume implementation could be to fetch the data from a local cache.  The Reactor Test module provides utilities that are helpful in testing how your Flux or Mono behaves. There is an API called the StepVerifier API that helps out with this. You create a StepVerifier and pass it the Publisher to be tested. The StepVerifier will subscribe to the Publisher when the verify method is called and then it compares the emitted values to your defined expectations.  See the following example:   A StepVerifier is created for the  and two expectations are defined - first one String is is expected to be emitted and then an error should be emitted with the type ArithmeticException. With the verify call, the StepVerifier starts subscribing to the Flux and the flow is initiated.  StepVerifier also has other features such as enabling post-execution assertions and support for virtual time to avoid long run times for tests related to time-based operators.  The Reactor Test module also provides another API, the  which is a Publisher that you can directly manipulate, triggering onNext, onComplete and onError events, for testing purposes.   As you might already have noticed from the log output of the simpleFluxExample, so far our Publisher has been executing on the main thread, just as the Subscriber. This is because Reactor does not enforce a concurrency model. Instead, the execution will for most of the operators continue on the same thread, leaving the choice to the developer. The execution model is determined by the  that is being used.  There are two ways of switching the execution context in a reactive chain: publishOn and subscribeOn. What differs is the following: - `publishOn -  changes the thread from which the whole chain of operators subscribes, based on the earliest subscribeOn call in the chain. It does not affect the behavior of subsequent calls to publishOn  The  class holds static methods to provide an execution context, such as: -  - A fixed pool of workers that is tuned for parallel work, creating as many workers as there are CPU cores. - `single for each call. -  - Dynamically creates a bounded number of workers. It has a limit on the number of backing threads it can create and can enqueue tasks to be re-scheduled when a thread becomes available. This is a good choice for wrapping synchronous, blocking calls. -  - immediately runs on the executing thread, not swithcing execution context -  - can be used to create a Scheduler out of any existing ExecutorService  Run the following example and observe the behavior:  Taking a look at the output  you can see that the first and second map are executed in a thread from Scheduler A, since the first subscribeOn in the chain switches to this scheduler and it affects the whole chain. Before the third map there is a publishOn switching the execution context to Scheduler B, making the third and fourth map being executed in this context . And finally there is a new publishOn switching back to Scheduler A before the last map operation.  As you might recall from the first part of this blog series, backpressure is the ability for the consumer to signal to the producer what rate of emission it can handle, so it does not get overwhelmed.  The example below demonstrates how the Subscriber can control the pace of emission by invoking the  method on the Subscription.   Run it and you will see that two values are emitted at a time as requested:   The Subscription also has a  method available to request the Publisher to stop the emission and clean up resources.  There are two types of Publishers available - cold and hot Publishers. So far we have focused on the cold Publishers. As we stated earlier, nothing happens until we subscribe - but this is actually only true for the cold Publishers.  A cold Publisher generates new data for each subscription. If there is no subscription, data never gets generated. On the contrary, a hot Publisher does not depend on having Subscribers. It can start publishing data without any Subscribers. If a Subscriber subscribes after the Publisher has started emitting values, it will only receive the values emitted after its subscription.  Publishers in Reactor are cold by default. One way of creating a hot Publisher is by calling the `publish method to trigger the emission of values. The Subscribers should then subscribe to this ConnectableFlux instead of the original Flux.  Let's have a look at a simple cold vs hot Publisher to observe the different behavior. In the coldPublisherExample below, the interval operator is used to create a Flux that emits long values starting at 0.   Running this will generate the following output:   Now you might wonder why anything happens when the main thread is asleep, but that is because the interval operator by default runs on the Schedulers.parallel Scheduler. As you can see both Subscribers will get the values starting from 0.  Now let's look at what happens when we use a ConnectableFlux:  This time we get the following output: As we can see, this time none of the Subscribers get the initially emitted values 0 and 1. They get the values that are emitted after they subscribe. Instead of manually triggering the publishing it is also possible to configure the ConnectableFlux so that it starts after n subscriptions have been made, using the  method.      When there is a need to use a source of information that is synchronous and blocking, the recommended pattern to use in Reactor is as follows:   The  method creates a Mono that produces its value using the provided Callable. By using the Schedulers.boundedElastic we ensure that each subscription happens on a dedicated single-threaded worker, not impacting other non-blocking processing.  Sometimes there is a need to propagate some additional, usually more technical data, through a reactive pipeline. Compare this to associating some state with a thread using ThreadLocal in the imperative world.  Reactor has a feature that is somewhat comparable to ThreadLocal but can be applied to a Flux or a Mono instead of a Thread, called a . This is an interface similar to a Map, where you can store key-value pairs and fetch a value by its key. The Context is transparently propagated throughout the whole reactive pipeline and can be easily accessed at any moment by calling the Mono.subscriberContext method.  The context can be populated at subscription time by adding either the  method invocation at the end of your reactive pipeline, as shown in the test method below.   Rector also offers a possibility to create a Flux or a Mono by programmatically defining the onNext, onError, and onComplete events. To do this a so called sink API is exposed to trigger the events. Some different sink variants exist, to learn more about it read further in the reference documentation: [Programmatically creating a sequence]   Debugging reactive code could become a challenge because of its functional, declarative style where the actual declaration  do not happen at the same time. The regular Java stack trace that is generated from a Reactor application will not include any references to the assembly code which makes it hard to identify what was the actual root cause of a propagated error.  To get a more meaningful stack trace, that includes the assembly information ` in your application. This cannot be used in a production environment though, because it involves a heavy-weight stack walking and would have a negative impact on performance.  For use in production, Project Reactor provides a separate Java Agent that instruments your code and adds debugging info without paying the cost of capturing the stacktrace on every operator call. To use it you need to add the  artifact to your dependencies and initialize it at the startup of your Spring Boot application: Reactor provides built-in support to enable and expose metrics both for Schedulers and Publishers. For more details, take a look at the [Metrics] section of the Reference guide.  This blog post provided an overview to Project Reactor, mainly focusing on Reactor Core features. The next blog post in this series will be about WebFlux - Spring's reactive web framework which uses Reactor as its reactive library!   [Project Reactor]  [Spring Web Reactive Framework]  [Reactor Debugging Experience]  [Flight of the Flux 1 - Assembly vs Subscription]
 This blog post is not about [Star Trek Borg].  -[readmore]-   In the presentation at SpringOne2GX we learnt that Google has been running containerized workloads in production for over ten years. Everything at Google runs in containers and today they launch over 2 billion containers per week . The architecture of Borg is summarized in the paper by the following picture:  ![borg-arch]   Back in 2014, Google started to work on a open source version of Borg, known as [Kubernetes] for an introduction. The presentation at SpringOne2GX contained the following architectural overview of Kubernetes:  ![kubernetes-arch]  As you can see Kubernetes has its similarities with Borg. One difference is the database in the lower right corner in the Kubernetes picture. It illustrates some kind of registry for Docker images, e.g [Docker Hub].   Google did a couple of very cool demonstrations during the presentation. First they demonstrated manually scaling up a number of pods  with a command like:  kubectl scale rc "name" --replicas="n"  In the demo Google scaled up from two pods to four pods and it was visualized in a user interface like:  ![kubernetes-arch]  In the picture above we can see two new pods  starting up. After a few seconds they turned into white, i.e. they were operational and ready to receive request form the load balancer.  The second demo was about zero downtime deployment and automatically perform a rolling upgrade of an application . The upgrade was initiated by a command like:  kubctl rolling-upgrade "name" --update-period="wait-time""  *Note:* The  parameter can be used to wait some time between the updates of each instance, e.g. give some time to ensure that everything works as expected before upgrading the next instance...  The screenshot below shows the point in time where the first instance . Once the second v2 instance is operational the remaining v1 instance will be shut down and the upgrade is complete. The load balancer was, of course, automatically updated during the process. The applications web user interface was used during the upgrade process to prove zero downtime.  ![kubernetes-arch]   If you find this interesting you should try it out, it's very easy!  The easiest way to run Kubernetes is on [Google Container Engine] for details.  I tried it out locally on Vagrant and VirtualBox and it was a no-brainer to get my first Kubernetes cluster up and running, prepared to scale and manage Docker Containers!  Once I've got time to deploy, run and scale some co-operating Docker Containers on Kubernetes I'll be back and report my findings.  Stay tuned!
This is the second part of my mini-series on how I used the go profiling and built-in benchmarking tools to optimize a naive ray-tracer written in Go. For part 1, [click here].   1. Sorting efficiently 2. Slices vs Arrays 3. Pointers or values? 4. AVX2 using c2goasm 5. A final word on threading 6. Conclusion  __  This part takes off directly after [part 1].  ![tm]  The full source code for my little ray-tracer can be found here: [https://github.com/eriklupander/rt]  Once I had gotten past all those pre-allocations in section 5 in [part 1], I was starting to run out of low-hanging fruit to optimize. A new heap profile to the rescue!  ![alt]  That  needed closer examination. The ray-tracing code sometimes must sort things, intersections in particular. Order of intersections can be important for things like refraction . Therefore, every time I had found the intersections of a Ray, I sorted them using :   As seen above, at this point, sorting accounted for about 16% of all allocations. The quick-fix was to switch to using  which requires implementation of the  interface:   This required creating a new type for []Intersection that implemented this interface:   I then just had to change a few method signatures to take  instead of  and I could use  which allocated way less memory.  This improvement reduced the duration from **1.9** seconds to **1.6** seconds and decreased the number of allocations by about 3 million.  Which one is better for representing 4 64-bit floating point values in a naive ray-tracer?  I originally implemented my  and  structs with slices for the underlying storage. I tried to read up about the performance-related pros- and cons of using slices over arrays, with the verdict often being "a slice is just a pointer to a type, some index and the underlying array" so it's just as fast.  Well - somewhere in time I decided I needed to test both ways and carefully benchmark both using `go bench` but mainly by looking at total render time.  I started by refactoring the entire codebase to use arrays as backing storage for  and :  **Old**  **New**  I think one of the largest benefits was that I now indexed directly into the underlying arrays instead of accessing  values through either the  or through a  method in mathematical functions. Here's a simple example from code that multiplies a row in one matrix by a column in the another: Honestly, I can't still say for sure exactly why arrays were faster **for my particular use-case**, but the speed-up turned out to be significant. The thing is, this refactoring affected so much of the code base that microbenchmarking individual functions was not very conclusive.  However - in the end, rendering the reference image went from **1.6** to **1.1** seconds. At this point, getting a ~30% decrease in total render time was very welcome.  I also read up a bit on whether one should pass parameters as pointers or values in Go in regard to performance, where the conclusion typically was something akin to "it's better to pass by value  up to a certain size of N bytes" - where N seemed to differ a bit but up to a kilobyte should be fine. Well - I had to test this as well.  I started with a really simple microbenchmark for my   function:   Results: Certainly not a huge difference, the "pass by pointer" one is about 8% faster in this microbenchmark. Given that a function such as the  one is used extensively - for example when determining a pixel's color given its material's ambient, diffuse and specular components - even a pretty small improvement such as this one can help improve overall performance.  Similarly, a benchmark comparing passing a 4x4 matrix and a 1x4 tuple to a multiply function shows an even more significant improvement using pointers:   I implemented this change in some critical code paths in the codebase and got a decent speedup.  New duration was **0.8s** compared to **1.1s** before. Allocations did not change.  I stumbled upon this [excellent article] which got me thinking that I perhaps could identify some bottleneck mathematical operation and try to improve its execution speed by taking explicit advantage of the SIMD, AVX and AVX2 CPU extensions available on most modern x86-64 microprocessors. I havn't been able to determine if the Go compiler itself takes advantage of SIMD/AVX on , but I don't think so at least for my purposes.  For details on this optimization, please check the article linked above. Here's a quick summary on how I went about replacing my  function with a [Plan9 assembly] implementation taking full advantage of AVX2 that we can call without the overhead of using CGO.  Write C code that uses [Intel intrinsics] in order to perform our Matrix x Tuple multiplication:  It's OK if all that C code doesn't make sense. Note that  equals our Go  and that we use  intrinsic function variants for double precision. Comments may provide some insight.  Once we had the C-code above ready - one could imagine we'd use CGO to call it, but that's one of the neat things about this approach - by generating native Plan9 Go assembly, we can basically eliminate the CGO function call overhead. In order to transform this C code into native Go assembly, we first must compile the code into standard x86-64 assembly code using the  compiler:  `clang -S -mavx2 -mfma -masm=intel -mno-red-zone -mstackrealign -mllvm -inline-threshold=1000 -fno-asynchronous-unwind-tables -fno-exceptions -fno-rtti -c -O3 cfiles/MultiplyMatrixByVec64.c`  This generates a  x86 assembly file.  Next, we turn to [c2goasm] for generating Go Plan9 assembly callable from a .go file.  `c2goasm -a -f cfiles/MultiplyMatrixByVec64.s internal/pkg/mat/MultiplyMatrixByVec64_amd64.s`  The resulting  contains our Go Plan9 assembly and looks like this :  Finally, we need some hand-written plain Go-code to glue the assembly in  to our ordinary Go code:   Once here, we can call  just like any other Go code.  So, how much faster is this code compared to our vanilla  we benchmarked previously? A new microbenchmark tells us this:   Result : More than twice as fast! Given that  is called several times on every ray / shape intersection test, this fix should definitely provide a nice speedup:  Rendering the reference image now took **0.6s** compared to **0.8s** before.  I've also played around with implementing the  and  product functions using the same approach.  turned out to be significantly _slower_ when done using intrinsics, while  was interesting. The microbenchmark showed the intrinsics version to be maybe 15% slower than the pure Go one, but a real render of an image was actually a few percent _faster_ when the Plan9 assembly verison  product was used. If anything, it serves as a good reminder that one should be careful drawing conclusions from microbenchmarking isolated functions - the performance of the program as a whole is the most important metric to look at when chasing optimizations.   With the assembler optimizations above done, I was more or less "done" with my optimizations. However, the topic of _multi-threading_ performance in the ray-tracer remained somewhat of a mystery to me. We saw that the initial improvement  was about **2.5x**. How does multi-threading work out after all other optimizations?  ![spreadsheet]  __  About **3.5x** faster using all available 8 thread/workers compared to a single worker. But one must note that there's very little improvement once we move past 4 threads on my machine.  I havn't figured this one out exactly, but I believe it boils down to a number of things:  1. Having a CPU  are probably shared within a single core. 2. Memory bandwidth? We're still allocating and de-allocating a substantial amount of memory. Perhaps the memory subsystem is holding the CPU back? 3. I've spent next to no effort actually thinking about _efficient_ multi-threading or optimizations on the CPU-level. I.e. things like [optimizing for CPU cache size]. In other words - my implementation could very well be a bad one as far as efficient multi-threading is concerned.  I've also run the ray-tracer on a more modern Intel Core i9 8 core / 16 thread MacBook Pro 2019 and on a desktop computer with an AMD Ryzen 2600X 6 core / 12 thread CPU, seeing similar behaviour where performance improvements are negligible after `worker count` > `num of physical cores`. However, I do remember running the ray-tracer on my Ryzen 2600X with the memory clocked to 2100 Mhz instead of the normal 3200 Mhz. I did notice that the CPU usage was down to 60-70% per core instead of the >99% I saw with the memory at its normal speed, which could indicate memory bandwidth or latency being a culprit. Perhaps I'll do a follow up on this particular topic!  I'm sure there's someone out there who could shed some light on this issue. Feel free to use the comments!  After all these optimizations , rendering a complex scene with tens of thousand of triangles, multi-sampling, soft shadows etc had become possible in reasonable amounts of time. This 1920x1080 render of a dragon took about 20 minutes:  ![dragon]  First a little reminder that the findings in this blog post are strictly anecdotal and specific to my use case - the naive ray-tracer originally written without performance in mind. Your mileage in other circumstances may vary!  I'm also well aware there's a lot more one can do such as more comprehensive [escape analysis] book.  To give an overview of this optimization journey, the following diagram gives a pretty good view:  ![optimizations]  Clearly, the single most important fix was caching the Inverse, probably followed by multi-threading and generally avoiding re-allocating structs and slices in the renderer. The performance gains from slices vs arrays, pointers vs values and sorting were not as clear, but together they did indeed provide a substantial gain. The intrinsics and Plan9 assembly was perhaps stretching things a bit, but nonetheless a fun thing to experiment with.  That's it! Hope you enjoyed reading my ramblings on ray-tracer performance optimizations in Go.  Feel free to share this blog post using your favorite social media platform! There should be a few icons below to get you started.
 In a project I have been working in recently a strange problem appeared.  We had a third-party dependency provided by the application server and as a consequence the same dependency marked as provided in our maven build script. In one of our own classes we called a static method in class supplied by the third-party dependency.   The application compiled and run perfectly on my development environment and all the tests passed nicely so I happily deployed it to the company test server. Clicking thru the application suddenly something went wrong and I had a nice error page on the screen. After consulting the server log my only clue was the following exception:   For a while I was pretty confused since I was certain of the existence of both the class and the method on the server. And after trying to figure out what could differ between the two environments the only thing I could think of was, me running a community edition of the third-party dependency while the server was running an enterprise edition of the same dependency . Surely this couldn’t be a problem? The different editions were still from the same code base and had the same version?  But since this was my only idea I had to dig into it, so I decided to disassemble CompanyThreadLocal on the server, using javap, and take a look at the method signatures. The class was embedded in a jar file but this little script fixed it for me:   The following output told me that the class and the method existed as expected on the server:   Then I did the same thing on our own class but used the –verbose switch on javap instead. An excerpt of that output:   Breakthrough!! Our class was expecting return type J from Foo.bar. To verify I did the same disassembling on my local Foo which confirmed my suspicions that the Enterprise Edition of Foo differed from the Community Edition. So even thou they had the same version number they differed in patch level.  Case closed!
 In this blog post we will add a [Zipkin server] can simplify both creation of trace events and sending them to the Zipkin server.  -[readmore]-  ![Zipkin Server]   The more cooperating microservices we get in our system landscape, the harder it gets to understand what's going on. Specifically, when the load increase and response times starts to get slower than expected...  To get a better understanding of what is going on we need to be able to trace a request as it pass through a number of cooperating microservices and measure the processing time in each microservice that is involved in responding to the request.  The trace events for one request must be collected, grouped together and presented in an understandable way. This is exactly what [Zipkin] is about!  A request through our system landscape can be visualized as:  ![call-graph]  Zipkin can visualize the processing time in each involved microservice for a sample request as:  ![Zipkin-sample]  Zipkin originates from Twitter .  The terminology used by Zipkin is based on a paper written by Google: [Dapper, a Large-Scale Distributed Systems Tracing Infrastructure].  [Spring Cloud Sleuth], to simplify both creation of trace events and sending them to the Zipkin server.   A *trace* represents the whole processing of a request. A *span* represents the processing that takes part in each individual microservice, as a step in the processing of a request. All trace events from processing a request share a common unique *Trace Id*. Each individual processing step, i.e. a *span*, is given a unique *Span Id*.  For further explanation of traces and spans see the [Spring Cloud Sleuth documentation regarding terminology].   Spring Cloud Sleuth intercepts incoming requests and either picks up the trace id from the incoming message or creates a new trace id if none was specified. For outgoing requests, it fills in the current trace id in the outgoing message. Both synchronous requests and asynchronous messages are supported, using HTTP for synchronous request and a Spring Cloud Stream binder  for asynchronous messages.  Spring Cloud Sleuth also:  1. handles creation of spans and span id's and sends them to the Zipkin server. 2. if the internal processing uses multiple threads, e.g. a Hystrix based circuit breaker, Spring Cloud Sleuth is capable to move thread local information, e.g. the trace id between the threads. 3. interacts with the logging framework used in each microservice  and adds the trace id and span id to each log record, see below for an example.   Setting up a Zipkin server is straight forward, similar to how you setup the other Spring Cloud servers, e.g. a Discovery, Edge or Configuration server:  1. The Zipkin server runs as a conventional Spring Boot application 1. The Gradle dependencies that enables the Spring Boot application to be a Zipkin server are:  compile "org.springframework.cloud:spring-cloud-sleuth-zipkin-stream" compile "org.springframework.cloud:spring-cloud-starter-sleuth" compile "org.springframework.cloud:spring-cloud-stream-binder-rabbit" runtime  > **Note:** The -suffix on the `spring-cloud-sleuth-zipkin-stream ` dependency enables the Zipkin server to receive trace events using an event bus based on . The dependency  binds the Zipkin server to use RabbitMQ as event bus.  1. The server only contains a single Java class:  and the only thing of interest in it is the following annotations on its `static main` - method:  @SpringBootApplication @EnableZipkinStreamServer  1.  is, as usual, what makes our application a Spring Boot application.  1. `@EnableZipkinStreamServer ` is what makes it a Zipkin server that is able to: 1. Receive trace events synchronously over HTTP 2. Receive trace events asynchronously over Spring Cloud Stream 3. Present a Web based UI for searching trace events manually 4. Present a RESTful and JSON-based API for searching trace events programmatically 5. Store trace events in memory  > **Note:** It is possible to configure a Zipkin server to store the trace events in a database, e.g. MySQL or Elasticsearch.  That's not bad for a single annotation, right?  The source code of the Zipkin server also contains:  1. Standard configuration files  > **Note:** The Zipkin server does not use the Configuration server at this time, to be fixed in a future blog post :-)  1. An empty integration test class, that verifies that the Zipkin server can start  1. A conventional Dockerfile  For further details see the folder  in the source code.   To enable our microservices to send trace events to the Zipkin server we have added the following dependencies in our Gradle build files:  compile compile  These dependencies will enable our microservices to stream tracing events asynchronously to the Zipkin server over the event bus.  To ensure that we send all trace messages to the Zipkin server we have added the following configuration to the common config file, , in the config repo:  # 1.0 means 100%, see http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling spring.sleuth.sampler.percentage: 1.0  > **Note #1:** To promote loose coupling we prefer sending trace events to the Zipkin server asynchronously over the event bus instead of sending events synchronously using HTTP directly to the Zipkin server. > > **Note #2:** From earlier blog posts we already have dependencies and configuration in place that binds the microservices to use RabbitMQ as the event bus. > > **Note #3:** A system with high traffic volumes can produce large amounts of trace events. It might be sufficient to only collect some of the events to understand what is going on. This can be achieved by lowering the value of the property . > > **Note #4:** The name of the microservices in the trace events are taken from the property  in each microservice local - file.  **A final note on use of Spring's RestTemplate:** To make it possible for Spring Cloud Sleuth to add tracing headers to the outgoing requests it is important that the RestTemplate is injected and not created directly in the code, e.g. not using `new RestTemplate`.  For a correct usage see :  private final ServiceUtils util; private final RestOperations restTemplate;  @Inject public ProductCompositeIntegration { this.util = util; this.restTemplate = restTemplate; }    For details on how to build and run the microservice landscape in this blog post series, see the [blog post #5].  > **Note:** To be able to run some of the commands used below you need to have the tools [cURL] installed.  In summary:  1. Open a terminal, create a folder of your choice and  into it:  $ mkdir a-folder-of-your-choice $ cd a-folder-of-your-choice  1. Since we have externalized our configuration into a configuration repository we first need to get it from GitHub:  $ git clone https://github.com/callistaenterprise/blog-microservices-config.git  1. Next, we get the source code from GitHub and checkout the branch used for this blog post:  $ git clone https://github.com/callistaenterprise/blog-microservices.git $ cd blog-microservices $ git checkout -b B10.1 M10.1  1. Now we can build our microservices with:  $ ./build-all.sh  1. Finally, we can bring up the dockerized microservice landscape and run a test:  $ . ./test-all.sh start  After a while, it should result in a response from the API request like:  $ curl -ks https://localhost:443/api/product/123 -H "Authorization: Bearer $TOKEN" | jq . { "productId": 123, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ], "serviceAddresses":  }  > **Note #1:** We will not shut down the microservice landscape . Below we will use it to try out the Zipkin server.  > **Note #2:** The first  in the command above is essential. It allows us to reuse the  environment variable that the script creates to store an OAuth Access Token, i.e. we don't need to acquire one ourselves.  1. Try a manual call like:  $ curl -ks https://localhost/api/product/123 -H "Authorization: Bearer $TOKEN" | jq . { "productId": 123, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ], "serviceAddresses":  }    Before we start to test Zipkin let's inspect a log event a bit closer, e.g.:  composite_1 | 2017-07-17 07:27:28.678 INFO [composite-service,8f79b9c84065ccd8,529909c553bfed03,true] 1 --- [ XNIO-2 task-9] o.s.b.a.s.o.r.UserInfoTokenServices : Getting user info from: https://auth:9999/uaa/user  > **Note:** You can see you log events with the command: `docker-compose logs -f`  The part  comes from Spring Cloud Sleuth with the following information:  1. The name of the component, e.g.  Taken from the  - property as described above.  1. The trace id, e.g.  If you search for a trace id in the log output you will find all log events from the involved microservices related to the processing of one request.  1. The span id . If you search for a span id in the log output you will find all log events from a single microservice. > **Note:** The id of the first span in the processing of a request, known as the root span, is used as the trace id.  1. A boolean known as exportable, e.g.  The value  indicates that the span will be reported to Zipkin.   Now, make some calls to the microservice landscape:  $ curl -k -H "Authorization: Bearer $TOKEN" https://localhost/api/product/456  Let's first take a quick look into RabbitMQ to verify that trace events are sent to the Zipkin server  in your web browser and log in using *guest/guest* and you should see something like:  ![rabbitmq-trace-events]  Now open the Zipkin Web UI in your web browser: [http://localhost:9411]:  ![Zipkin-1]  Select the  and ensure that you have set a proper time interval and click on the "Find Trace" - button:  ![Zipkin-2]  A number of traces are displayed. If you click on one of them it will expand and show something like:  ![Zipkin-3]  > **Hint:** Click on the "Expand All" button to see the complete call graph.  I guess the graph is self explanatory :-)  To find a specific request you can use the query field to specify the HTTP request URI   Go back to the initial search dialog an enter  in the query field:  ![Zipkin-5]  Click on the "Find Trace" - button and you should see traces that match the query you entered:  ![Zipkin-6]  Click on one of the matched traces and verify that the HTTP request URI matches:  ![Zipkin-7]  Also try out clicking on the "Dependencies" tab:  ![Zipkin-4]  Could be useful in a larger microservice - landscape, don't you think?   The Web UI uses a query API in the Zipkin server that is public, i.e. you can use it to build your own search front end. For details see: [http://zipkin.io/zipkin-api/#/]. Let's try it out!  > **Hint:** Prepare for receiving large responses! They are not expected to be consumed by the human eye, but programmatically :-)  Start with repeating the query for a specific HTTP request URI:  $ curl http://localhost:9411/api/v1/traces?annotationQuery="http.path=/api/product/456" | jq .  Next try searching for a specific Trace Id :  $ curl http://localhost:9411/api/v1/trace/917e8549d0324e3f | jq .  Finally lookup all traces where a specific microservice has been involved:  $ curl http://localhost:9411/api/v1/traces?serviceName=edge-server | jq .  If you want to refine the traces-queries you can experiment with the query parameters: ,  and . Se the [API docs] for details.   Examples for the interested reader to try out on yourself:  1. Make a microservice slow by increasing its response time  1. Force the Circuit Breaker to Open its Circuit and verify the shorter response time when "fail fast" is applied  > **Hint:** See section 3.3 - 3.7 in [this blog post] for instructions.   Next up in the [blog series - Building Microservices], is centralized logging with the ELK stack. Thanks to trace ids introduced by Spring Cloud Sleuth in the log events we are already half way, stay tuned...
 As [Groovy ] users.  In terms of refactoring, IntelliJ is still outstanding for Groovy developers. But there is a solution to the basic needs for Eclipse developers. IBMs mash-up platform WebSphere sMash hosted by the open source [Project Zero ] bundles an Eclipse editor and debugger for Groovy. Although developed under the umbrella of Project Zero, the Groovy plug-in is generally applicable to any Java project using Eclipse that needs to integrate Groovy development.  Using this plug-in, you can add Groovy coding to any project. It even supports WTP, with full dynamic compilation. As with Java classes, you don't need to rebuild / redeploy / restart Tomcat while debugging web applications in WTP. Just edit, save and refresh in browser.  Project Zero does not provide documentation on how to set it up for general use , so I thought I share my experience of doing so. It is fairly straight forward. The checklist assumes that you have a Java/Java EE project in your Eclipse workspace. I've used the Ganymede Enterprise Development distribution of Eclipse 3.4.  Here we go:  1. Use the update manager in eclipse to install the sMash plug-in for Eclipse: [http://www.projectzero.org/zero/silverstone/latest/update/zero.eclipse/] 2. Add the Groovy nature to your Java project using the context menu (right-click on the project / **Groovy/Add Groovy Nature** 3. In the project properties dialog: * Select **Java Build Path** and add your Groovy source folders  * Select **Groovy Project Properties** and update the **Groovy compiler output location** if needed. In case you build with maven, change it to . 4. In case you changed the **Groovy compiler output location** in the project properties dialog, you need to clean the project, to make it "hit". Use menu **Project/Clean..**.  Happy Groovy debugging in Eclipse!
 Just came home from a very interesting conference a couple of days ago. I still feel a bit jet lagged after the long trip over the atlantic ocean from Fort Lauderdale, Florida, but it was really worth the effort.  Fort Lauderdale was greeting us with a comfortable wintery weather with temperatures at around 25°C so it was very refreshing to take a walk outside during the lunch breaks.  The conference was hosted by No Fluff Just Stuff whose conferences aim to be stuffed with as much information as possible without the usual exhibition and advertisements seen at many other conferences. I think they have a really good concept there, which makes it easier to connect with fellow software engineers without worrying about what they are selling.  -[readmore]-  The Rich Web Experience is a conference aimed at web client development in all it forms; browser web applications, mobile web applications, native mobile apps etc etc. It was this year colocated with the Project Automation Experience where the focus was more on build systems, CI, cloud computing etc.  Since I have a big interest in client development, most of the sessions I attended were from the Rich Web Experience.  One of the most interesting things that I noticed was that almost none at all was talking about server generated HTML anymore. Almost everyone talked about self contained JavaScript/HTML5 clients consuming services published by their respective back end systems. I attended a presentation hosted by Dan Allen from JBoss who was presenting his idea of future mobile web applications and even he  looking at a solution based on GWT, Errai and POH5, Plain Old HTML 5. When I asked him a direct question about the future for JSF he was a bit slippery but he said it is definitely time to start looking around on what's out there and that it wasn't impossible that JSF would transform to something else in the future, unknown what.  I also had a quick after session chat with Dylan Schiemann, co-founder of Dojo Toolkit which was really inspiring. He said they are working on a new Grid component for Dojo called dgrid which sounded very interesting. It is designed to be more light-weight and way faster than previous grids to fit even into mobile devices.  Other highlights from the conference was Venkat Subramaniam's iOS and JavaScript sessions, Tim Berglunds poetic coding examples, Gabriel Dayley's and Tom Valletta's excellent talks on HTML5, node.js and WebSockets and of course Eric Wendelin's JavaScript test automation presentation.  I can really recommend the RWX 2012 if you are interested in client development.  Some links for the interested reader to look more at:  - [http://mustache.github.com] – Logic less templating system, amongst others for JavaScript - [http://requirejs.org] – JavaScript file and module loader - [http://documentcloud.github.com/underscore] – Utility-belt library for JavaScript - [http://nodejs.org] – Server side JavaScript, a must have - [http://maqetta.org] – WYSIWYG editor for HTML5, looks interesting at first glance - [http://coffeescript.org] – A language that compiles to JavaScript and adds syntactic sugar. CoffeeScript is almost to JavaScript what Groovy or Scala is for Java - [http://www.dartlang.org] – Google's new language for the web - [http://www.html5rocks.com/en/tutorials/device/orientation]. - [https://github.com/SitePen/dgrid] – Home of the Dojo dgrid project - [http://spinejs.com] – A minimalistic MVC utility for JavaScript - [http://code.google.com/p/js-test-driver] – Drives your JavaScript tests from the browser - [http://pivotal.github.com/jasmine] – BDD style tests for your JavaScript application - [http://www.jboss.org/errai] – A framework for building GWT applications.
 During last week I was in the US visiting Rich Web Experience conference. It was very nice to get some extra summer feelings and runaway from the dark and rainy November in Europe. The conference was very good in total: well organised, skilled speakers and valuable contents justifying the name of the conference. In other words despite of the jet lag it was worth spending time and energy to attend this conference.  Different talks and sessions from the conference inspired me to write about both new and old topics in the landscape of front-end development. Let’s start with a new one, the hottest one: **The next generation of beloved and hated AngularJS**.   The next generation of AngularJS === AngularJS 2.0 is knocking on the door. It's a significant departure from Angular 1.x with no migration path. We are simply talking about a totally new framework. **There is no release date set yet, but the consensus is that version 2.0 will be ready late 2015**.  Many comments that I have heard during coffee breaks and some sessions were like: “Angular 2.0 is written with a new language called AtScript”. This is both true and not, because Angular 2.0 comes with AtScript that is actually a superset of TypeScript with addition for annotation and introspection capabilities. Basically AtScript is like TypeScript wherein additions to the native JavaScript syntax are optional.  ![AtScript_TypeScript.png]  For those who are skeptical about the need for such an extension to JavaScript you don’t need to use AtScript at all. You could write an Angular 2.0 app in ES5 if you like. Observe, it is not recommended.  The Angular team have always had a lot of respect for the TypeScript, now it’s just being confirmed with the upcoming marriage. The feelings shared developer-to-developer varies a lot, some are welcoming this change while many others are sceptical.  Here is a code snippet that shows what AtScript will look like in the context of an Angular 2.0 directive:  <exp-hello></exp-hello>  @ComponentDirective({ selector: 'exp-hello', template: 'Hello world!' })  A few other points to note about AtScript: - It will transpile to both JavaScript and Dart. Thus, with Angular 2.0, both AngularJS & AngularDart will share a common code base - Since AtScript is a superset of TypeScript, TypeScript is valid AtScript . Roadmap for AtScript includes full alignment with TypeScript and an eventual proposal to ECMA as a standard . - There has been talk already with the major IDEs to support these language extensions. Since TypeScript already has pretty good support in various IDEs, the addition of annotations would not be a huge undertaking   Angular 2.0 kills off many APIs of 1.x, at the writing moment we know that following is going to disappear: - Controllers - Directive definition objects - $scope - angular.module - jQlite  All completely gone. Pretty radical, so what’s left? This is very upsetting news for all developers that have invested their soul and heart to understand and gradually master the current APIs.  Bottom line, version 2.0 is a fundamental re-think of Angular from the ground up. Let’s hope it will deliver what it promises: "Simply better experience for the developers".  My personal opinion is that this fundamental change will drive adoption for the next generation of Angular application developers. The changes are happening for a good cause, the major goal with 2.0 is to improve development experience for Angular apps. Even if it hurts a lot this gives us developers some very bright hopes, let’s wait and see.  Watch out for announcements and alot more details on the [status page of Angular 2.0]
 Working in a continuous delivery environment can feel a little daunting - any changes you make will be rapidly delivered to your production environment. Although the intention is to provide immediate benefit for your customer, without proper risk management there is a real risk of exposing bugs and triggering outages. In this blog post I will look at one strategy that uses metrics to reduce those risks.  The scenario starts with a familiar email:  > "Hi team. According to us you are consumers of V1 of our service. We have just released V2 which includes a wealth of > great new features. We have deprecated V1 and would appreciate your help to remove this as soon as possible".  Our application used V1 to fetch data to provide a rich customer experience. In certain cases the data was not available and to handle this we applied a fallback. This feature was not mission critical but as we were proud of our customer experience we much preferred the rich data over the somewhat ugly fallback.  As we had a little time to spare we decided to prioritise the migration to V2. This meant that we would be the first users of V2 in a customer facing environment. Being early adopters carried with it the risk that we would be exposed to any unfound bugs in the new service.  The API itself had been restructured - fields had been renamed, moved and extended. After some discussions with the service providers and after a few manual experiments we made some assumptions about the behaviour of V2. These assumptions were difficult to test in our staging environment and could only first be proven when running at scale in production. If our assumptions were incorrect then the customer would be the first to know.  With these risks now identified we decided that switching from V1 to V2 in a single step was not an option. We decided to split the migration into a number of small steps where any negative consequences of the migration could be hidden from the customer until we were happy we had gotten it right. Each step was considered complete when the content of that step had been delivered to the production environment. As a side note this strategy allowed the team to re-prioritise as other needs arose without blocking the delivery pipeline and without the need for complex merges in a migration branch.  The first step was to add an endpoint to our application where internal users  could make a call that in turn only called V2. This step allowed us to test our client code and configuration - here we picked up, for example, that the credentials used by V2 had been deployed incorrectly which would have caused an outage had we been running at scale.  We were now ready to run V2 at scale however we were still worried about whether the service would behave as we had assumed. To deal with this we decided to call both V1 and V2, to expose the result of V1 to the customer and to throw away the V2 result. We added a metric - in this case a [Counter] - to distinguish whether our V2 call managed to fetch rich data or was forced to choose the ugly fallback response. This [Counter] was then transformed into a [Hit Rate]:  Hit Rate = sum of rich responses /   The Hit Rate was added to one of our dashboards and displayed prominently on a monitor for the entire team to monitor.  We now realised that we had never determined a lower boundary for an acceptable Hit Rate, or gone as far as to formalise this metric as a service level indicator (for more on SLIs see the [Google Site Reliability]. Instead we guessed - a Hit Rate of 95% would be acceptable to us. Imagine our shock when the Hit Rate for V2 fluctuated as low as 60%!  Obviously something was not behaving as expected, but was the result for V2 better or worse than V1? After a quick retrofit to add metrics to V1 we found a Hit Rate of around 80% - still significantly less than our lower boundary of 95% but notably better than V2. With the help of some targeted logging we could then locate a number of bugs in the V2 service and identify some incorrect assumptions in our application. We even located a bug in V1 that had crept into V2. After these fixes the hit rate of V2 was consistently above 95%.  Before exposing our customers to V2 we monitored the Hit Rate for a period of time to see how our application behaved during the peaks and troughs of our business cycle. This involved no effort other than casting an occasional glance at our dashboard. A few requests were timing out at peak traffic but this was easily resolved by adding a number of retries and did not significantly impact the Hit Rate.  Now we felt confident that we could expose the customer to V2 and to remove V1. The switch was made and we continued to monitor the Hit Rate of V2. Migration had been a success and the customer actually ended up with a richer experience than they had before without being exposed to any bugs or outages on the way!  The final step was to perform some clean up. We removed our V1 client and informed the service providers that they could safely revoke our credentials and mark us as migrated.  We also removed the metrics associated with the Hit Rate for V2. This feature was not considered mission critical. The effort required to maintain and monitor the Hit Rate was not justifiable - there was always the risk that by keeping the metric on the dashboard any visible degradation would trigger effort that may be better applied to more mission critical activities.  By using metrics we were able to confidently make potentially risky changes, at each step gauging the impact quickly and visibly in a production environment yet without harming the customer experience. These metrics allowed us to learn about the behaviour of the new service. These learnings were used both to plan our next step and to provide feedback to the service provider allowing them in turn to improve quality for all consumers. Although this process contained significantly more steps than a [big-bang adoption] the effort involved was not significantly different  and avoided the uncertainties and stress associated with a rollback had things gone wrong.
 If you need to automate a fairly complex process - like a batch job - Groovy may come in handy. Designing a Java batch job is typically a task that involves the following mechanisms:  - Job control infrastructure that triggers the job as a shell command - A script that  that initializes the class path and triggers the main class - A main class that implements the job control logic 1. Create POJOs from input XML 2. Execute the batch business logic by invoking service pojos in multiple components  3. Produce the output XML files  It is generally cumbersome to realize most of these mechanisms and steps in Java. Ant is nice for very high-level tasks, but doesn't solve the jar distribution problem in a nice way. Batch processing logic typically involve processing that is complex in Java - or at least complex to set-up in Java: Parsing XML into POJOs . We may not even have a schema. Next, batch processing in Java is often about comparing, sorting, merging and filtering of POJOs, an creating new result structures to be serialized in XML. If you once did this kind of processing in Smalltalk or Ruby, you start dreaming of what you could have done in your spare time if you didn't have to work around the clock coding Java iteration logic.   Well, Smalltalk or Ruby may not be an option, if you are required to solve your problem without impact on what's deployed on your batch execution environment. Groovy is a language built on the JDK, that shares a lot .  Groovy is not only a dynamic language for the JVM - it also has the JDK as its object model. Your Java code and Groovy code shares the same object model.   To simplify packaging and class-path management of the batch and all its required jars . This will include a dependency to the Groovy runtime jar. That way, we can bootstrap our scripting language at the same time as we resolve all dependencies of the Groovy batch script. The complete distribution of the batch script will consist of the following artifacts:  - The bootstrapping ant script - A maven POM that declares the dependencies - The Groovy script file   The best of all: The ant script  is as simple as this:   The Maven POM is just as minimalistic. It simply declares the dependencies:   And finally, the Groovy script named :   The shell command: 
 A really useful plugin to Eclipse is [FindBugs], install the plugin and you can actually find bugs in your code.  It's a good tool to use during projects, but also when performing code reviews. You can get a feeling quite quick on areas that are containing more problems than others. You can use the plugin and run it on a whole project on or a separate java file. It's quite fast running it on a whole project, so just go ahead and install it.  The Find Bug plugin comes with a bug explorer:  _Bild saknas_  Here you can see the severity of the bugs and select the ones you think will cause problems. The code is also marked with a bug:  _Bild saknas_  Another good plugin to be used during development is the [EclEmma], Java Code Coverage for Eclipse. With the plugin you get a really colorful Eclipse environment, read rows are not tested and green are tested.  _Bild saknas_  It's a really easy way of getting coverage on module or on a specific class. If you have goals that some percentage should be reached on your code, it's quite easy to get the coverage.  Just keep in mind that there is no use to add a lot of "unnecessary" testing just to get the coverage percentage up.
 Yesterday I attended the [Software Craftsmanshop 2010] is claimed to be the birthplace of modern computing with machines such as the Colossus decoder as well as the Turing-Welchman Bombe machine which cracked a massive amount of enigma codes during WW2. Really cool stuff.  So what about the conference itself? Well, this was not an ordinary conference with speakers and keynotes. No this was all hands-on with 130 delegates and their computers  coding. Personally I attended these three sessions; the Refuctoring Master Class, The Refactoring Golf and the Robot tournament. This is a brief writeup of what was going on in these sessions.  It started off really great with Jason Gormans tutorial on "refuctoring" . Jason started up showing us some good examples of mortgage driven development, the way to make code so unreadable that no one other than yourself can maintain it. The way this was illustrated was to take the hello world example and gradually refactor the code into an almost unreadable form. Then we all paired up to continue this "code scrambling" task. The next step was to switch pairs and let a new partner add some new requirement into the code, which for many in the group obviously had a hard time accomplish. I must admit that there were some circular dependencies I was particularly proud of...  After a short break it was time for a round of golf, ahum, refactoring golf I meant to say. This session was about , as in real golf, to take on a couple of "holes" and finish off with as few points as possible. A hole in this case was a code example in two states, one "before" and one "after". The task was to refactor the before-code to equal the after-code in as few steps as possible. You were getting various points on every move you'd make and the pair with the miminum score for each hole was up on display to show their moves. The task at hand was in itself quite challenging and the session showed us that it is important to know your IDE and its possibilities to help you write clean code. I can't tell if this was the real intent of the session but there were at least a couple of aha's for me in that one.  After a heavy lunch of english sandwiches and a fascinating tour of the Bletchley Park premises it was time for the last session...  It was time for war. Time to fight. Time to separate the men and women from the chickens. It was time... for the Robot tournament. Ok, hm, after that intro, obviously there were about 50 computer geeks in the room so how cool could it be, eh? Well it turned out to be pretty cool actually. To describe the battles in more detail, it was all about playing games  where all contenders went up against each other. This was realized on a server setup by the session leader, Matt Wynne. Every contender could code their robot in whatever language they preferred as long as it could be run with a bash script on Matt's Linux machine. The robot was then "curled" to the server and every now and then a round of matches started where every robot was up against each other whereafter a score board with the results was presented. The purpose of the session was to illustrate the notion of continous delivery, starting simple so you can release early and often, not striving for perfection before relasing anything. At the end of the session Matt posed the question how many of us used unit testing to build our robots and how useful those tests were. In my opinion, starting off with a test made understanding how to solve the task in the best way much easier and also made me deliver value much faster. It was a really engaging session with lots of adrenaline...  All in all, Software Craftsmanship 2010 was a pretty great conference with lots of great and inspiring people. I am definately going home with some new experience in my portmanteau. One thing though, there are a lot of people travelling around conferences worldwide advocating the software craftsmanship way of being. Where were they this sunny day of hands-on sessions in Bletchley Park?  Thanks to @jasongorman for organizing this event!
 Apache Avro is becoming a popular choice for Java Object Serialization in Event Driven Architectures using Apache Kafka, due to its compact binary payloads and stringent schema support. If combining Event Notification using Kafka with traditional Request-Response, it is convenient to use the same serialization mechanism for the domain objects, regardless of if they are part of events emitted over Kafka or requested through a REST API. Here's how to do that in a Spring-MVC REST environment.  -[readmore]-  [comment]: #  [Apache Avro]: https://avro.apache.org/ [Apache Kafka]: Https://kafka.apache.org/ [Confluent]: https://www.confluent.io/ [Schema Registry]: https://docs.confluent.io/current/schema-registry/docs/index.html [Event Driven Architectures]: https://martinfowler.com/articles/201701-event-driven.html [Google Protobuf]: https://developers.google.com/protocol-buffers/ [Avro RPC]: https://github.com/phunt/avro-rpc-quickstart [Spring Boot]: http://projects.spring.io/spring-boot/ [Kafka Benchmark]: https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines [github.com/callistaenterprise/blog-avro-spring]: https://github.com/callistaenterprise/blog-avro-spring  [Event Driven Architectures] are becoming increasingly more popular, partly due to the challenges with tightly coupled micro services. When streaming events at scale, a highly scalable messaging backbone is a critical enabler. [Apache Kafka] is widely used, due to its distributed nature and thus extreme scalability. In order for Kafka to really deliver, individual messages needs to be fairly small . Hence verbose data serialization formats like XML or JSON might not be appropriate for event notifications.  While there are several serialization protocols offering compact binary payloads , [Apache Avro] is frequently used together with Kafka. While not necessarily the most elegant framework, the [Confluent] Kafka packaging provides a [Schema Registry], which allows a structured way to manage message schemas and schema versions, and the Schema Registry is based on Avro schemas.   Avro schemas can be defined in two ways: In JSON syntax, or in *Avro IDL*, a custom DSL for describing datatypes and RPC operations. While the JSON syntax might seem more appealing, it lacks a decent **include** mechanism, making it hard to decompose and reuse common datatypes between schemas. Hence Avro IDL seems to me to be the syntax most appropriate for serious use. Below is a simple example of an Avro IDL schema, defining a Car type with a mandatory VIN and an optional Plate Number:   An Avro schema may be used in runtime  or compiled into language-specific bindings for e.g. Java. The following is an example of a Maven configuration snippet to feed Avro schemas through the Avro IDL compiler:   When feeding it through the IDL compiler, a corresponding Java class is generated:   The resulting Java class can then be used to efficiently serialize and deserialize Java objects to and from byte arrays . Using the resulting Java classes when reading from or publishing to Kafka topics is straight forward.   But what if you want to use the same Schema definitions in your RESTful API ? Avro comes with a proprietary RPC mechanism [Avro RPC], with an http server implementation built on top of Netty. But such a mechanism doesn't easily integrate with other REST frameworks, like e.g. a [Spring Boot] application. I was a little surprised to find that there seems to be no formal support in neither Avro nor Spring to easily integrate Avro serialization with the *HttpMessageConverter* abstraction of Spring MVC.  Luckily, this can fairly easily be done using the existing Avro Serializer/Deserializer framework and the Spring MVC interfaces. Let's see how :   Lets start by defining two generic interfaces for serialization and deserialization to and from byte arrays:    Now let's use the **org.apache.avro.specific** mechanism to implement serialization and deserialization for all Java classes generated from Avro IDL :   A bit verbose, but nothing fancy in there, just the boiler plate code for using the Avro **org.apache.avro.specific** mechanisms.   Next step is to provide an implementation of Spring MVC's *HttpMessageConverter*, using the Avro serializers. The *AbstractHttpMessageConverter* base class provides most the boiler plate code necessary, so we just needs to complement it with what Mime types and Java types the MessageConverter supports, and the actual conversion to and from those types. We'll do it in two different flavors, to support binary or JSON serialization respectively:   Simple enough. Now we need to configure Spring MVC to use the new MessageConverter in Controllers and in REST clients using RestTemplate:    Once we have the MessageConverter configured, we can use the Avro generated classes as parameter and return types in our Controller implementation, relying on the MessageConverter doing the correct Serialization/Deserialization based on content type negotiation:   Similar for consuming a REST endpoint using RestTemplate:   As usual with most of the Spring APIs, the end result is reasonably elegant and non-intrusive, isn't it?   So why would you like to use Avro serialization in a REST API anyway? If you are investing in an Event-Driven Architecture and are using Kafka as event distribution platform, Avro is the recommended choice due to its compact binary message format and good Schema versioning support from the Schema Registry. But then there may be a small area within your solution where a Synchronous Query API is needed, maybe to support a subsystem or client that is not yet ready to go all-in event driven. In such a situation, it makes perfect sense to reuse the same Avro-based Domain Object types from the existing Event streams to define your REST-based API.  Your mileage may vary ...  Enjoy!
 In a [previous blog] we described how to develop non-blocking REST services using Spring MVC and Spring Boot. In this blog we will add the aspect of testing non-blocking REST services.  Testing a REST service includes not only basic unit testing of the business logic inside the service, but also ensuring that communication aspects are handled correctly. For example, verifying that the asynchronous machinery works as expected in a non-blocking service and that HTTP headers are handled correctly.  -[readmore]-  Testing a traditional blocking REST service using [Spring MVC Test Framework] is straightforward using its fluent API. See the following example copied from the link above:   If we try to apply this approach to non-blocking REST Services developed with Spring MVC and Spring Boot we run into two problems:  1. When using Spring Boot we typically have no configuration file to specify when using the  annotation. 2. The non-blocking program model in Spring MVC results in that the   method will return before the request is processed, i.e. we have to instruct the test to wait for the completion of the asynchronous processing before we can perform any assertions on the test result.  Let's see how we can handle these problems!  Instead of using  and a classic XML-configuration file Spring Boot provides a corresponding annotation, . This annotation can be used to load and configure a Spring  for integration tests using Spring Boot  and specifying the Spring Boot main class of the application, e.g. .  With that explained, the only thing we need to do is to replace the annotation . An example from our sample code:   To wait for the asynchronous completion of the request processing our test can use a helper method  in the class , see its [javadoc] for details. The key thing is to split the usage of fluent API in the example above in two separate statements like:   However, from my own experiences, this construct doesn't seem to work in all cases. After searching for similar issues on Internet, I found a lot of recent bug-fixes in the MVC Test Framework regarding handling non-blocking, deferred asynchronous results. So we seems to be on the bleeding edge here. Hopefully this will be sorted out in near time, but until then we need a robust workaround.  A number of possible workarounds are suggested, but the one that always seem to work is to add an extra statement in between the two above: . This statement doesn't complete until the asynchronous processing is done so after it returns it is safe to perform the assertions on the result.  The revised code looks like:   Do you want to try it out on your own?  Please, check out our code example and try it yourself:   A test run typically produce an output like:   A proper test report is also created, see :  ![Test Report]  With some minor changes to your test code you can keep on writing tests based on [Spring MVC Test Framework] also for asynchronous REST services. With a replacement of one annotation in the test class we also can enjoy the Spring Boot programming model, e.g. avoid using XML based Spring configuration files!
Many organizations gradually improve in maturity when it comes to web application development. The value of an explicit, governed best-practice has been learned the hard way.  - Architectural layers have been sorted out to make sure concerns are separated for flexibility and maintainability. - Roles and disciplines have been sorted out to make sure the right skill comes into play for the various aspects of the architecture . - A framing reference architecture has been sorted out, providing a right-grained model shared across all technical disciplines, to drive best-practice governance.  Then the portal project started. It started as an infrastructure project. A portal platform is to be implemented. The focus is initially on security and CMS. Eight weeks later, the portal project is eight times larger. The portal project has been out there talking to business users. They've learned what the users need. The users need information at their fingertips, currently hidden in silos across the IT landscape. Users are happy. Suddenly someone show up from IT to ask what they need - ready to deliver value - now!   Yes, the portal project depends on a quick win. The big portal server is running out there with a calendar, a news feed and a link to the time reporting system. Not very exciting. Some real business data easily available, could justify the investment. Suddenly the portal cowboys are everywhere. Html consultants are buying expensive high-end IDE wizardry bound to the portal vendor, so that they can point-and-click their way to JSP/JSF/SDO/Vendor X glue/JCA/almost JSR 168 portlets that integrates with CICS, MySAP and the database API of the legacy order system. By the way, there was a checkbox in IDE, that that made it wrap all back-end access into web services. A single click in the IDE deployed it all to the portal server. The ad-hoc web services turned out to be popular for re-use. The portal guys deliver new services in a fraction of time  compared to the sturdy ESB guys.   But what about test-driven development, contract first canonical web services, portable builds, lightweight, open source IDE with all build files generated from Maven build files, portability across web apps and portlet apps, in-house glue to avoid vendor lock-in and all other best-practice principles applied? They started where we started and don't want to be any more. Let's stop them! Let's catch them in our enterprise architecture governance process!  No, don't! They represent a sound approach to business-driven IT. And they deliver. Well, there may be some issues once a service interface is broken, the platform is to be upgraded, load increases jada-jada. Of cause agile, business focused portal development is suitable for some solutions and less suitable for others.   Instead of "to conform or not to conform", it may be time to extend the reference architecture to support  business-driven portlet projects. In the sought for a governance model, we came up with a portlet classification model as an initial idea. What do you think? Does it help sorting out borderlines between portal and application responsibilities?  _Bild saknas_  Each type  differentiates in its relationship to information services.  - Type 1 integrates purely at the html level and thus strictly tied to web browser and presentation skills. Integration of Google Maps could illustrate this type. - Type 2 portlets consume information with low semantic structure. They are typically developed "on-line" using tools within the portal server itself. The focus is on creating information mash-ups from various sources, using XML-scentric languages like xpath and xslt, or tools that generate such constructs. The skill set is comparable to the integration developer role. - Type 3 portlets contribute business process tools that consume semantically rich services - both for information retrieval and also for invoking transactions that change the state of the back-end system. Type 3 portlets are developed by software engineers using todays best-practice within Java development. These portlets are developed, integration tested, versioned and finally released through the standard software release process of the enterprise. They consume governed, enterprise services. A type 3 portlet may - due to its mature change control environment - be exposed for remote consumption in other portals via WSRP.   What about the ad-hoc services? Type 3 . This is typically done by a different team, in control of established contract- and versioning strategies.
 In part 5, we'll get our "accountservice" up and running on a locally deployed [Docker Swarm] cluster and discuss the core concepts of container orchestration.  This blog post deals with the following:  - Docker Swarm and container orchestration - Containerize our accountservice using Docker - Setting up a local Docker Swarm cluster - Deploying the accountservice as a Swarm Service - Run the benchmarks and collect metrics  _Gophers beware: After writing this part of the [blog series], I realized there's nothing Go-specific in this part. I hope you'll enjoy it anyway._  Before going practical, a quick introduction of the concept of a "container orchestrator" may be of use.  As an application becomes more complex and has to handle an  hardware. Container orchestration allows us to view all that hardware as a single logical entity.  This article about [container orchestration] sums it up as:  "abstracting the host infrastructure, orchestration tools allow users to treat the entire cluster as a single deployment target."  I can't summarize it better myself - using a container orchestrator such as Kubernetes or Docker Swarm allows us to deploy our software components as [services] running on one or more nodes of our available infrastructure. In the case of Docker - the Swarm mode is about managing a cluster of Docker Engines called a swarm. Kubernetes uses a slightly different nomenclature and hierarchy of key abstractions, but the concept on the whole is roughly the same.  The container orchestrator not only handles the lifecycle of our service for us, it also provides mechanics for things like service discovery, load-balancing, internal addressing and logging.  In Docker Swarm, there's three concepts that entails an introduction:  - Node: A node is an instance of the Docker engine participating in the swarm. Technically, see it as a host having it's own CPU resources, memory and network interface. A node can be either a _manager node_ or a _worker node_. - Service: A service is the definition of what to execute on the worker nodes as defined by a container image and the commands you instruct the container _replicated_ or _global_, see the docs for details. A service can be seen as an abstraction for letting an arbitrary number of containers form a logical "service" accessible through its name throughout and possibly outside the cluster without having to know anything about the internal network topology of the environment. - Task  _node_.  The diagram below shows a possible  deployment of our microservice landscape with two nodes running a total of five container instances abstracted by two services - "accountservice" and "quotes-service".  ![swarm overview]  While this part doesn't change any of the Go code from the previous parts, we're going to add some new files for running on Docker. Feel free to checkout the appropriate branch from git to get the source of the completed state of this part.  git checkout P5  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._   For this part to work, you'll need to have Docker installed on your dev machine. I suggest following [this guide].  A [Dockerfile] containing whatever you want it to contain. Let's get this started by creating a file called _Dockerfile_ in the _/accountservice_ folder:  FROM iron/base  EXPOSE 6767 ADD accountservice-linux-amd64 / ENTRYPOINT ["./accountservice-linux-amd64"]  A quick explanation: - FROM - defines the base image that we'll start building our own image from. iron/base is a very compact image well suited for running a Go application. - EXPOSE - defines a port number we want to expose on the internal Docker network so it becomes reachable. - ADD - Adds a file _accountservice-linux-amd64_ to the root  of the container filesystem. - ENTRYPOINT - defines what executable to run when Docker starts a container of this image.  As you see, the file we're adding has this "linux-amd64" in its name. While we can name a Go executable just about anything, I like using a convention where we put the OS and target CPU platform into the executable name. I'm writing this blog series on a Mac running OS X. So if I just build a go executable of our accountservice from the commandline when in the _/goblog/accountservice_ folder:  > go build  That will create an executable called _accountservice_ in the same folder. The problem with that executable is that it won't run in our Docker container as the underlying OS there is Linux-based. Therefore, we need to set a few Environment variables before building so the go compiler and linker knows we're building for another OS and/or CPU architecture - in our case Linux.  Again, from the _/goblog/accountservice_ folder:  > export GOOS=linux > go build -o accountservice-linux-amd64 > export GOOS=darwin  This produces an executable binary named using the _-o_ flag. I usually brew my own little shell script that automates this for me.  Since both OS X and our Linux-based container runs on the AMD64 CPU architecture we don't need to set  env var. But if you're building something for an 32-bit OS or perhaps an ARM processor you would need to set GOARCH appropriately before building.  Now it's time to build our very first Docker image containing our executable. Go to the parent folder of the _/accountservice_ folder, it should be _$GOPATH/src/github.com/callistaenterprise/goblog_.  When building a Docker container image, we usually tag it with a name usually using a [prefix]/[name] naming convention. I typically use my github username as prefix, e.g. _eriklupander/myservicename_. For this blog series, I'll use "someprefix". Execute the following command from the root project folder  to build a Docker image based on the Dockerfile above:  > docker build -t someprefix/accountservice accountservice/  Sending build context to Docker daemon 13.17 MB Step 1/4 : FROM iron/base ---> b65946736b2c Step 2/4 : EXPOSE 6767 ---> Using cache ---> f1147fd9abcf Step 3/4 : ADD accountservice-linux-amd64 / ---> 0841289965c6 Removing intermediate container db3176c5e1e1 Step 4/4 : ENTRYPOINT ./accountservice-linux-amd64 ---> Running in f99a911fd551 ---> e5700191acf2 Removing intermediate container f99a911fd551 Successfully built e5700191acf2  Nice! Our local docker image repository now contains an image named _someprefix/accountservice_. If we'd be running with multiple nodes or if we'd want to share our new image, we'd use [docker push].  We can now try to run this image directly from the command line:  > docker run --rm someprefix/accountservice Starting accountservice Seeded 100 fake accounts... 2017/02/05 11:52:01 Starting HTTP service at 6767  However - note that this container is _not_ running on your host OS localhost anymore. It now lives in it's own networking context and we can't actually call it directly from our host operating system. There are ways to fix that of course, but instead of going down that route we'll now set up Docker Swarm locally and deploy our "accountservice" there instead.  Use Ctrl+C to stop the container we just started.  One of the goals of this blog series is that we want to run our microservices inside a container orchestrator. For many of us, that typically means Kubernetes or Docker Swarm. There are other orchestrators as well such as [Apache Mesos], but this blog series will focus exclusively on Docker Swarm based on Docker 1.13.  The tasks involved when setting up a Docker Swarm cluster on your dev computer may vary somewhat depending on how you installed Docker itself. I suggest following [this guide].   A Docker Swarm cluster consists of at least one Swarm Manager and zero to many Swarm Workers. To keep things simple, my examples will just use a single Swarm Manager - at least for now. The important thing is that after this section you need to have a working Swarm Manager up and running.  The example here uses [docker-machine] on how to create a Swarm.  This command initializes the docker-machine host identified as _swarm-manager-1_ as a swarm node with the IP address of the same _swarm-manager-1_ node:  > docker $  If we'd be creating a multi-node swarm cluster we'd make sure to store the _join-token_ emitted by the command above as we'd need it later if we were to add additional nodes to the swarm.  A docker [overlay network] is a mechanism we use when adding services such as our "accountservice" to the Swarm so it can access other containers running in the same Swarm cluster without having to know anything about the actual cluster topology. Let's create such a network:  docker network create --driver overlay my_network  _my_network_ is the name we gave the network.  Almost there! Now it's time to deploy our "accountservice" as a [Docker Swarm service]. The _docker service create_ command takes a lot of parameters but isn't that scary, promise. Here's the command we'll execute to deploy the "accountservice":  > docker service create --name=accountservice --replicas=1 --network=my_network -p=6767:6767 someprefix/accountservice ntg3zsgb3f7ah4l90sfh43kud  Here's a quick rundown of the arguments:  - --name: Assigns a logical name to our service. This is also the name other services will use when addressing our service within the cluster. So if you had a another service that would like to call the _accountservice_, that service would just do a GET to _http://accountservice:6767/accounts/10000_ - --replicas: The number of instances of our service we want. If we'd have a multi-node Docker Swarm cluster the swarm engine would automatically distribute the instances across the nodes. - --network: Here we tell our service to attach itself to the overlay network we just created. - -p: Maps [internal port]:[external port]. Here we used 6767:6767 but if we'd created it using 6767:80 then we would access the service from port 80 when calling externally. Note that this is the mechanism that makes our service reachable from outside the cluster. Normally, you shouldn't expose your services directly to the outside world. Instead, you'd be using an EDGE-server  that would have routing rules and security setup so external consumers wouldn't be able to reach your services except in the way you've intended them to. - someprefix/accountservice: This how we specify _which_ image we want the container to run. In our case this is the tag we specified when we created the container. Note! If we'd be running a multi-node cluster we would have had to push our image to a Docker repository such as the public  Docker Hub service. One can also set up private docker repositories or use a paid service if you want your images to stay private.  That's it. Run this to see if our service was started successfully.  > docker service ls ID NAME REPLICAS IMAGE ntg3zsgb3f7a accountservice 1/1 someprefix/accountservice  Sweet. We should now be able to curl or even use a web browser to query our API. The only thing we need to know up front is the public IP of the Swarm. Even if we're running only one instance of our service on a swarm with many nodes, the overlay network and Docker Swarm lets us ask _any_ of the swarm hosts for our service based on its port. That also means that two services cannot expose _the same_ port externally. They may very well have the same port _internally_ but to the outside world - the Swarm is one.  Anyway - remember that environment variable _ManagerIP_ we saved earlier?  > echo $ManagerIP 192.168.99.100  If you've changed your terminal session since then, you can re-export it:  > export ManagerIP=`docker-machine ip swarm-manager-0`  Let's curl:  > curl $ManagerIP:6767/accounts/10000   It's alive!  While using the Docker command-line API to examine the state of your Swarm  that we can deploy as a Docker Swarm service. This provides us with an alternate mechanism for looking at our cluster topology. It can also be used just for making sure we can reach a service exposed at a given port within our cluster.  Installing the visualizer from a pre-baked container image is a one-liner:  docker service create \ --name=viz \ --publish=8080:8000/tcp \ --constraint=node.role==manager \ --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \ manomarks/visualizer  This creates a service we can access at port 8000. Direct your browser to _http://$ManagerIP:8000_:  ![viz]  I've made a little Swarm visualizer myself called ["dvizz"] force graphs. You can install it directly from a pre-baked image if you like:  docker service create \ --constraint node.role==manager \ --replicas 1 --name dvizz -p 6969:6969 \ --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \ --network my_network \ eriklupander/dvizz  Direct your browser to _http://$ManagerIP:6969_:  ![dvizz]  It's a bit buggy so don't take it too seriously, but it's rather fun to look at that graph bouncing around when scaling services up and down - something we'll do in part 7 of the blog series. Feel free to branch dvizz or submit a pull request if you want to help making it even cooler, more useful or less buggy!  Not much of a Microservice landscape with only one type of service  tagged as _eriklupander/quotes-service_:  > docker service create --name=quotes-service --replicas=1 --network=my_network eriklupander/quotes-service  If you do a _docker ps_ to list running Docker containers we should see it started :  > docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 98867f3514a1 eriklupander/quotes-service "java -Djava.security" 12 seconds ago Up 10 seconds  8080/tcp  Note that we're not exporting a port mapping for this service which means it won't be reachable from outside of the Swarm cluster, only internally on port 8080. We'll integrate with this service later on in Part 7 when we'll be looking at Service Discovery and load-balancing.  If you added "dvizz" to your Swarm, it should show something like this now that we've added the "quotes-service" and "accountservice".  ![dvizz2]  To make things a little easier for us along the way, we can add a shell script to automate things for us when rebuilding/redeploying. In the root _/goblog_ folder, create a shell script named _copyall.sh_:  #!/bin/bash export GOOS=linux export CGO_ENABLED=0  cd accountservice;go get;go build -o accountservice-linux-amd64;echo built ;cd ..  export GOOS=darwin  docker build -t someprefix/accountservice accountservice/  docker service rm accountservice docker service create --name=accountservice --replicas=1 --network=my_network -p=6767:6767 someprefix/accountservice  This script sets up environment variables so we safely can build a statically linked binary for Linux/AMD64, does this and then runs a few Docker commands to deploy it as a Docker Swarm service. Saves time when prototyping!  Build scripts for Go is a topic I won't dive into in this blog series. Personally, I like the simplicity of shell scripts, though I've occasionally used a [gradle plugin] is quite popular as well.  From now on, all benchmarking and collection of CPU/memory metrics will happen on the service when its running inside the Docker Swarm. This means the results from previous blog posts isn't comparable to what we're going to get from now on.  Both CPU utilization and memory use will be collected using _docker stats_ while we're going to use the same Gatling test as before.  If you feel like running the load tests yourself, the requirements for this introduced in Part 2 still applies. Just note that you'll need to change the _-baseUrl=_ argument to the IP of your Swarm Manager node, e.g:  > mvn gatling:execute -Dusers=1000 -Dduration=30 -DbaseUrl=http://$ManagerIP:6767   > docker stats $  CONTAINER CPU % MEM USAGE / LIMIT accountservice.1.k8vyt3dulvng9l6y4mj14ncw9 0.00% 5.621 MiB / 1.955 GiB quotes-service.1.h07fde0ejxru4pqwwgms9qt00 0.06% 293.9 MiB / 1.955 GiB  Right after startup, the container consisting of a bare linux distro and our running "accountservice" uses ~5.6 mb of RAM out of the 2GB I've allocated to the Docker Swarm node. The Java-based quotes-service hovers just under 300 mb - though it should be said that it certainly can be reduced somewhat by tuning its JVM.   CONTAINER CPU % MEM USAGE / LIMIT accountservice.1.k8vyt3dulvng9l6y4mj14ncw9 25.50% 35.15 MiB / 1.955 GiBB  At 1K req/s within the Swarm running on a Virtualbox instance instead of native OS X as in part 2 and 3, we see numbers pretty consistent with the earlier figures - as expected slightly higher memory use  and a roughly similar CPU utilization.   ![performance]  The mean latency is now up to 4 ms. The exact reason for this to increase from the sub-millisecond mean when running directly could be several, I'm guessing there's an overhead involved when the Gatling test that's running on the native OS accesses the Swarm running on a virtualized instance with a bridged network and some perhaps some routing going on within the Swarm and it's own overlay network. It's still really nice to be able to serve a peak of 1K req/s at a 4ms mean latency including reading from the BoltDB, serializing to JSON and serving it over HTTP.  In case we need to greatly decrease the number of req/s in later blog posts due to large overhead when adding things like tracing, logging, circuit breakers or whatnot - here's a figure when running 200 req/s:  ![performance200]  This sums up part 5 of the blog series where we have learned how to bootstrap a Docker Swarm landscape  locally and how to package and deploy our "accountservice" microservice as a Docker Swarm Service.  In [part 6], we'll add a Healthcheck to our microservice.
There are many languages targeting the Java VM right now. The most popular seem to be the dynamic languages like. A rising star that has caught lots of attention recently is the statically type language [Scala]. Scala integrates both object-oriented and functional features, compiles to Java byte code and integrates seamlessly with the Java VM and the Java class libraries. Scala does also target the .NET platform CLR.  In Scala everything is an object, even functions. This makes it possible to pass functions as arguments, assign them to variables and return them from functions. The following code declares a function addTo  that takes an integer x as an argument and returns a function that will add the number x to its argument.  You can use this function like this:  A call to add3 evaluates to 8  Simple numbers are of course also objects in Scala. Java has primitive types like int and long but in Scala these are objects with methods. So if you write 2 + 4, that actually means 2.+ in Scala. You call the method + on the first object with the argument 4. This type of infix syntax can be used for all methods that take one argument. So instead of writing  you can simply write  which is pretty much plain English and this feature makes Scala suitable for writing domain specific languages.  Scala is, as I mentioned earlier, a type safe language, but that does not mean that you have to explicitly declare the type everywhere. The Scala compiler does a good job of inferring the type of objects from the context in which they are declared and this makes the resulting code very compact. Scala also integrates fully with your Java classes. You can specify imports of java classes in your Scala code and use them easily.  Maybe the most interesting feature of Scala is that it has adopted the concurrent oriented programming style found in. The concurrent oriented programming style is based on lightweight processes that pass messages between them. The lightweight processes in Scala  are event based and does not have a one-to-one mapping to the underlying operating system threads. This enables great scalability on multi core processors. Since the trend of processor technology seems to be an increasing number of cores and constant or decreased clock speed, this is a very valuable property. The great thing with the Scala implementation of Actors is that it is not a core feature of the language, but is implemented as a library. This shows that Scala is a very expressive language.  Scala also has XML built in as a first class entity of the language. You can mix XML right into the code and it is recognized by the compiler and replaced by objects from the scala libraries.  The has lots of documentation and an Eclipse plugin is available, so go check it out. And while you are at it, check out the web framework that is written in Scala.
  Four of us started the [SpringOne2GX], a platform for cloud applications. This blog post compiles our first impressions of Cloud Foundry.  -[readmore]-  During the tutorial we run Cloud Foundry as a [PaaS].  The findings below are baed on an assumption that we want to deploy and manage a typical Java application, e.g. that is deployed on a set of applications servers  with a load balancer in front and that uses a set of resources like databases and messaging systems. E.g.:  ![sample-app]  For an overview of Pivotal Web Services components see: [Cloud Foundry Components].   All infrastructure required by an application can be setup in Cloud Foundry using a [CLI] setups.  First you have to authenticate yourself by the command `cf login`, like:  $ cf login API endpoint: https://api.run.pivotal.io Email> magnus.larsson.ml@gmail.com Password> Authenticating...  Targeted org mltrial Targeted space development  ...and Cloud Foundry will setup the CLI to work with your default environment, in this case the  space.  We are now ready to deploy out first application!   Cloud Foundry makes deployment of applications very simple and straightforward .  Before you deploy an application you specify its demands on the environment in a manifest-file, by default named . It can look something like:  applications: - name: cf-spring-mvc-demo instances: 2 memory: 512M host: cf-spring-mvc-demo-mlce domain: cfapps.io path: target/cf-spring-mvc-demo-0.0.1-SNAPSHOT.war env: property1: value1  I guess the file is more or less self explaining but some highlights:  *  is used to declare how many instances of the application that Cloud Foundry shall start up when the app is started *  specifies how much memory each app instance is allowed to consume. *  +  is used to specify the hostname of the application, i.e. the load balancer  will automatically be configured to route incoming requests to this hostname to one of the application instances. *  specifies where the binaries of the app can be found. *  allow us to setup environment specific variables for the application.  The application can now be deployed with the command:  cf push  After the deploy is complete we can inspect the status of the application with the command:  $ cf app cf-spring-mvc-demo ... state since cpu memory disk details #0 running 2015-09-16 01:42:58 AM 0.1% 511.7M of 512M 156.4M of 1G #1 running 2015-09-16 07:09:59 AM 0.1% 491.3M of 512M 156.4M of 1G  ...and the application homepage . Requests to the application will be passed by the load balancer to the application instances in a round robin fashion.   [Pivotal Web Services] comes with a Marketplace that provides a number of services ready to be used by our applications with only a few cf-cli-commands:  $ cf marketplace  service plans description 3scale free_appdirect, basic_appdirect*, pro_appdirect* API Management Platform cleardb spark, boost*, amp*, shock* Highly available MySQL for your Apps. cloudamqp lemur, tiger*, bunny*, rabbit*, panda* Managed HA RabbitMQ servers in the cloud elephantsql turtle, panda*, hippo*, elephant* PostgreSQL as a Service memcachedcloud 100mb*, 250mb*, 500mb*, 1gb*, 2-5gb*, 5gb*, 30mb Enterprise-Class Memcached for Developers memcachier dev, 100*, 250*, 500*, 1000*, 2000*, 5000*, 7500*, 10000*, 20000*, 50000*, 100000* The easiest, most advanced memcache. mongolab sandbox Fully-managed MongoDB-as-a-Service newrelic standard Manage and monitor your apps redis dedicated-vm Redis service to provide a key-value store rediscloud 100mb*, 250mb*, 500mb*, 1gb*, 2-5gb*, 5gb*, 10gb*, 50gb*, 30mb Enterprise-Class Redis for Developers searchly small*, micro*, professional*, advanced*, starter, business*, enterprise* Search Made Simple. Powered-by Elasticsearch stamplay plus*, premium*, core, starter* API-first development platform  **Note:** Only some of the available services are shown for brevity.  If you for example need a MySQL database pre-setup and configured for high availability you can use the  service.  First we provision it to our current space  using a free plan called :  $ cf create-service cleardb spark mydb  Now we can bind the database to our application:  $ cf bind mlce-spring-music mydb   Since we are using a Spring Boot application and it only use one database, Cloud Foundry will be able to automatically inject a  bean representing the -database in our application!  $ cf services  name service plan bound apps last operation mydb cleardb spark mlce-spring-music create succeeded  So with no further configuration we are good to go!   The application can be scaled either manually or automatically.   An application can be scaled manually either vertically, by increasing the memory usage, or horizontally, by increasing the number of instances, e.g.:  $ cf scale cf-spring-mvc-demo -m 1024M  Each application instance now can use 1 GB of memory.  $ cf scale cf-spring-mvc-demo -i 4  The application will now run in four instances and the load balancer have been updated automatically to spread the requests to all four instances.   Pivotal provide an add-on service called a Autoscaler that can be used to direct Cloud Foundry to automatically scale up  an application. It comes with a user interface that looks like:  ![sample-app]  As you can see within the red block in the picture above you can specify minimum and maximum number of instances and the CPU thresholds for when to add and decrease the number of instances.   The Cloud Foundry runtime monitors our application instances automatically and restarts them as needed if they  fails and crashes.   Cloud Foundry also supports zero downtime upgrades of applications using both [Blue-Green Deployment] for details.   If an application instance or node crashes HTTP sessions on that application assistance are normally lost and users typical has to login again. Cloud Foundry can however be configured to replicate HTTP sessions either using [Spring Session] that can customize the runtime to share HTTP sessions between application instances.   Cloud Foundry collects logs from our application instances. The logs can be monitored by the command:  $ cf logs  The `cf logs` command runs by default in tail-mode, e.g. runs and displays new logs until stopped.  If I want to redirect the logs to an external log management tool, e.g. the [ELK-stack], that LogStash can use to pick up logs and send them to ElasticSearch for visualization in Kibana.  Redirect logs to a syslog can be done by a command like:  $ cf cups log-drain -l syslog://<URL-FROM-PAPERTRAIL>  The logs can be seen directly in [papertrailapp.com] like:  ![sample-app]   In the same way, monitoring can easily be enabled for an application. For example we can provision New Relic as our monitoring tool with:  $ cf create-service newrelic standard my-new-relic  ...and we can bind our New Relic service to our application with:  $ cf bind mlce-spring-music my-new-relic  That's all it takes, after using the application you can see graphs in the New Relic dashboard like for HTTP requests:  ![sample-app]  ...and for database related work:  ![sample-app]    Cloud Foundry integrates very nicely with Spring Cloud and provides for example [Spring Cloud Services for Pivotal Cloud Foundry] that packages server-side components in the Spring Cloud projects, such as Spring Cloud Netflix Eureka and Hystrix Dashboard and Spring Cloud Config, and makes them available as services in the Marketplace.   This looks a lot like Docker containers, right? Yes, to some extent... Cloud Foundry, however, extends the functionality of plain Docker containers with, for example, its marketplace of ready to use services.  ...and the best of it all...  Since Cloud Foundry 1.5, [Cloud Foundry supports Docker]!  We never got time to look into Cloud Foundries support for Docker during the tutorial, but that is added to the TO DO list :-)   So how do I as a developer verify that my application works as expected when deployed in Cloud Foundry? Deploying it to a development space in a common Cloud Foundry installation is of course one option, but it typically not what a developer want...  ...and installing Cloud Foundry locally is known for being very messy and resource consuming, neither an option...  There is however a new option, recently released, called [Lattice]. Similar to Cloud Foundry but smaller and far easier to install locally. Look out for another blog post regarding Lattice!
 This is a brief summary of [Spring Cloud Contract] in Clearwater Beach, Florida.   Given the premise that reliable integration testing of a microservices landscape can be a difficult and complex task, it may be preferable to write tests on a per-service basis and use frameworks such as [MockMVC] from Pivotal held a talk on Spring Cloud Contract and how the framework may be beneficial for both consumers and the producer of an API. Even though the title used the ubiquitous "microservice" word, the technology and concepts should apply to HTTP APIs and testing those in general.  ![Craig Walls] _Craig Walls from Pivotal held a number of Spring-related talks during the conference_  [Spring Cloud Contract] is an attempt from Spring to sort of "flip" the way one might typically declare and program stub behaviour by letting the _producer_ service be the one providing stubs for its API in the form of Maven artifacts. The formal name seems to be "Consumer-Driven-Contracts".  The premise of the producer-side of an API publishing stubs for its services isn't strictly unique, but usually one writes and defines mocks for external services in one's test code rather than pulling in a binary testability  seems like a really great idea!  Anyway - from a more technical perspective the stubs and their behaviour is declared using a Groovy-based DSL. In principle:  "given a request having these characteristics, respond with a response that looks like this."  Nothing unique - it's a very common pattern seen from other frameworks and language such as [nock] from the world of NodeJS.  A simple example where we use an API to check whether a person should be allowed to buy beer given their age :  request { description method 'POST' url '/check' body headers { header 'Content-Type', 'application/json' } } response { status 200 body headers { header( 'Content-Type', value ) } }  Pretty straightforward, right? Note the regular expression checking if the age is 0-19. In this case there will be no beer for the requestor. The maven plug-in will then compile and generate stub classes packaged into a .jar file that may be uploaded to a Maven repository .  In actual test code running JUnit or whatever, you write your tests as always and declare an _@AutoConfigureStubRunner_ annotation on the test class level, feeding it the maven unique identifier - with support for versioning wildcards. E.g:  @RunWith @SpringBootTest @AutoConfigureStubRunner public class BeerApplicationServiceTests {  On a slightly higher level, there's definitely an interesting upside to this approach for API developers. When releasing a new version of your API - possbily with breaking changes - you could definitely consider simultaneously releasing a Spring Cloud Contracts stub jar file your consumer can plug directly into their unit/integration tests. Craig used Facebook as an example - Facebook has lots of APIs with lots of new releases, where changes in new versions are communicated using release notes. One can envision the benefit for consumers if Facebook said:  "Hey guys, in three months version 6.0 of the adverts API will go live. Feel free to download "com.facebook:adverts-contracts:6.0" from maven central anytime you feel like starting work on using the new API. It's available today."  Of course the example is my personal interpretation and I have not idea whether there exists an "adverts API" of if Facebook uses that kind of informal communication, but I think you get the general idea I'm  trying to convey. I'm sure an approach similar to this example would be useful for many devs out there.  On a personal level I find the reasoning behind Spring Cloud Contracts quite appealing and it is definitely something I will consider adopting in the future - either in my current project or in some future endeavour.  It should be noted that while Spring Cloud Contracts on the framework level may seem to be somewhat bound to the JVM, Spring, MockMVC, RestAssuredMockMvc etc. and the Maven ecosystem  that may start a real server running the stubs using WireMock. I.e - you can deploy the stubs as a standalone service that your services can use in integration tests just as if they were real ones.  One can also generate java-based tests using EXPLICIT mode  that will perform real HTTP calls which can be useful to let some of the services being called in a test be stubbed out while others are not.  I will however definitely consider applying the core concepts to my [Go-microservices]-based HTTP mocks could be supplied from producer microservices as an importable artifact into the unit tests of consumer services.
[Multi Tenancy] usually plays an important role in the business case for [SAAS] solutions. [Spring Boot] and [Hibernate] provide out-of-the-box support for different Multi-tenancy strategies. Configuration however becomes more complicated, and the available code examples are limited. In the first part of this blog series, we'll start by exploring the Multi Tenancy concept and three different architectural patterns for multi tenant data isolation. In the forthcoming episodes, we'll deep dive into the details of implementing the different patterns usign Spring Boot, [Spring Data] and [Liquibase].  -[readmore]-  [comment]: #  [Multi Tenancy]: https://whatis.techtarget.com/definition/multi-tenancy [SAAS]: https://en.wikipedia.org/wiki/Software_as_a_service [Spring Framework]: https://spring.io/projects/spring-framework [Spring Boot]: https://spring.io/projects/spring-boot [Spring Data]: https://spring.io/projects/spring-data [Hibernate]: https://hibernate.org/orm/ [Liquibase]: https://www.liquibase.org/  [comment]: #  [neighbours]: /assets/blogg/multi-tenancy-with-spring-boot/undraw_neighbors_ciwb.png [SingleDatabaseMultiTenancy]: /assets/blogg/multi-tenancy-with-spring-boot/SingleDatabaseMultiTenancy.png [SeparateDatabaseMultiTenancy]: /assets/blogg/multi-tenancy-with-spring-boot/SeparateDatabaseMultiTenancy.png [SeparateSchemaMultiTenancy]: /assets/blogg/multi-tenancy-with-spring-boot/SeparateSchemaMultiTenancy.png  - Part 1: What is Multi Tenancy  - [Part 2: Outlining an Implementation Strategy for Multi Tenant Data Access] - [Part 3: Implementing the Database per Tenant pattern] - [Part 4: Implementing the Schema per Tenant pattern] - [Part 5: Implementing the Shared database with Discriminator Column pattern using Hibernate Filters] - [Part 6: Implementing the Shared database with Discriminator Column pattern using Postgres Row Level Security] - Part 7: Summing up    By allowing one single, highly scalable software solution to serve many different customers, a scalable, elastic, agile and cost effective solution can be built. A software architecture in which a  single instance of the software serves multiple tenants is frequently called a *multi-tenancy* architecture. A *tenant* is a group of users who share a common access with specific privileges to the software instance. Everything should be shared, except for the different customers' data, which should be properly separated. Despite the fact that they share resources, tenants aren't aware of each other, and their data is kept totally separate.  ![neighbours][neighbours]   As usual with architectural patterns, a multi-tenant architecture has to balance two partly conflicting needs or forces: On one hand, we would like to share as much as possible, in order to achieve:  * Better use of resources: One machine reserved for one tenant isn't efficient, as that one tenant is not likely to use all of the machine's computing power. By sharing machines among multiple tenants, use of available resources is maximized. * Lower costs: With multiple customers sharing resources, a vendor can offer their services to many customers at a much lower cost than if each customer required their own dedicated infrastructure. * Elasticity and Agility: With a shared infrastructure, onboarding new tenants can be much easier, quicker and cost efficient.  On the other hand, we would like to have a fool-proof separation of between tenants, in order to guarantee the privacy, confidentiality and consistency of each tenant's data. We also have to avoid the problem with "noisy neighbors", where a tenant that misbehaves potentially can disturb its neighboring tenants.    As we can see, a challenge lies in separating the **data** for each tenant, while still sharing as much as possible of the other resources. Three principal architectural patterns for Multi Tenancy can be identified, which differs in the degree of  separation of the tenant's data.  * **Database per tenant**  ![SeparateDatabaseMultiTenancy][SeparateDatabaseMultiTenancy]  The most obvious way to separate the data owned by different tenants is to use a separate database per tenant. Using this pattern, the data is physically isolated per tenant, and hence the privacy and confidentiality of the data can easily be guaranteed . The tradeoff is equally obvious, since the database infrastructure as well as database connection pools cannot be shared between tenants.  * **Schema per tenant**  ![SeparateSchemaMultiTenancy][SeparateSchemaMultiTenancy]  A slight variation to is to use a separate *database schema* per tenant, while sharing the database instance. The data for each tenant is logically isolated by the semantics of separate schemas as provided by the database engine. If the schemas is owned by a separate database user per tenant, the database engine's security mechanism further guarantee the privacy and confidentiality of the data .  * **Shared database, using a Discriminator Column**  ![SingleDatabaseMultiTenancy][SingleDatabaseMultiTenancy]  The final pattern uses a fully shared database, in which data for all tenants are stored in the same table but the maximum sharing of resources. From an infrastructure perspective, it is the conceptually simplest solution, whereas the complexity is pushed into the application. Since data is not separated at the database level, administrative housekeeping such as backups per tenant becomes more difficult.   Hence there are different pro's and con's with the three patterns above. The choice between them will be governed by the requirements of a particular solution. Database-per-tenant provides very strong data isolation between tenants, but requires more infrastructural resources and administrative work in setting up new tenants and performing database migrations. Hence there is an upper limit on the scalability of the Database-per-tenant pattern, both in size and the time required to onboard new tenants. Shared-database-with-Discriminator-column provides maximal sharing of infrastructural resources and hence excellent scalability, but with data isolation between tenants only guaranteed by the application layer.  If you have a smaller number of tenants  and require strong guarantees for tenant data isolation, Database-per-tenant and Schema-per-tenant are the most frequent choices. Among them, Schema-per-tenant is usually a good balance between data separation and resource sharing. If you have a large number of tenants, Shared-database-with-Discriminator-column might be the only viable solution.  Sometimes, the most pragmatic approach is a mixed model, supporting different customer segments using different models.   In this blog post, we have explored the Multi Tenancy concept and discussed three different architectural patterns for multi tenant data isolation. In the [next part], we'll dive into an implementation strategy for for Multi Tenant Data Access using Spring Boot, Spring Data, Hibernate and Liquibase, that allows us to implement the different multi tenant patterns transparently and efficiently.   Below are some links to background material and further suggested reading.  [Microservice Architectures — Single Tenant or Multi-Tenant?]  [Azure Multi-tenant SaaS database tenancy patterns]  [AWS Tenant Isolation]  [Multi-tenancy Design Patterns in SaaS Applications: A Performance Evaluation Case Study]
 A major challenge in a distributed system  into a centralized database for analysis and visualization.  -[readmore]-  New components in our test landscape are marked with red borders:  ![ELK Stack]  Table of Contents =================  1. Introduction 1. Code walkthrough 1. Build and Run 1. Try out centralized logging with the ELK stack 1. Next up...   Let's start with a short description of the new components and some new challenges!   The ELK stack from [Elastic] consist of:  * **[Logstash]** Logstash can collect log events from multiple types of sources using [*input*][logstash-input] plug-ins, transform it to a format you prefer using [*filter*][logstash-filter] and [*codec*][logstash-codec] plug-ins and send it to a number of destinations using [*output*][logstash-output] plug-ins.  * **[Elasticsearch]** Elasticsearch is a distributed and scalable full-text search database, that allows you to store and search large volumes of log events.  * **[Kibana]** Kibana lets you visualize and analyze your log events stored in Elasticsearch.  Elastic provides official Docker images for [Logstash][logstash-docker], [Elasticsearch][elasticsearch-docker] and [Kibana][kibana-docker].   Collecting log events in a container world is not the same as before when we were used to collect log events by [tailing][tail] log files. That does not work in a containerized world where a container's file system is ephemeral by default, i.e. it is destroyed if the container crashes. So, for example, after a restart there will be no log file to read log events from to understand what caused the restart. Instead we need to handle log events from containers as an event stream. For more information on the subject see [the twelve factor app - treat logs as event streams][12-factor-log-stream].   In [Docker v1.6.0].   When selecting a log driver, we have to match the supported log drivers from Docker with the supported input plugins from Logstash. Both Gelf and Syslog formats are supported by Docker and Logstash. [Syslog] multiline log events, e.g. log events with stack traces. For details see discussions [here][logstash-gelf-muliline-issue-1] and [here][logstash-gelf-muliline-issue-2]. Therefore, we will use the old Syslog format in this blog post.   There are two source code files of interest for enabling the ELK stack:  1. The **Docker Compose file**, where we bring in the ELK stack plus configure use of the selected Docker Log Driver, i.e. Syslog, for each microservice 2. The **Logstash configuration file**, where we specify and configure what *input*, *codec*, *filter* and *output* elements we want to use.  The source code contains two Docker Compose files:  1. One with the ELK stack, , 2. One without the ELK stack,  .  The shell script  is used to control which Docker Compose file that is in use by setting up the environment variable :  export COMPOSE_FILE=docker-compose-with-elk.yml #export COMPOSE_FILE=docker-compose-without-elk.yml   The most interesting new parts of the compose file  are the following:   Adding the ELK stack can, as mentioned above, be done by using official Docker images from Elastic like:  elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:5.2.2 ports: - "9200:9200" environment: - "xpack.security.enabled=false" - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  kibana: image: docker.elastic.co/kibana/kibana:5.2.2 ports: - "5601:5601" environment: - "xpack.security.enabled=false"  logstash: image: docker.elastic.co/logstash/logstash:5.2.2 ports: - "25826:25826" volumes: - $PWD/elk-config:/elk-config command: logstash -f /elk-config/logstash.config  > **Note #1:** The Docker images comes with a commercial, non free, extension called [X-Pack]. X-Pack bundles security, alerting, monitoring, reporting, and graph capabilities into one package. X-Pack includes a trial license for 30 days. To avoid dependencies to non-free extensions of the ELK stack we have disabled the X-Pack by setting the environment variable  to  for Elasticsearch and Kibana. > > **Note #2:** To reduce the memory used for our test purposes, we have limited the memory usage for Elasticsearch by setting the environment variable  to `-Xms512m -Xmx512m`.   To be able to capture log events from the startup of our microservices it is important that they don't start before Logstash is ready to receive log events. In the same way Elasticsearch needs to be up and running before Logstash can start to send log events to Elasticsearch.  > **Note:** In a production environment, we are probably using a container orchestrator like Docker in Swarm mode or Kubernetes. Using a container orchestrator, we can set up Logstash and Elasticsearch as services that, at least conceptually, always are up.  During development, we want to use Docker Compose to start up all containers  with one command. This leads to that we need to instruct Docker Compose to not start depending containers until the containers they depend on are started and ready to receive requests.  To achieve this we can use:  1.  - instructions, to tell Docker runtime how to monitor a container and ensure that it is ok and ready to receive requests. 2.  - instructions, to tell Docker to not start a container until the container it depends on is started. By also specifying the condition  we ask Docker runtime to wait to start the container until the container it depends on not only is started but also reports that it is healthy .  For the Elasticsearch container we have specified the following health check:  elasticsearch: ... healthcheck: test: ["CMD", "curl", "-f", "http://localhost:9200"] interval: 10s timeout: 5s retries: 10  For Kibana and Logstash we have added a dependency to the Elasticsearch container :  kibana: ... depends_on: elasticsearch: condition: service_healthy  logstash: ... depends_on: elasticsearch: condition: service_healthy  For Logstash we have also specified a health check:  logstash: ... healthcheck: test: ["CMD", "curl", "-f", "http://localhost:8080"] interval: 10s timeout: 5s retries: 10  All microservices that wants to send log events to Logstash defines a dependency to Logstash like:  pro: depends_on: logstash: condition: service_healthy   As described above we have decided to use the Syslog driver.  To configure a container to send its log events using Syslog in Docker you can specify something like the following in the Docker Compose file:  logging: driver: syslog options: syslog-address: "tcp://host:port"  Since we gave the Logstash container the name  and published the port  you might think that the  should be set to: , but that does not work :-(  The reason for this is that the the Syslog driver is running in the Docker daemon and the Docker daemon can't access our application's specific network where the hostname  is defined. Instead we can use port-mapping, i.e. expose the Syslog port in the Logstash container in the Docker host. This means that the Docker daemon can address the Logstash Syslog port using the local IP address .  So, the configuration looks like this:  logging: driver: syslog options: syslog-address: "tcp://127.0.0.1:25826"  For further details see the following [GitHub issue].   Let's go through the *input*, *codec*, *filter* and *output* elements in the Logstash configuration file. The source code can be found at .  1. ** and **  input {  http { port => 8080 type => "healthcheck" }  syslog { type => syslog port => 25826  codec => multiline { pattern => "^<%" negate => true what => previous } }  }  We accept input from  for the health check declared for Logstash in the Docker Compose file .  1. ****  filter {  if [type] == "healthcheck" { drop  }  mutate { strip => "message" }  grok { match => { "message" => "<%" } }  if "multiline" in [tags] { mutate { gsub => [ "message", "<\d+>.*?:\s", ""] } }  mutate { strip => "ml_thread" remove_field => [ "level", "version", "command", "created", "message", "tag", "image_id", "severity", "priority", "facility", "severity_label", "facility_label", "syslog_pri"] }  }   The filter processing does the following:  1. First, we start with dropping any input from the  input, i.e. health checks. 1. Next, we remove any leading or trailing whitespace from the message field to make it easier to parse. 1. Using a [Grok filter] we extract and name the parts we are interested in from the log event. Fields in the log event created by syslog are prefixed with  and fields coming from our microservices are prefixed with .  > **Note:** If you find it hard to get your Grok patterns set up correctly I suggest you try out the [Grok debugger]!  We collect the following  fields :  1. : the  part of a syslog message 1. : the  part of a syslog message 1. : the  part of a syslog message 1. : the  part of a syslog message  We collect the following fields from the Java logging framework, [Logback]:  1. : timestamp 1. : log level 1. : thread id 1. : name of the logging Java class 1. : the actual message in the log event  We also collect the following trace information from Spring Cloud Sleuth :  1. : name of the microservice 1. : the trace id 1. : the span id 1. : boolean indicating if the span was reported to Zipkin or not  1. If the log event is a multiline event we have to strip of some leading text on each line added by syslog, e.g.: `<30>Sep 9 06:32:08 89f30c64f36a[1966]: ` 1. Finally, we get rid of some fields in the log event that we don't want to store in Elasticsearch.   1. ****  output { elasticsearch { hosts => "elasticsearch" ssl => "false" user => "logstash_system" password => "changeme" } stdout { codec => rubydebug } }  Log events are sent to:  1.  for storage, to be picked up by Kibana 2.  for debugging purposes, using the `docker compose logs` command   For details on how to build and run the microservice landscape in this blog post series, see the [blog post #5].  > **Note #1:** To be able to run some of the commands used below you need to have the tools [cURL] installed. > > **Note #2:** Since *blog post #5* this blog series is based on [Docker for Mac], that was used in earlier blog posts. If you have Docker Machine installed be sure to run the following command to direct your Docker client tools to work with Docker for Mac: > > $ eval $  In summary:  1. Open a terminal, create a folder of your choice and  into it:  $ mkdir a-folder-of-your-choice $ cd a-folder-of-your-choice  1. Since we have externalized our configuration into a configuration repository we first need to get it from GitHub:  $ git clone https://github.com/callistaenterprise/blog-microservices-config.git  1. Next, we get the source code from GitHub and checkout the branch used for this blog post:  $ git clone https://github.com/callistaenterprise/blog-microservices.git $ cd blog-microservices $ git checkout -b B11 M11  1. Now, we can build our microservices with:  $ ./build-all.sh  1. Finally, we can bring up the dockerized microservice landscape and run a test:  $ . ./test-all.sh start  > **Note #1:** We will not shut down the microservice landscape , since we will use it in the next section.  > **Note #2:** The first  in the command above is essential. It allows us to reuse the  environment variable that the script creates to store an OAuth Access Token, i.e. we don't need to acquire one ourselves.  After a while, the processing should end with the response from a API request like:  $ curl -ks https://localhost:443/api/product/123 -H "Authorization: Bearer $TOKEN" | jq . { "productId": 123, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ], "serviceAddresses":  } End: Sun Sep 1 13:33:25 CEST 2017  1. Try a manual call like:  $ curl -ks https://localhost/api/product/456 -H "Authorization: Bearer $TOKEN" | jq .     1. First verify that Elasticsearch is up and running:  $ curl http://localhost:9200 | jq . { "name" : "hK0W3cd", "cluster_name" : "docker-cluster", "cluster_uuid" : "KauuQNszTXKOoOdhwgZK6Q", "version" : { "number" : "5.2.2", "build_hash" : "f9d9b74", "build_date" : "2017-02-24T17:26:45.835Z", "build_snapshot" : false, "lucene_version" : "6.4.1" }, "tagline" : "You Know, for Search" }  1. Next verify that we have some indices in Elasticsearch containing data from Logstash:  $ curl http://localhost:9200/_cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open logstash-2017.09.01 Sv3K6CEyQ2GEl6HtjU3zKQ 5 1 544 3 1.2mb 1.2mb yellow open .monitoring-kibana-2-2017.09.01 -WfBYJJETTm2qRshojGHBw 1 1 30 0 46.7kb 46.7kb yellow open .kibana tjoLGboOQxOIpsXPQAiV6g 1 1 1 0 3.1kb 3.1kb yellow open .monitoring-logstash-2-2017.09.01 NZVw7B7aRjGh4_Pk1Q_hGw 1 1 30 0 51kb 51kb yellow open .monitoring-es-2-2017.09.01 x7JK10GNRwSqJyq63-Padw 1 1 440 0 709.7kb 709.7kb yellow open .monitoring-data-2 75Uv2spMTGiz_-qzjfx3dg 1 1 4 0 9.7kb 9.7kb   1. Finally, we need to configure Kibana a bit:  1. Open [http://localhost:5601] i your web browser. You should see something like:  ![kibana-1]  1. Accept the default values for what index to use and click on the Create button.  ![kibana-2]  1. Kibana will now display the fields in the Logstash index. Click on the "_Discover_" tab in the blue menu to the left.  ![kibana-3]  1. Kibana will now show log events. Select and add the following fields: , , ,  and .  ![kibana-4]  1. Finally configure the time range and refresh rate to be used by Kibana: 1. Click on the display of the current time window in the upper right corner, i.e. "_Last 15 minutes_" 2. A setting dialog for the "_Time Range_" is displayed 3. Click on the "_Auto-refresh_" tab in the top level menu 4. A setting dialog for the "_Refresh Interval_" is displayed 5. Change the default value "_Off_" to "_5 seconds_"  6. Click on the collapse button "^" in the upper right corner to get rid of the setup dialog  <img src="/assets/blogg/build-microservices-part-8/kibana-5.png" width="600">   Start to make a request using a unique product number, e.g. :  $ curl -ks https://localhost/api/product/123456 -H "Authorization: Bearer $TOKEN" | jq .  Search for a business key, i.e. the Product Id  in our case:  ![kibana-6]  You will find two log events related to this business key. To find all other log events  and make a new search based on that:  ![kibana-7]  Voila, here are all the log events for the processing of the request!  **Nice, isn't it!**  > **Note:** If you wonder from where the very useful traceId's came, you can take a look into the previous blog post [blog post #7] about [spring-cloud-sleuth]!   Remove the search criteria  from the search dialog in Kibana.  Next, scale the Recommendation Service to two instances:  $ docker-compose scale rec=2  After a while you should see a log event in Kibana from the  with a message like:  Started RecommendationServiceApplication in 13.863 seconds   Now, filter log events in Kibana so that you only will see log events from the Recommendation Service. Click on the "_ml___service_" field in the list of selected fields to the left, then click on the magnifying glass with a "+" - sign after the .  <img src="/assets/blogg/build-microservices-part-8/kibana-8.png" width="300">  A filter for the recommendation service is now displayed under the search field:  <img src="/assets/blogg/build-microservices-part-8/kibana-9.png" width="300">  Now, only events from the "_recommendation-service_" should be visible!  Make a few new calls and see how the value of the "_syslog___hostname_" varies as requests are load balanced over the two instances:  ![kibana-10]  There is much more to say about the ELK stack but this is as far we get in this blog post. We have, at least, seen how to get started using the ELK stack in a containerized world :-)   Now we have seen how we can use the ELK stack to capture, search and visualize log events. In the [previous blog post][blog post #7] we saw how we could use Zipkin together with Spring Cloud Sleuth to understand the response times in a request that is processed by multiple microservices. One piece of important information is still missing though, i.e. how much hardware resources are utilized by our microservices on an individual level. For example, how much CPU, memory, disk and network capacity is each microservice instance consuming?  This is a question that I will try to answer in the next blog post in the [blog series - Building Microservices]  [comment-links]: #   [blog series]: https://callistaenterprise.se/blogg/teknik/2015/05/20/blog-series-building-microservices/ [blog post #5]: /blogg/teknik/2016/09/30/building-microservices-part-5-springcloud11-docker4mac/ [blog post #7]: /blogg/teknik/2017/07/29/building-microservices-part-7-distributed-tracing/  [spring-cloud-sleuth]: http://cloud.spring.io/spring-cloud-sleuth/ [spring-cloud]: http://projects.spring.io/spring-cloud/  [Elastic]: https://www.elastic.co [Logstash]: https://www.elastic.co/products/logstash [Elasticsearch]: https://www.elastic.co/products/elasticsearch [Kibana]: https://www.elastic.co/products/kibana [X-Pack]: https://www.elastic.co/products/x-pack  [logstash-docker]: https://www.elastic.co/guide/en/logstash/current/docker.html [elasticsearch-docker]: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html [kibana-docker]: https://www.elastic.co/guide/en/kibana/current/docker.html  [logstash-input]:https://www.elastic.co/guide/en/logstash/current/input-plugins.html [logstash-codec]: https://www.elastic.co/guide/en/logstash/current/codec-plugins.html [logstash-filter]: https://www.elastic.co/guide/en/logstash/current/filter-plugins.html [logstash-output]: https://www.elastic.co/guide/en/logstash/current/output-plugins.html  [logstash-gelf-muliline-issue-1]: https://github.com/elastic/logstash/issues/4308 [logstash-gelf-muliline-issue-2]: https://github.com/logstash-plugins/logstash-input-gelf/issues/37  [tail]: https://en.wikipedia.org/wiki/Tail_  [12-factor-log-stream]: https://12factor.net/logs  [comment-images]: #  [system landscape]: /assets/blogg/build-microservices-part-5/log-events.png [implementation-1]: /assets/blogg/build-microservices-part-5/implementation-1.png [implementation-2]: /assets/blogg/build-microservices-part-5/implementation-2.png
 You are a dedicated team. You have worked hard for a long time building a large complex system. It hasn’t been easy but your reasonably happy with the state your product is in. The system has been running in various staging environments for a long time. You have good test coverage. You use continuous integration.  Now comes the final sprint. The plan is to stabalize the system in those weeks that are left to enable a smooth production start.  Fix those unresolved bugs that hurts the most. Check the logs for any abnormal situations. Do some user training. Migrate production data. Prepare for the production start.  Are there some things you should avoid in this situation?  -[readmore]-  There is a great risk that new functionality introduces new bugs. In worst case in may slow down the system in which case you need to optimize your database queries which can possible add some new bugs to the system. If you are really late and the pressure is high it may even be tempting to do the unspeakable - cut down on tests, and we all know what that leads to.  May have unwanted side-effects. If there already are 100+ jars in the project dependency list, maybe that's enough ?  May seem like a good idea, but stay away from it. Generally beware of architects and their ideas.  May save some short term cost, but is really risky. Unless you are absolutely sure that all the guys that are left can fix bugs in all parts of the system as fast as those not longer there.  Takes the focus from what is most important. Besides, planning is boring.  If you really want to increase the risk delaying the production start, do it all.  If your life feels too slow, feel free to apply any number of the things above in your project. Don’t call me if it doesn’t work out. Been there, done that, survived - sort of.
The vendors begin to ship tooling for the vision of re-usable "use-case components"! At Cadec 2007 and Cadec 2008 we talked about the value and challenges associated with modularization of web applications. A lot of projects we've been into, would have saved a considerable amount of time for developer- tester- and business resources if we could build modular web dialogs  with ease.  As frameworks mature, they get support for the required underspinnings. Open source stacks may now be combined to acheive the values of packaged, reusable, componentized user functions . The "Web Framework Dream Team" described at Cadec 2008 is one such example. But what are the vendors doing in this space?  IBM? Nothing. SAP? Netweaver Web Dynpro framework and tooling is close. Red Hat? Jboss Seam leverages the jBPM engine to create a full-featured framework named Seam Page Flow. The IDE-support is still lagging, though. So, who is in the front? It seems to be Oracle with their upcoming Oracle JDeveloper 11g. The preview reveals a full graphical editor and a framework for re-use of use-case components. Oracle labels them "Task Flow". As usual, jumping into a vendor suite locks you in, but with an architecture like Task Flow fully integrated into the bigger picture , the business case may be there.
Many are the situations where there’s a need to organize information for subsequent use, and one way to do this is to use a controlled vocabulary. This need may arise in a limited setting, where your requirements can be managed off the cuff, or it could arise in a setting where secondary use of information is foreseen but exactly how is unknown. In these more intricate circumstances, an ontology may serve you well.  -[readmore]-  In this short blog series, I intend to explore the topic of ontologies, with a focus on the technical aspects of those. But to discuss this topic, we first need to settle on a terminology. To do that, we need to explore the terminology of terminology itself, and the semiotic triangle in particular.   As any triangle, the semiotic triangle has three vertices. These are:  - **Symbol/term**: A symbol is some item used to denote some concept. Mostly when we communicate, the symbol is a word or a set of words. This symbol we call a **term**. - **Concept** : A concept is the abstract idea of something. - **Referent**: A referent is an actual example of a concept, and the set of all referents are the set of all actual examples of that concept.  ![]  For a less theoretical explanation: an example. The symbol here is the string “tree”, which expresses the concept of a tree. The referents are actual trees.  ![semiotic triangel2.png]   Since we work with IT, I’ll add an example closer to our everyday business. Assume you and your colleagues are going on a retreat . To accommodate the wishes of the restaurant, dinner needs to be pre-ordered. To gather responses, your boss creates a poll on Slack. Let’s say there are three options for the main course: meat, fish and vegetarian. Let’s also say there are twelve responses, eight fish and four vegetarian.  There are three concepts, the three possible choices: “a choice of main course where the main ingredient is meat of some kind”, “a choice of main course where the main ingredient is fish of some kind” and “a choice of main course which is vegetarian”. There are three corresponding terms: “meat”, “fish” and “vegetarian” . The referents in this case are the actual responses from you and your colleagues: 8 responses of fish, 4 responses of vegetarian, and 0 responses of meat. As you can see, it is possible that a concept has no actual referents. That doesn’t make it any less of a concept.  So far, I haven’t yet touched on the actual topic of ontologies, but this theoretical foundation will be important in later parts of this series. The [following posts] and technical aspects of creating and managing an ontology.   Links for further reading:  [In My Own Terms]  [Glossary of terminology management]  [Glossary of terms used in terminology ]
 In part 2 of this blog series, we will: - Set up our Go workspace - Build our first microservice - Serve some JSON over HTTP using Gorilla Web Toolkit.  To stay focused on the Go fundamentals, we'll wait a bit with deploying it on Docker Swarm.  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  While serving JSON over HTTP isn't the only option for inter-service or external communication, we'll focus on HTTP and JSON in this blog series. Using RPC mechanisms and binary messaging formats such as [Protocol Buffers].  Another aspect to take into account is that many useful frameworks  relies on HTTP headers to transfer state about the ongoing request between participants. Examples we'll see in later blog posts is how we'll pass correlation ID's and OAuth bearers in HTTP headers. While other protocols certainly supports similar mechanisms, many frameworks are built with HTTP in mind and I'd rather try to keep our integrations as straightforward as possible.  Feel free to skip the section if you're already a seasoned Go dev. In my humble opinion, the structure of a Go workspace took some time getting used to. Where I'm used to typically having the root of a project as workspace root, Go conventions about how to properly structure a workspace so the go compiler can find source codes and dependencies is somewhat unorthodox, placing your source code under the _/src_ folder in a subtree named after its source control path. I strongly recommend reading the [official guide] before getting started. I wish I had.  Before writing our first lines of code , it should be straightforward enough.  In this blog series, we'll be using the built-in Go SDK tools we just installed for building and running, as well as following the idiomatic way of setting up a Go workspace.  All commands are based on a OS X or Linux dev environment. If you're running Windows, please adapt the instructions as necessary.  mkdir ~/goworkspace cd goworkspace export GOPATH=  Here we created a root folder and then assigned the environment variable [GOPATH] to that folder. This is the root of our workspace under which all Go source we write or 3rd party libraries we'll use will end up. I recommend adding this GOPATH to your _.bash_profile_ or similar so you don't have to reset it each time you open up a new console window.  Given that we're in the root of the workspace , execute the following statements:  mkdir -p src/github.com/callistaenterprise  If you want to follow along and code stuff yourself, execute these commands:  cd src/github.com/callistaenterprise mkdir -p goblog/accountservice cd goblog/accountservice touch main.go mkdir service  OR - you can clone the git repository containing the sample code and switch to branch P2. From the _src/github.com/callistaenterprise_ folder you created above, execute:  git clone https://github.com/callistaenterprise/goblog.git cd goblog git checkout P2  Remember - _$GOPATH/src/github.com/callistaenterprise/goblog_ is the root folder of _our project_ and what's actually stored on github.  Now we should have enough structure to easily get us started. Open up _main.go_ in your Go IDE of choice. I'm using IntelliJ IDEA with their excellent Golang plugin when writing the code for this blog series. Other popular choices seems to be [Eclipse  IDE.  The _main_ function in Go is exactly what you expect it to be - the entry point of our Go programs. Let's create just enough code to get something we can actually build and run:  package main  import ( "fmt" )  var appName = "accountservice"  func main { fmt.Printf }  Now, let's run it. Make sure you're in the folder corresponding to your _$GOPATH/src/github.com/callistaenterprise/goblog/accountservice_  > go run *.go Starting accountservice >  That's it! This program will just print and then exit. Time to add our very first HTTP endpoint!   *Note: The basics of these HTTP examples were derived from an excellent [blog post]*  To keep things neat, we'll put all HTTP service related files into the _service_ folder.   Create the file _webserver.go_ inside the _/services_ folder:  package service  import ( "net/http" "log" )  func StartWebServer {  log.Println err := http.ListenAndServe // Goroutine will block here  if err != nil { log.Println log.Println } }  We're using the built-in _net/http_ package to execute _ListenAndServe_ which starts a HTTP server on the specified port.  Update _main.go_ so we call the _StartWebServer_ function with a  hard-coded port:  package main  import ( "fmt" "github.com/callistaenterprise/goblog/accountservice/service" // NEW )  var appName = "accountservice"  func main { fmt.Printf service.StartWebServer // NEW }  Run the program again:  > go run *.go Starting accountservice 2017/01/30 19:36:00 Starting HTTP service at 6767  We now have a simple HTTP server listening to port 6767 on localhost. [Curl] it:  > curl http://localhost:6767 404 page not found  A 404 is exactly what we're expecting as we havn't declared any routes yet.  Stop the Web server by pressing Ctrl+C.  It's time to actually serve something from our server. We'll start by declaring our very first [route] that we'll use to populate the Gorilla router. In the _service_ folder, create _routes.go_:  package service  import "net/http"  // Defines a single route, e.g. a human readable name, HTTP method and the // pattern the function that will execute when the route is called. type Route struct { Name string Method string Pattern string HandlerFunc http.HandlerFunc }  // Defines the type Routes which is just an array  of Route structs. type Routes []Route  // Initialize our routes var routes = Routes{  Route{ "GetAccount", // Name "GET", // HTTP method "/accounts/", // Route pattern func { w.Header w.Write }, }, }  In the snippet above, we declared the path _/accounts/_ which we later can curl. Gorilla also supports complex routing with regexp pattern matching, schemes, methods, queries, headers values etc. so one is certainly not limited to just paths and path parameters.  For now, we will just return a tiny JSON message we've hard-coded as response:    We'll also need some boilerplate code that hooks up the actual [Gorilla Router] to the routes we declared. In _service_ folder, create _router.go_:  package service  import ( "github.com/gorilla/mux" )  // Function that returns a pointer to a mux.Router we can use as a handler. func NewRouter *mux.Router {  // Create an instance of the Gorilla router router := mux.NewRouter  // Iterate over the routes we declared in routes.go and attach them to the router instance for _, route := range routes {  // Attach each route, uses a Builder-like pattern to set each route up. router.Methods. Path. Name. Handler } return router }  In the import section for _router.go_ we see that we have declared a dependency on the _github.com/gorilla/mux_ package. See [here] for a good explanation on how go dependencies are fetched using _go get_.  In order for the above file to compile and run, we'll need to use _go get_ to fetch the declared package into our workspace:  > go get  This may take a little while since the Go tool is actually downloading all the source code required by the gorilla/mux package from https://github.com/gorilla/mux. This source code will end up in _$GOPATH/src/github.com/gorilla/mux_ on your local file system and it will be built into your statically linked binary.  Now, revisit _webserver.go_ and add the two following lines at the start of the StartWebServer function:  func StartWebServer {  r := NewRouter // NEW http.Handle // NEW  This attaches the Router we just created to the http.Handle for the root path _/_. Let's compile and run the server again.  > go run *.go Starting accountservice 2017/01/31 15:15:57 Starting HTTP service at 6767  Try to curl:  > curl http://localhost:6767/accounts/10000   Nice! We've just created our first HTTP service!  Given that we're exploring Go-based microservices due to alleged awesome memory footprint and good performance, we'd better do a quick benchmark to see how this performs. I've developed a simple [Gatling].  If you want to run the load-test yourself, make sure the "accountservice" is up and running on localhost and that you have cloned the source and checked out branch P2. You'll also need to have a Java Runtime Environment and [Apache Maven] installed.  Change directory to the _/goblog/loadtest_ folder and execute the following command from the command-line:  > mvn gatling:execute -Dusers=1000 -Dduration=30 -DbaseUrl=http://localhost:6767  This should start and run the test. The arguments are:  - users: Number of concurrent users the test will simulate - duration: For how many seconds the test will run - baseUrl: Base path to the host providing the service we're testing. When we move over to Docker Swarm, the baseUrl will need to be changed to the public IP of the Swarm. More on that in [part 5].  After the test has finished, it writes results to the console windows as well as a fancy HTML report into _target/gatling/results/_.   _Note: Later on, when the services we're building are running inside Docker containers on Docker Swarm, we'll do all benchmarks and metrics capturing there. Until then, my mid-2014 MacBook Pro will have to suffice._  _Before_ starting the load test, the memory consumption of the Go-based "accountservice" is as follows according to the OS X task manager:  ![mem use]  1.8 mb, not bad! Let's start the Gatling test running 1K req/s. Remember that this is a _very_ naive implementation that just responds with a hard-coded string.  ![mem use2] Ok, serving 1K req/s makes the "accountservice" consume about 28 mb of RAM. That's still perhaps 1/10th of what a Spring Boot application uses at startup. It will be very interesting to see how this figures changes once we start to add some real functionality to it.  ![cpu use] Serving 1K req/s uses about 8% of a single Core.  ![performance] Note sure how Gatling rounds sub-millisecond latencies, but mean latency is reported as 0 ms with _one_ request taking a whopping 11 millisecond. At this point, our "Accountservice" is performing admirably, serving on average 745~req/s in the sub-millisecond range.  In the [next part], we'll actually make our _accountservice_ do something useful. We'll add a simple embedded database with Account objects that we'll serve over HTTP. We'll also take a look at JSON serialization and check how these additions to the service affects its footprint and performance.
 I have been implementing new JMS services in my project. The services are defined by XML schemas. During development I implemented basic unit tests to make sure JAXB validation worked as expected. The test was catching some of the validation errors that caused by missing data or wrong occurrences of some elements. I was quite satisfied when I finalized the implementation with my unit test, though I knew that I hadn't tested all variants of possible data in the database that could possibly exist.  Next step before deliver to test was to deploy in a test environment and then trigger the MDB to create the message and send it to my destination queue. The project was already using the tool [Hermes JMS] for browsing the queues.  I was a bit surprised over how easy it was to start up with Hermes. The project had already configured all queues and necessary setup for the JMS/MQ part of course in a xml file. What I needed to do was only to download the Hermes and then install it on my PC on the expected path  of Hermes is then using the existing configuration called hermes-config.xml created by the project and I was able to reach all queues immediate.  Using the tool was easy. You get the sessions  on the left hand; on the right hand you have the messages for the selected queues and below you can check out the content of the messages when selecting one.  During the development of new services it is hard to do functional tests on the integration. By using the tool I could easily browse and check the messages and exceptions that occurred with a database with some real data. Of course it is time consuming to test this way - but still to write all kind of variants that the database can hold for you would probably take even longer. So I think this was the best way to combine some unit testing on basic level and the do some more hands on test to see what happens when you really trigger off the messages. Functional test on this level is hard to test by unit test or integration test that is automated. By doing this functional testing before delivering to the test team I can at least be sure that they will be able to test the new integration.  By the way on the homepage for Hermes there is a great header called "Donating" with the wonderful text: It would be nice if I could cover these  with donations from happy users so if you feel inclined then please donate". This type of licence is called "commercially friendly way". Ironic and humoristic? I think so. But still it is a good and easy tool to be used when interacting with JMS providers and I can really recommend it.
 Create highly concurrent software that is very easy to write and reason about. -[readmore]-   I have spent some time using Google’s Go language, so I was thrilled when I stumbled upon Parallel Universe’s [Quasar] framework. If you have been using Go then you are quite familiar with its Channels and go-routines.  Well, now it’s seems like we can use the same way of writing our software on the JVM with Quasar fibers and channels.   To make things somewhat more clear an explanation might be in order. Depending on what language background you have this is most likely nothing new to you, but you might be used to other names for these features. As mentioned earlier Go has named these features Channels and Go-rutines. In Erlang you spawn processes, I could continue listing other implementations since it’s seems like every language has their own implementation, heck there is even a couple of other implementations for Java then Quasar Fibers. I guess I have to mention Haskell threads and sparks not to get my inbox filled with angry Haskellers emails.  So what is a Fiber or any other implementation of the same functionality? Well if you were old enough to us Java 1.1 then you have already seen them on the JVM under the name Green threads, but was later removed due to various reasons. A Fiber or lightweight-thread  thread mapping, meaning that its maps a M number of application threads onto N number of threads in the operating system. While Java today uses 1:1 mapping meaning that an application-level thread maps to one os thread. And that’s one of the reasons we see non-blocking programing becoming yet again more and more popular since in todays Java, blocking a thread on application-level actual means blocking resources on os level which is quite resource consuming.  I thought about writing something on how the Quasar Framework achieves these lightweight-threads. But if you are anything like me you will just scroll past all this text in search for a block of code. If you should be interested in how they do the short answer is that they implement continuations using bytecode instrumentation and using a separate stack to hold state. The long answer you can get from [Ron Presslers presentation at JVMLS].  So now we got some idea what a Fiber is, but what problem do they solve?   Using the multithreaded approach that’s been the de facto standard for some years now have some problems when it comes to handling concurrency. To be clear I’m talking about Java, so Node folks no need to flame. It’s not uncommon that we see thread-pool starvation and high memory consumption due to “one-request-one-thread” mapping. One way of solving these issues is using non-blocking techniques like callbacks and monads. The problem with these techniques is not that they don’t work, but they can be quite hard to understand and debug if you as most of us are used to the old way of understanding what will happen when by just looking at the code. Quasar fibers gives you a programming model that you already are familiar with but does not hog all your os resources.   If you are more interested in reading about non-blocking techniques and reactive frameworks I truly recommend my colleges great [presentation].    To demonstrate how we can use fibers in a real life situation we are going to build ourselves a simple API-Gateway, a well now pattern for fronting for example [microservices]. The idea is that one request leads to multiple internal API calls and we have a gateway facing the user to be able to customize the outer API to fit the need. For example we might want to keep down the amount of API calls for mobile users to lower the round trips by mashing multiple internal responses to one external. It also gives us the possibility to provide a more fine-grained API to lower the cost of bandwidth.  ![API-Gateway]  We will deploy our gateway on Tomcat 8.    First of we need to modify which class loader Tomcat should use for loading our webapp. Easiest way to do that is to download **comsat-tomcat-loader.jar** and drop it into you shared lib folder. Then update your **META-INF/context.xml** like.   To get started here is the content of my build.gradle file.   As you see there is something called comsat that keeps showing up, Comsat is an open-source set of libraries that integrates with Quasar. The reason for using these are because if we would do an actual thread blocking operation inside a fiber it would actually block the underlying os thread, thus it would be pointless to run it in a fiber instead of an actual thread.  We will be using Jersey to define our REST-Services and for that we will use the comsat-jersey-server to make our services fiber aware. **WEB-INF/web.xml**   So now everything is setup and it’s time for us to write our first REST service.   This might be the naïve first approach to creating a gateway. We have a list of some endpoints that we iterate over and sum up the response that’s then returned. If we were to run this service in a default servlet container each user of this service would hold one os thread for as long as it took to get the response from all the endpoints in our list. But we have configured to use the comsat servlet container. Which instead of fetching a thread from the pool for each user, spawns a new fiber. I have annotated get function with  which is one way of telling Quasar what method to instrument.  If you have a keen eye you would see that I initiate the Apache HttpClient with the **FiberHttpClientBuilder** provided by the comsat Apache HttpClient library. This is because as I mentioned earlier that if we were to do a blocking IO operation as **HttpClient.execute** does we would block the underlying os thread. What **FiberHttpClientBuilder** does is to wrap Apache **HttpAsyncClient** and letting us operate on it as if it were a blocking operation.  To speed this method up we are now going to spawn new Fibers for each Http request and use Quasar Channels to safely communicate between fibers to collect and build the response.   So with not much effort and easy to read code, we are able to handle a higher number of concurrency without having to resort to complex asynchronous API’s. Quasar hides that for us and gives us the possibility to work with a programming model we are quite familiar with.   Quasar is a fairly new framework, as of now version 0.7 is out, but it is gaining some traction. I think it’s really interesting and it’s worth keeping an eye on. The biggest concern I have is that you have to make 3d-party libraries work in a Fiber context trough wrapping its aync-api. Parallel Universe provides good documentation and support classes to do so, but I would still write my software asynchronous from top to bottom, and it’s not that hard to do given that you have the right tools to do so.  I would recommend everyone to give it a go, since it shows a lot of potential and lets you write easy to read and write code in a programming model that might fit you.
 [Gatling] is a powerful load-testing framework with excellent support for testing http-based applications out of the box , but unfortunately they too are out-of-date with the 2.2.x versions.  Since I recently needed a custom protocol to load test a Serverless architecture based on AWS [Lambda] functions and [Kinesis] streams, I had to dig into the API changes to get things working. I ended up reading the source code for the latest version of both the http and jms protocols, in order to understand the APIs. It took me quite a while, so I might as well share my findings!  -[readmore]-  [comment]: #  [Gatling]: http://gatling.io/ [GatlingProtocolBreakdown]: https://github.com/jagregory/gatling/blob/master/GatlingProtocolBreakdown.md [extensions]: http://gatling.io/docs/2.2.3/extensions/index.html [write-custom-protocol-for-gatling]: https://www.trivento.io/write-custom-protocol-for-gatling/ [load-testing-zeromq-with-gatling]: http://mintbeans.com/load-testing-zeromq-with-gatling/ [AWS]: https://aws.amazon.com [Lambda]: https://aws.amazon.com/lambda [Kinesis]: https://aws.amazon.com/kinesis [gatling-aws.git]: https://github.com/callistaenterprise/gatling-aws.git  So here we go:  Most of the work involved in building a custom Gatling protocol consists of creating the internal DSL used in the Gatling scenario. The DSL for a protocol is typically split in two parts: Configuration of the Protocol, and invoking the protocol Actions and optionally cheking the Action's result. For both the Protocol and the Actions, you need to create a definition of the Protocol and the Actions themselves, as well as a ProtocolBuilder and ActionBuilders to support the DSL for using them. Optionally, you may also want a custom Check and corresponding CheckBuilder to validate the outcome of an Action.  In writing a custom protocol for invoking AWS Lambda functions, we will hence need to define a number of classes:  * AWSProtocol * AWSProtocolBuilder * LambdaAction * LambdaActionBuilder * LambdaCheck * LambdaCheckBuilder  Armed with the DSL these classes provide, we should be able to use them in a Gatling test scenario like this:   Let's begin! We start by defining the Protocol and corresponding ProtocolBuilder. Our protocol should be configured with 3 mandatory properties: accessKey, secretKey and region. These properties should be available for the Action later on. The Protocol looks like this:   In order to make the protocol properties easily accessible for the Action, we wrap them in a Gatling protocol Component:   Now that we have the protocol itself defined, we need a ProtocolBuilder to support the DSL for creating and configuring the protocol:   The Builder defines the 3 methods that our DSL provides for configuring the Protocol with the parameters for accessKey, secretKey and region. Notice the usage of intermediate classes: We start with the AwsProtocolBuilderBase, and pass through AwsProtocolBuilderSecretKeyStep and AwsProtocolBuilderRegionStep until we land in a fully configured protocol AwsProtocolBuilder. This pattern is typical for creating an internal DSL.  Next, we define the LambdaAction, which performs the actual work. The Action takes a mandatory parameter functionName, an optional payload as paramter for the Lambda function and an optional list of Checks to validate the outcome. The executeOrFail method below is where the Lambda function call is made.   The DSL for the LambdaAction is provided by two Builders. LambdaActionBuilder provides access to theprotocol attributes from within the LambdaAction, whereas the LambdaProcessBuilder provides the DSL for configuring an optional argument payload to the Lamdba, and for configuring optional Checks.    Note how the LambdaProcessBuilder uses copy to implement the optional payload and checks, since the LambdaProcessBuilder itself is immutable. If an optional payload is provided, we create a copy of the builder with the optional parameter set.  We also need to define the Checks classes to support validating the Lambda function result. All checks operate on the resulting payload as a String:   The Extender and Preparer functions are required by the Gatling base check support.  We support validating the Lambda executing result using a Regex, an XPath or JsonPath expression or by providing a custom function.      We also provide a supporting trait LambdaCheckSupport for the DSL to construct and configure the checks:   Finally, we define a trait AwsDsl to provide the toplevel DSL builder object , as well as a DSL builder method lambda for the action:    And we're done. Not as simple as I thought when I started, but quite doable. The documentation for the Gatling extension mechanism could definitely be better, but it is extremely powerful once you understand it. I hope this article can be useful for others. You can find the full source code here:  https://github.com/callistaenterprise/gatling-aws.git.
 As our microservices and the landscape they operate in grows more complex, it also becomes increasingly important for our services to provide a mechanism for Docker Swarm to know if they're feeling healthy or not. Therefore, we'll take a look at how to add health checks in this sixth part of the [blog series].  For example, our "accountservice" microservice isn't very useful if it cannot: - Serve HTTP - Connect to its database  The idiomatic way to handle this in a microservice is to provide an [healthcheck endpoint] that in our case - since we're HTTP based - should map to _/health_ and respond with a HTTP 200 if things are OK, possibly together with some machine-parsable message explaining what's OK. If there is a problem, a non HTTP 200 should be returned, possibly stating what's not OK. Do note that some argue that failed checks should return 200 OK with errors specified in the response payload. I can agree with that too, but for the case of simplicity we'll stick with non-200 for this blog post. So let's add such an endpoint to our "account" microservice.  As always, feel free to checkout the appropriate branch from git to get all changes of this part up front:  git checkout P6  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  Our service won't be of much use if it cannot access its underlying database. Therefore, we'll add a new function to the [IBoltClient]_:  type IBoltClient interface { OpenBoltDb QueryAccount Seed Check bool // NEW! }  The Check method is perhaps a bit naive, but will serve its purpose for the sake of this blog. It specifies that either _true_ or _false_ will be returned depending on whether the BoltDB was accessible or not.  Our implementation of _Check is not very realistic either, but it should explain the concept well enough:  // Naive healthcheck, just makes sure the DB connection has been initialized. func  bool { return bc.boltDB != nil }  The mocked implementation in [mockclient.go] follows our standard stretchr/testify pattern:  func  bool { args := m.Mock.Called return args.Get }    This is very straightforward. We'll start by adding a new _/health_ route to our _/accountservice/service/routes.go_ file below the existing route to _/accounts/_:  Route{ "HealthCheck", "GET", "/health", HealthCheck, },  We declared that the route shall be handled by a function named HealthCheck that we now will add to the _/accountservice/service/handlers.go_ file:  func HealthCheck { // Since we're here, we already know that HTTP service is up. Let's just check the state of the boltdb connection dbUp := DBClient.Check if dbUp { data, _ := json.Marshal writeJsonResponse } else { data, _ := json.Marshal writeJsonResponse } }  func writeJsonResponse { w.Header w.Header w.WriteHeader w.Write }  type healthCheckResponse struct { Status string  }  The _HealthCheck_ function delegates the check of the DB state to the _Check.   From the _/goblog/accountservice_ folder, build and run:  > go run *.go Starting accountservice Seeded 100 fake accounts... 2017/03/03 21:00:31 Starting HTTP service at 6767  Open a new console window and _curl_ the _/health_ endpoint:  > curl localhost:6767/health   It works!   ![docker healthcheck]  Next, we'll use the Docker [HEALTHCHECK] mechanism to let Docker Swarm check our service for liveness. This is done by adding a line in the Dockerfile:  HEALTHCHECK --interval=5s --timeout=5s CMD ["./healthchecker-linux-amd64", "-port=6767"] || exit 1  What's this _"healthchecker-linux-amd64"_ thing? We need to help Docker a bit with these health checks as Docker itself doesn't provide us with an HTTP client or similar to actually execute the health checks. Instead, the HEALTHCHECK directive in a Dockerfile specifies a command  of the program that was run, Docker will determine whether the service is healthy or not. If too many subsequent health checks fail, Docker Swarm will kill the container and start a new instance.  The most common way to do the actual healthcheck seems to be [curl] installed and at this moment we don't really want to deal with that. Instead, we'll use Go to brew our own little healthchecker program.   Time to create a new sub-project under the _/src/github.com/callistaenterprise/goblog_ path:  mkdir healthchecker  Then, create _main.go_ inside the _/healthchecker_ folder:  package main  import ( "flag" "net/http" "os" )  func main { port := flag.String flag.Parse  resp, err := http.Get // Note pointer dereference using *  // If there is an error or non-200 status, exit with 1 signaling unsuccessful check. if err != nil || resp.StatusCode != 200 { os.Exit } os.Exit }  Not an overwhelming amount of code. What it does:  - Uses the [flags] support in golang to read a _-port=NNNN_ command line argument. If not specified, fall back to port 80 as default. - Perform a HTTP GET to 127.0.0.1:[port]/health - If an error occurred or the HTTP status returned wasn't 200 OK, exit with a non-zero exit code. 0 == Success, > 0 == fail.  Let's try this. If you've stopped the "accountservice", start it again either by _go run *.go_ or by building it in a new console tab by going into the _"/goblog/accountservice"_ directory and build/start it:  go build ./accountservice  **Reminder: If you're getting strange compile errors, check so the GOPATH still is set to the root folder of your Go workspace, e.g. the parent folder of _/src/github.com/callistaenterprise/goblog_**  Then switch back to your normal console window  and run the healthchecker:  > cd $GOPATH/src/github.com/callistaenterprise/goblog/healtchecker > go run *.go exit status 1  Ooops! We forgot to specify the port number so it defaulted to port 80. Let's try it again:  > go run *.go -port=6767 >  No output at all means we were successful. Good. Now, let's build a linux/amd64 binary and add it to the "accountservice" by including the healthchecker binary in the Dockerfile. We'll continue using the _copyall.sh_ script to automate things a bit:  #!/bin/bash export GOOS=linux export CGO_ENABLED=0  cd accountservice;go get;go build -o accountservice-linux-amd64;echo built ;cd ..  // NEW, builds the healthchecker binary cd healthchecker;go get;go build -o healthchecker-linux-amd64;echo built ;cd ..  export GOOS=darwin  // NEW, copies the healthchecker binary into the accountservice/ folder cp healthchecker/healthchecker-linux-amd64 accountservice/  docker build -t someprefix/accountservice accountservice/  One last thing, we need to update the "accountservice" _Dockerfile_. It's full content looks like this now:  FROM iron/base EXPOSE 6767  ADD accountservice-linux-amd64 /  # NEW!! ADD healthchecker-linux-amd64 / HEALTHCHECK --interval=3s --timeout=3s CMD ["./healthchecker-linux-amd64", "-port=6767"] || exit 1  ENTRYPOINT ["./accountservice-linux-amd64"]  Additions:  - We added an ADD statement which makes sure the healthchecker binary is included in the image. - The HEALTHCHECK statement specifies our binary as well as some parameters that tells Docker to execute the healthcheck every 3 seconds and to accept a timeout of 3 seconds.  Now we're ready to deploy our updated "accountservice" with healthchecking. To automate things even further, add these two lines to the bottom of the _copyall.sh_ script that will remove and re-create the accountservice inside Docker Swarm every time we run it:  docker service rm accountservice docker service create --name=accountservice --replicas=1 --network=my_network -p=6767:6767 someprefix/accountservice  Now, run _./copyall.sh_ and wait a few seconds while everything builds and updates. Let's check the state of our containers using _docker ps_ that lists all running containers:  > docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 1d9ec8122961 someprefix/accountservice:latest "./accountservice-lin" 8 seconds ago Up 6 seconds  107dc2f5e3fc manomarks/visualizer "npm start" 7 days ago Up 7 days  The thing we're looking for here is the _""_ text under the STATUS header. Services without a healthcheck configured doesn't have a health indication at all.   To make things a bit more interesting, let's add a testability API that lets us make the endpoint act unhealthy on purpose. In _routes.go_, declare a new endpoint:  Route{ "Testability", "GET", "/testability/healthy/", SetHealthyState, },  This route  provides us with a REST-ish endpoint for failing healthchecks on purpose. The _SetHealthyState_ function goes into _goblog/accountservice/handlers.go_ and looks like this:  var isHealthy = true // NEW  func SetHealthyState {  // Read the 'state' path parameter from the mux map and convert to a bool var state, err = strconv.ParseBool  // If we couldn't parse the state param, return a HTTP 400 if err != nil { fmt.Println w.WriteHeader return }  // Otherwise, mutate the package scoped "isHealthy" variable. isHealthy = state w.WriteHeader }  Finally, add the _isHealthy_ bool as a condition to the HealthCheck function:  func HealthCheck { // Since we're here, we already know that HTTP service is up. Let's just check the state of the boltdb connection dbUp := DBClient.Check  if dbUp && isHealthy { // NEW condition here! data, _ := json.Marshal( ... ... }  Restart the accountservice:  > cd $GOPATH/src/github.com/callistaenterprise/goblog/accountservice > go run *.go Starting accountservice Seeded 100 fake accounts... 2017/03/03 21:19:24 Starting HTTP service at 6767  Make a new healthcheck call from the other window:  > cd $GOPATH/src/github.com/callistaenterprise/goblog/healthchecker > go run *.go -port=6767 >  First attempt successful. Now change the state of the accountservice using a curl request to the testability endpoint:  > curl localhost:6767/testability/healthy/false > go run *.go -port=6767 exit status 1  It's working! Let's try this running inside Docker Swarm. Rebuild and redeploy the "accountservice" using _copyall.sh_:  > cd $GOPATH/src/github.com/callistaenterprise/goblog > ./copyall.sh  As always, wait a bit while Docker Swarm redeploys the "accountservice" service using the latest build of the "accountservice" container image. Then, run _docker ps_ to see if we're up and running with a healthy service:  > docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 8640f41f9939 someprefix/accountservice:latest "./accountservice-lin" 19 seconds ago Up 18 seconds   Note CONTAINER ID and the CREATED. Call the testability API on your docker swarm IP :  > curl $ManagerIP:6767/testability/healthy/false >  Now, run _docker ps_ again within a few seconds.  > docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 0a6dc695fc2d someprefix/accountservice:latest "./accountservice-lin" 3 seconds ago Up 2 seconds   See - a brand new CONTAINER ID and new timestamps on CREATED and STATUS. What actually happened was that Docker Swarm detected three  consecutive failed healthchecks and immediately decided the service had become unhealthy and need to be replaced with a fresh instance which is exactly what happened without any administrator intervention.  In this part we added health checks using a simple _/health_ endpoint and a little healthchecker go program in conjunction with the Docker HEALTHCHECK mechanism, showing how that mechanism allows Docker Swarm to handle unhealthy services automatically for us.  In the [next part], we'll dive deeper into Docker Swarm mechanics as we'll be focusing on two key areas of microservice architecture - Service Discovery and Load-balancing.
 This is the third part of my [blog series on reactive programming], which will give an introduction to WebFlux - Spring's reactive web framework.  -[readmore]-    The original web framework for Spring - Spring Web MVC - was built for the Servlet API and Servlet containers.  WebFlux was introduced as part of Spring Framework 5.0. Unlike Spring MVC, it does not require the Servlet API. It is fully asynchronous and non-blocking, implementing the Reactive Streams specification through the Reactor project .   WebFlux requires Reactor as a core dependency but it is also interoperable with other reactive libraries via Reactive Streams.    Spring WebFlux supports two different programming models: annotation-based and functional.   If you have worked with Spring MVC, the annotation-based model will look quite familiar since it is using the same annotations from the Spring Web module as are being used with Spring MVC. The major difference being that the methods now return the reactive types Mono and Flux. See the following example of a RestController using the annotation-based model:   Some explanations about the functions used in the example: - The  function is used to transform the item emitted by a Mono by applying a synchronous function to it. - The  function is used to transform the item emitted by the Mono asynchronously, returning the value emitted by another Mono. - The  function provides a default value if a Mono is completed without any data.   The functional programming model is lambda-based and leaves the application in charge of the full request handling. It is based on the concepts of  and .  HandlerFunctions are used to generate a response for a given request:   The RouterFunction is used to route the requests to the HandlerFunctions:   Continuing with the same student example we would get something like the following using the functional style.  A StudentRouter:   And a StudentHandler: Some explanations about the functions used in the example: - The  function has the same purpose as , but instead of providing a default value, it is used for providing an alternative Mono.   Comparing the two models we can see that: - Using the functional variant requires some more code for things such as retrieving input parameters and parsing to the expected type. - Not relying on annotations, but writing explicit code does offer some more flexibility and could be a better choice if we for example need to implement more complex routing.   WebFlux runs on non-Servlet runtimes such as Netty and Undertow  as well as Servlet 3.1+ runtimes such as Tomcat and Jetty.  The Spring Boot WebFlux starter defaults to use Netty, but it is easy to switch by changing your Maven or Gradle dependencies.  For example, to switch to Tomcat, just exclude spring-boot-starter-netty from the spring-boot-starter-webflux dependency and add spring-boot-starter-tomcat:     Spring Boot provides auto-configuration for Spring WebFlux that works well for the common cases. If you want full control of the WebFlux configuration, the  annotation can be used .  If you want to keep the Spring Boot WebFlux config and just add some additional WebFlux configuration, you can add your own @Configuration class of type WebFluxConfigurer .  For details and examples, read the [WebFlux config] documentation.  To get Spring Security WebFlux support, first add the spring-boot-starter-security dependency to your project. Now you can enable it by adding the  annotation to your Configuration class   The following simplified example would add support for two users, one with a USER role and one with an ADMIN role, enforce HTTP basic authentication and require the ADMIN role for any access to the path /students/admin:   It is also possible to secure a method rather than a path, by first adding the annotation  to your config:   And then adding the  annotation to the methods to be secured. We might for example want our POST, PUT and DELETE methods only to be accessible by the ADMIN role. Then the PreAuthorize annotation could be applied to those methods, like:   Spring Security offers more support related to WebFlux applications, such as CSRF protection, OAuth2 integration and reactive X.509 authentication. For more details, read the followig section in the Spring Security documentation: [Reactive Applications]    Spring WebFlux also includes a reactive, fully non-blocking web client. It has a functional, fluent API based on Reactor.  Let's take a look at a  simplified example, how the WebClient can be used to query our StudentController:     For testing your reactive web application, WebFlux offers the WebTestClient, that comes with a similar API as the WebClient.  Let's have a look at how we can test our StudentController using the WebTestClient:    With Spring 5, WebSockets also gets added reactive capabilities. To create a WebSocket server, you can create an implementation of the  interface, which holds the following method:   This method is invoked when a new WebSocket connection is established, and allows handling of the session. It take a  as input and returns Mono&lt;Void> to signal when application handling of the session is complete.  The WebSocketSession has methods defined for handling the inbound and outbound streams: Spring WebFlux also provides a  with implementations for Reactor Netty, Tomcat, Jetty, Undertow, and standard Java.  For more details, read the following chapter in Spring's Web on Reactive Stack documentation: [WebSockets]   RSocket is a protocol that models Reactive Streams semantics over a network. It is a binary protocol for use on byte stream transports such as TCP, WebSockets, and Aeron. For an introduction to this topic, I recommend the following blog post that my colleague Pär has written: [An introduction to RSocket]  And for more details on Spring Framework's support for the RSocket protocol: [RSocket]  This blog post demonstrated how WebFlux can be used to build a reactive web application. The next and final blog post in this series will show how we can make our entire application stack fully non-blocking by also implementing non-blocking database communication - using R2DBC !   [Spring Framework documentation - Web on Reactive Stack]  [Spring Boot Features - The Spring WebFlux framework]  [Spring Security - Reactive Applications]
 Full-stack Enterprise Service Buses  like BizTalk, WebSphere MB, Mule, ServiceMix et.al have been dominating the Enterprise Integration scene for quite some time. But with the rise of [Microservices] and its enabling tools such as [Spring Boot] and [Docker], light-weight Integration Frameworks are becoming more and more attractive. In this post, we'll compare the two most popular frameworks, [Apache Camel] and [Spring Integration] in terms of expressiveness and conciseness.  -[readmore]-  [comment]: #  [Spring Integration]: http://projects.spring.io/spring-integration/ [Spring Integration Cafe Example]: https://github.com/spring-projects/spring-integration-samples/tree/master/dsl/cafe-dsl [Apache Camel]: http://camel.apache.org/ [Apache Camel Cafe Example]: http://camel.apache.org/cafe-example.html [ESB]: https://en.wikipedia.org/wiki/Enterprise_service_bus [Microservices]: https://callistaenterprise.se/blogg/teknik/2015/05/20/blog-series-building-microservices/ [Spring Boot]: http://projects.spring.io/spring-boot/ [Docker]: https://www.docker.com/ [Enterprise Integration Patterns]: http://www.enterpriseintegrationpatterns.com/ [Ramblings]: http://www.eaipatterns.com/ramblings.html  [comment]: #  [EIP-book]: /assets/blogg/apache-camel-vs-spring-integration/eip-book.png [cafe-example]: /assets/blogg/apache-camel-vs-spring-integration/cafe-example.png  Integrating Enterprise Applications is challenging, but luckily the challenges are often well known and understood. [Enterprise Integration Patterns], the seminal work by Gregor Hophe and Bobby Woolf, provides a catalogue of typical, recurring challenges and software patterns to address them.  ![EIP Book][EIP-book]  Implementing these patterns can be a daunting task, though. This is where an Integration Framework can be of great help. While there are quite some Integration Frameworks out there, two of them have gained broad adoption: [Apache Camel] and [Spring Integration]. These two frameworks both share the same goal: to provide an easy-to-use mechanism for implementing typical integration tasks  transport adapters, ready-made implementations of EIPs, as well as basic management support via JMX. Hence from a functionality point of view, the two frameworks are quite equal.  Apache Camel was released in version 1.0 in 2007. Already from version 1.0, it came with a Java DSL as well as an XML DSL built on top of Spring XML. Now being at version 2.15, there are additional DSLs in a wide variety of languages . Spring Integration was released in version 1.0.0 two years later, in 2009. Being part of the Spring family, XML-based configuration was initially the only option, but since recently Spring Integration also provides a Java DSL.  While one can argue that one particular syntax is better than another  are typically being *read* for more often than it is written, the ability to clearly and concisely communicate its intention is a key discriminating factor.  The often used Cafe example  can illustrate the difference. The domain is that of a Cafe, and illustrates Routing, Splitting and Aggregation.  ![Camel Alternatives][cafe-example]  Both Spring Integration and Apache Camel includes this example as part of their sample projects. In the [Spring Integration Cafe Example], the integration flow looks like this:   There are quite a lot of details in there , but the key point here is the fundamental use of Gateways and Channels to implement the higher-level EIPs.  In contrast, the [Apache Camel Cafe Example] focus on the higher-level EIPs, using a vocabulary that is more close to the "business" than the technical domain:   Agreed, the two examples are not directly comparable , but I think the fundamental difference in approach is clearly visible.  Spring Integration and Apache Camel are both well-designed and highly capable light-weight integration frameworks. From a feature perspective, they are more or less equal. Any of them would be an excellent choice in the assignment I'm currently working on. But if you, like me, have the luxury to choose between them, I think the semantic expressiveness of the Apache Camel DSL, its ability to clearly communicate the intention of a particular integration flow, is an important competitive edge.
 [Spring Boot] makes it very easy to create Spring based applications. Spring Boot takes an opinionated view of the Spring platform and third-party libraries, allowing us to get started with a minimal configuration. For example we can develop a Jave EE based web application without any configuration files. Not even a  file is required!  -[readmore]-  When required, however, we can take control over parts of the configuration and override the conventions that Spring Boot puts in play. We can also, if we really must, use traditional XML configuration files for some parts of the configuration. Please read the excellent [documentation] for extensive information.  In this blog we will develop a plain REST service using the [Spring Web MVC framework] as our build system.  If you want to check out the source code and test it on your own you need to have Java SE 7 and Git installed. Then perform:   This should result in a tree structure like:   Let's start with defining a vanilla REST Service using Spring MVC, next we look at the "magic" Spring Boot Application class and the Gradle build file that also is of great importance. We will wrap up the walk through with looking as some examples of how you easily can override the conventions that Spring Boot puts in play.  The REST service is implemented as a vanilla Spring MVC Rest-controller. The service takes two query parameters,  and , that defines the boundaries of the processing time of the service. The service will simulate a response time between the given boundaries by using .   The  annotation makes the code REST aware and the  annotation declares what URI the service should be mapped to. As you can see the service return an instance of the class . It is a POJO-class that looks like:   Spring MVC will automatically marshal the  object to either JSON or XML depending on what the client requests in the HTTP Accept header, by default JSON will be used. To have the XML processing automated we need the XML annotations  and  plus that JAXB is available on the classpath .  Here is where all the magic happens that for example removes the burden of developing and maintaining XML configuration files!  The source code required to do that looks like:   That is not that much source code, right?  The  annotation tells Spring to look for REST controllers in our code.  The  annotation tells Spring Boot to “guess” how you want to configure Spring, based on the dependencies we have in our build file. In this case it will figure out that we want to build a web app and run it on an embedded Tomcat instance with Spring MVC enabled .  Let's look into the build file to understand that a bit more...  As mentioned earlier we use Gradle as our build system. It gives us a much more compact and easier to maintain build file compared to using Maven. Please see the this [blog about Gradle] if you want to know more.  The build file, , looks like:   - First we declare that we want to use v1.0.0 of the Spring Boot Gradle-plugin - Next we applies a number of plugins, specifically the spring-boot and war plugins - Finally we setup a number of dependencies: -  drags in a convenient set of dependencies for building web applications, e.g. Spring Framework, Spring MVC, the Servlet API and embedded Tomcat. -  can be used to replace Tomcat with Jetty as our embedded Servlet container  -  adds some interesting out-of-the-box features for metrics and monitoring -  and  enables, as described above, our REST controller to automatically marshal our result to XML if so requested by the caller -  drags in a convenient set of test dependencies, such as JUnit, Mockito and Hamcrest  **Note #1:** The version of the Spring Boot Gradle-plugin decides the versions of the  dependencies.  **Note #2:** The  dependencies are very convenient to use but if they don't suite your specific needs you can easily replace them with your own. They are only a set of prepackaged "pom type" dependencies that Spring Boot puts together for you, for example see [spring-boot-starter-web v1.0.0].   First, if you want to reuse some of your existing XML configuration files you can simply load them using an  annotation or as the Spring Boot Reference Guide states in [§14.2 Importing XML configuration]:  > If you absolutely must use XML based configuration, we recommend that you still start with a  class. You can then use an additional  annotation to load XML configuration files.  No doubt on what direction the Spring Boot people wants you to go :-)  Now over to a sample Java based configuration. I want to be able to use this code example for some load testing and therefore I want to be able to configure the embedded Tomcat runtime environment, e.g. adjusting the max number of threads in the request thread pool. I can do that by defining a  - bean in my  - class:   The implementation looks like:   The  that use Tomcat specific API's to set the  property.  You can also see that we have avoided to hard code the new  value but instead used property injection. The value of the property  can be found in the file , standardised by Spring Boot. It looks like:   As you might already have guessed the other property server.port is another Spring Boot convention for setting the HTTP port for the embedded Servlet container.  Ok, now we are ready to see if this works!  Let's begin with building the war-file:   You will find the war-file in the folder :   Start the web app in an embedded Tomcat instance with the command:   It should result in something like:   Now try out the REST service with a command like:   Here we ask the REST service to process our request and respond in between 1 and 2 secs. The response comes from Tomcat  in a json-format. The json-response reports that the internal processing actually took 1374 ms.  Let’s try to ask for a response in XML format this time:   That worked pretty good as well.  Shall we try another web server?  Open the file  in a text editor and uncomment the dependency to . Restart the web-app and look for the following in the output from the startup:   Seems like SpringBoot deployed our web app in an embedded Jetty container, try it out and look for Jetty in the server-information:   Great!  Let’s wrap up the test run with a quick look at what the  dependency gives us out-of-the-box. First fire off a number of calls to the REST service and then give the command:   Ok, so we have made twelve calls to the service. No try out:   ...and we get some metrics regarding the response time of our service. Without writing a single line of code!  An impressive test run, right?  So far we have been doing everything on the command line, but you can of course import the project into a IDE that supports Gradle, e.g. Eclipse or IntelliJ. Below is a screen shot from my favorite IDE, IntelliJ, where I have Spring Boot running in one sub window and a Terminal in another sub window where I run my test commands:  ![]  Getting started with a Spring project, specifically when based on Java EE, has never been this easy! With Spring Boot you can get a new application up and running in a few minutes and, at least initially, disregarding all questions related to deployment descriptors and configuration files. When required you can take control over the conventions Spring Boot puts in play and refine parts of the configuration to meet your needs!  I can only encourage you to try this out in your next Spring project!
 One of the last days in my current project I was about to freeze the codebase and finish things up. Luckily, I had a couple of hours left to play around with something that I have been thinking about throughout the project but never had time to implement. My mission was to deploy the application to a remote tomcat container. The requirements I made up was:  - Maven-based  - Minimal configuration - Multiple servers  - Application server independent  After some research, I found Cargo that seemed to meet my requirements and I decided to try it out. I used the following setup to test Cargo:  - Apache Tomcat 7.0.26 - Maven 3.0.3 - example.war deployed in my local maven repository  First of all I created a new empty project called , the idea was to use this project to deploy my  to various servers:   Next, I wanted to be able to deploy to two different servers:  and . Since I used different ports, different protocols and different context paths for these two environment I created two Maven profiles that can be activated using the . In these two profiles I set property values for each environment.   It is now possible to activate the qa or prod profile by adding  or   I was surprised how easy it was to configure Cargo's maven plugin. There are basically three blocks of configuration 1) The container to use, 2) Configuration how to access the container and 3) the war-artifact to deploy to the remote container.  Since I do not want to exploit the username and password to the application server, I enter these manually during deploy time. Here is the configuration used:   The last thing we need to add to the configuration is to include a dependency to the war-artifcact that we want to deploy.   It is now possible to deploy to out qa or prod server. Note that if we don't want to expose the username and password in the script  we can force the user to specify them on the command line. We are now ready to deploy, execute the following command:   The example above does not work out of the box without some configuration of the application server. For example, in the Apache Tomcat case, you will need the manager web application to be deployed and configured to allow username/password access of a certain user. You can find all the details in the reference documentation as well as in the Apache Tomcat container documentation:  - [Cargo Reference Documentation] - [Cargo Tomcat Reference]
 One of the trickiest problems in distributed systems  is to understand what is going on and even more important what is going wrong, where and why. In this blog post we will see how we can configure our microservices to use correlation-ids to identify log-events that are related to each other.  -[readmore]-  [comment]: #  [blog series]: https://callistaenterprise.se/blogg/teknik/2015/05/20/blog-series-building-microservices/ [Logstash]: https://www.elastic.co/products/logstash [generating unique ids]: http://www.javapractices.com/topic/TopicAction.do?Id=56 [JavaDoc UUID.randomUUID]: http://docs.oracle.com/javase/8/docs/api/java/util/UUID.html#randomUUID-- [spring-cloud-sleuth]: http://cloud.spring.io/spring-cloud-sleuth/ [issue-39]: https://github.com/spring-cloud/spring-cloud-sleuth/issues/39 [spring-cloud]: http://projects.spring.io/spring-cloud/ [ThreadLocal]: http://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html [slf4j-MDC]: http://www.slf4j.org/manual.html#mdc [ClientHttpRequestInterceptor]: https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/http/client/ClientHttpRequestInterceptor.html [Servlet Filter]: http://www.oracle.com/technetwork/java/filters-137243.html [RestTemplate]: https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/web/client/RestTemplate.html [Netflix Hystrix]: https://github.com/Netflix/hystrix [HystrixConcurrencyStrategy]: https://netflix.github.io/Hystrix/javadoc/com/netflix/hystrix/strategy/concurrency/HystrixConcurrencyStrategy.html  [comment]: #  [system landscape]: /assets/blogg/build-microservices-part-5/log-events.png [implementation-1]: /assets/blogg/build-microservices-part-5/implementation-1.png [implementation-2]: /assets/blogg/build-microservices-part-5/implementation-2.png   Let's start with recapturing our small microservice landscape from our [blog series]:  ![system landscape][system landscape]  As the picture illustrates each microservice write log information to a local file.  > **OMFORMULERA:** Assume that we use some tooling like [Logstash] to collect the log files to a central place. Also assuming that there is a lot of concurrent activities going on in the system landscape it will be very hard to identify what log events that are related to each other, i.g. belonging to one and the same processing of a request like the one illustrated in the picture with the red line.  > **LÄGG TILL**: Google spec och begrepp, http://research.google.com/pubs/pub36356.html  To be able to find related log events we need to mark them with a id that is unique for each processing flow of such a request. We call this id a ***correlation-id***.   The logic for implementing a distributed correlation-id can look like:  **ASSUMING HTTP**   1. The first microservice in a processing flow that receives a new request  creates a unique id, and use it as its correlation id for every log message it writes during the processing of the request.  When the first microservice call other microservices  it adds the correlation-id to a HTTP header with a well known name.  2. All other microservices takes the id from the HTTP header mentioned above in the incoming requests and use it as its correlation-id, i.e. they use the correlation-id for logging and when calling other microservices in the same way as the first microservice did.  To simplify the logic a bit we can generalise the initial processing to:  * Look for the HTTP header in the incoming request. If found use it, otherwise create a unique id.  This means that we can provide one common implementation for handling correlation-id independent of whether the microservice happens to be the first in a processing flow or if it is called from another microservice that already has set the correlation-id in the HTTP request header.   First , let's look for an already existing implementation!  Since earlier this year it actually exists a [spring-cloud] project targeting our needs: [spring-cloud-sleuth]   [spring-cloud-sleuth] was recently released in v1.0, but has some shortcomings that prevents us from using it. Specifically, it lacks the capability of passing correlation ids through Hystrix, see: [issue #39][issue-39]. This is a capability that we unfortunately need for a fully working solution.  So, for the time being, we have to implement this our selfs :-(  But, as it turns out, is not that hard and can also be done in a fairly generic way almost transparent for developers of a microservice!  At a high level we need:  1. An interceptor for incoming requests, where we can pick up  the correlation-id or if not found create a new one.  1. A place where we can store the correlation-id during the processing.  1. A mechanism for automatically adding the correlation-id to log events, making it transparent to the developer of a microservice how the correlation-id is added to the log event.  1. An interceptor for outgoing requests, where we can add a header in the outgoing request for the currently used correlation-id    To meet these requirements we can use:  1. A [Servlet Filter] to intercept incoming requests  1. A ][JavaDoc UUID.randomUUID]. For more advanced ways to create unique ids see [generating unique ids][generating unique ids].  1. [Slf4j MDC][slf4j-MDC] to store the correlation-id  and automatically write the correlation-id to log-events  1. A configuration for the log framwork used  that writes the correlation-id in the log-messages  1. A Spring [ClientHttpRequestInterceptor] to intercept outgoing request submitted using Spring [RestTemplate].   It assumes that all processing for a specific request within a microservice is performed within one and the same thread. That assumption is fine in cases where we only use blocking I/O for outgoing requests and in no other ways change thread during the processing of a request.  Our code base is  based on blocking I/O so that should not be a problem.  But the circuit breaker, [Netflix Hystrix], use a thread-pool to be able to supervise the executution and optionally interrupt it due to detected problems, e.g. a timeout. So the request made by our composite service to the three core services are actually performed in a thread from a thread pool allocated by Hystrix.  **NOTE ON** not using thread pool in Hystrix, results in no timeout capability...  This means that the ThreadLocal variable, the MDC, is lost when Hystrix change thread for the processing of the request. If we loose the MDC we alos loose the correlation-id...  Fortunately this problem is relatively simple to fix. Using a [HystrixConcurrencyStrategy], Hystrix allow us to register a listener for when the thread switch happens and we can at that time transfer the MDC object to the allocated thread from the thread pool.   To summerize the implementation:  1. A Servlet filter is used to handle incoming HTTP requests 1. The correlation id is stored in the Slf4j MDC  1. LogBack is configured to always write the correlation id to log messages 1. The Hystrix Circuit Breaker is configured to transfer the MDC between threads used in the processing 1. An interceptor in RestTemplate sets the correlation id in outgoing HTTP requests  The first case with no correlation-id in the incoming request can be illustrated as:  ![implementation-1][implementation-1]  The second case with a correlation-id in the incoming request can be illustrated as::  ![implementation-2][implementation-2]   * build... * run... * observe...    The servlet filter is setup in the  project and the class  where we can find its declaration:  @Bean public Filter logFilter { LOG.debug; return logFilter; }  The  has the following parts of special interest :  private LambdaServletFilter logFilter =  -> {  ...  if  { corrId = UUID.randomUUID; LOG.debug; }  LOG.debug; MDC.put; MDC.put;  ...  try { chain.doFilter;  } finally { if  { LOG.debug("Remove from MDC: ", mdc_key_corrId, MDC.get, mdc_key_component, MDC.get; } MDC.remove; MDC.remove;  ... } };   All microservices now have a  - file in its folder . They all include a common configuration file found in . This file defines a common log format where the correlation id is taken from the MDC and is printed in each log message based on:  <property name="LOG_PATTERN" value="%d:%L - %m%n"/>    Two microservices use a Hystric Circuit Breaker, the  and the . The main-method in both services register a MDC-aware  like:  public static void main { ... HystrixPlugins.getInstance; ... }  The  - class is found in the util-project. Its only purpose is to register a wrapper class for calls to the Circuit Breaker:  public class MDCHystrixConcurrencyStrategy extends HystrixConcurrencyStrategy { @Override public <T> Callable<T> wrapCallable { return new MDCHystrixContextCallable<>; } }   Its the wrapper class  that makes the heavy lifting by injecting the MDC in the thread allocated by Hystrix:  public class MDCHystrixContextCallable<K> implements Callable {  private final Callable<K> actual; private final Map parentMDC;  public MDCHystrixContextCallable { this.actual = actual; this.parentMDC = MDC.getCopyOfContextMap; }  @Override public K call throws Exception { Map childMDC = MDC.getCopyOfContextMap;  try { MDC.setContextMap; return actual.call; } finally { MDC.setContextMap; } } }  The constructor is called by Hystrix in the parent thread before each processing and it copies the MDC. The call - method is called in the allocated thread and injects the parents MDC in the thread before it calls the actual implementation .  To initialize the machinery we also need to init and shutdown the Hystrix context. We use a SErvlet filter to do that. In the  project and the class  we can find:  @Bean public Filter hystrixFilter { return hystrixFilter; }  private LambdaServletFilter hystrixFilter =  -> {  HystrixRequestContext ctx = HystrixRequestContext.initializeContext; try { chain.doFilter; } finally { ctx.shutdown; } };  That completes the handling of passing the MDC between threads allocated by Hystix :-)   In the  project and the class  we can see how the RestTemplate is injected with a :  @Bean public RestTemplate restTemplateWithLogInterceptor { ... restTemplate.getInterceptors; return restTemplate; }  The  sets the correlation id header in the outgoing HTTP request as:  private ClientHttpRequestInterceptor logInterceptor =  -> {  String corrId = MDC.get; LOG.debug; HttpHeaders headers = request.getHeaders; headers.add; return execution.execute; }; 
In this part, we'll implement the Database-per-tenant pattern using Hibernate out-of-the-box support for Multi Tenancy, with Database Migrations using Liquibase and support for dynamically adding new tenants.  -[readmore]-  [comment]: #  [Multi Tenancy]: https://whatis.techtarget.com/definition/multi-tenancy [SAAS]: https://en.wikipedia.org/wiki/Software_as_a_service [Spring Framework]: https://spring.io/projects/spring-framework [Spring Boot]: https://spring.io/projects/spring-boot [Spring Data]: https://spring.io/projects/spring-data [Hibernate]: https://hibernate.org/orm/ [JPA]: https://en.wikipedia.org/wiki/Jakarta_Persistence [experimental support for the shared-database-using-discriminator-column pattern]: https://hibernate.atlassian.net/browse/HHH-6054 [Liquibase]: https://www.liquibase.org/ [Database Schema Migration]: https://en.wikipedia.org/wiki/Schema_migration [Flyway]: https://flywaydb.org/ [Cross Cutting Concern]: https://en.wikipedia.org/wiki/Cross-cutting_concern [Aspect]: https://en.wikipedia.org/wiki/Aspect-oriented_software_development [Reversibility]: https://martinfowler.com/articles/designDead.html#Reversibility [Github repository]: https://github.com/callistaenterprise/blog-multitenancy [database branch]: https://github.com/callistaenterprise/blog-multitenancy/tree/database  [comment]: #  [neighbours]: /assets/blogg/multi-tenancy-with-spring-boot/undraw_neighbors_ciwb.png [Database Plan]: /assets/blogg/multi-tenancy-with-spring-boot/database-plan.jpg  - [Part 1: What is Multi Tenancy] - [Part 2: Outlining an Implementation Strategy for Multi Tenant Data Access] - Part 3: Implementing the Database per Tenant pattern  - [Part 4: Implementing the Schema per Tenant pattern] - [Part 5: Implementing the Shared database with Discriminator Column pattern using Hibernate Filters] - [Part 6: Implementing the Shared database with Discriminator Column pattern using Postgres Row Level Security] - Part 7: Summing up   A fully working, minimalistic example for this part can be found in the [Github repository] in the [database branch].   [Hibernate] provides out-of-the-box support for the two first multi-tenancy patterns , and [experimental support for the shared-database-using-discriminator-column pattern]. The built-in support is activated by configuring an Hibernate Entity Manager with the desired  and inject suitable implementations of the  and  interfaces.   Since these properties needs to be set when the Entity Manager is created, we need to override the default EntityManger configuration provided by Spring Boot with an explicit configuration.   The  encapsulates a strategy for resolving which tenant to use for a specific request, whereas the `MultiTenantConnectionProvider ` encapsulates a strategy for selecting an appropriate database connection for that tenant. From [the last episode], we already have a transparent mechanism for retrieving the Current Tenant. Let's just package that mechanism up as an Hibernate-specific implementation:     The  responsibility is to provide tenant-aware JDBC connections.   As we can see, we need a 'master' dataSource for Hibernate to query for database Metadata during startup, and separate 'tenant' dataSources for each tenant. Since we must be able to add new tenants dynamically, adding new dataSources for new tenants must be dynamic as well. The general idea is to use a *master repository* for managing information about each tenant .  Let's start by defining a master datasource:   Next, we define a JPA entity to represent meta data about a Tenant:   A Spring Data Repository allows us to query for tenant information, given a tenantId:   We surely don't want to store passwords in plain text for tenants, so let's assume a simple encryption service to at least store encrypted passwords .   We are now ready to implement the  interface. Since we will need a separate dataSource per tenant, we store the dataSources in a LoadingCache which creates a new dataSource for a tenant on first access and evicts and closes dataSources for tenants which hasn't been active for a while.    We now need to configure two entityManagers: One master entityManager to provide meta data for the tables and to host the tenant repository, and a separate entityManager to serve the tenant-specific databases. The entityManagers need their own transaction managers as well.  We start with the master entityManager:   This configuration is very similar to the Spring Boot auto-configuration. Since we need dual entityManagers, we still have to configure it explicitly.  We do the same for the tenant entityManager, but this time we configure the Hibernate multi-tenancy properties:   Since we mark the tenantEntityManagerFactory and tenantTransactionManager as , they will be used by default in any component that autowires a PersistentContext or EntityManager.  We have externalized most of the configuration into properties, which we define in application.yml:    Next step is to create a mechanism to onboard new Tenants, by creating the Database and User to use for the new Tenant. We do this using raw SQL which is database vendor specific, since there is no standardized way to do this. The example below contains SQL for PostgreSQL, you may need to tweek it to work with another database:   This will create a new Database and Database User with the same name as the TenantId. We will also need to create the database tables for the newly created tenant, using a Liquibase migration.   We'll wrap this up in a :   The process for onboarding new tenants will likely differ from case to case. Since there is an upper limit on the scalability when using a database per tenant, the number of tenants must be reasonably small. Hence there is most likely some administrative procedure in place before onboarding a new tenant. Let's for simplicity add a simple, administrative REST endpoint to create new tenants.   Let's also for completeness add a simplistic encryption implementation, to encrypt the tenant passwords.    The last piece required is a mechanism to extend Liquibase-based Database Migrations to apply the migrations not only to the Master database  but to each tenant's database as well. By default in Spring Boot, if liquibase is enabled, a database migration is executed on application startup, if needed. We extend this to include the tenant databases as well.  We'll start with the liquibase config for the master database:   This is again more or less identical to to the Spring Boot auto-configuration, but since we need one config for the master database and a separate config for the tenant databases, we need to configure it explicitly.  Let's continue with the ctenant database migrations. We'll need to query the TenantRepository for all tenants, and run a migration on each of them:   Finally, we just need to wire up the config:   The liquibase configuration is externalized into application.yml:    We now have a dynamic implementation of the Database-per-tenant Multi Tenancy pattern!  A fully working, minimalistic example can be found in the [Github repository] in the [database branch].   The Database-per-tenant pattern provides strong data separation between tenants, but has an obvious upper limit on how many tenants it can cater for: Each tenant requires a separate dataSource and corresponding separate database connections, hence it won't scale beyond say maybe a hundred tenants.  In the [next part], we'll tweak the solution to implement the Schema-per-tenant pattern, still using Hibernate's out-of-the-box support.   The following links have been very useful inspiration when preparing this material:  [www.bytefish.de/blog/spring_boot_multitenancy.html]  [sunitkatkar.blogspot.com/2018/05/adding-tenants-without-application.html]
 Integrating the Eclipse environment with Maven has always been a challenge, as we have reported upon [before]. I want the productivity of the Eclipse IDE **and** the expressive power, consistency and repeatability of Maven. But a fundamental difference in the underlying paradigms of Eclipse versus Maven have made that coexistence awkward and ugly:  - Eclipse assumes all dependent jar files for a project are explicitly listed within the project , whereas Maven relies on dependencies being defined in  files and resolved at build time using an underlying repository model  Furthermore, there is a difference in expressiveness, where several fundamental capabilities in Maven have no Eclipse counterparts:  - Eclipse assumes all projects lives within a flat structure, with no hierarchical dependencies between projects, whereas Maven allows hierarchical composition of projects and builds - Eclipse assumes projects are always built the same way, whereas Maven provides a sophisticated Build Lifecycle model - Eclipse assumes a project have one classpath, whereas Maven provides different classpath scopes  - etc.  There are two principal ways to try to bridge the gap and allow the different paradigms to coexist:  - Either plug in Maven knowledge into Eclipse, to allow for Eclipse projects to access dependencies via Maven pom.xml files and execute Maven build lifecycle stages inside Eclipse, or - Plug in Eclipse knowledge into Maven, to allow Maven to generate the necessary Eclipse project files to reflect the information in the   The first approach is the most appealing, but the Maven plugins to Eclipse have for several years been too immature, unstable and error-prone to be of any use. Instead we have resorted to the second approach, where the eclipse plugin to Maven have been capable of generating rudimentary Eclipse project files. Together with the use of Eclipse external build tool definitions, it has been possible to define a semi-automated process that is a bit awkward but good enough.  I was therefore delighted to read that [Sonatype], a new commercial company started by the Maven engineers and specializing in Maven support, has finally brought the m2eclipse plugin to a near-stable state as an Eclipse Technology project. The inconsistencies and flaws from past years are removed. The plugin is The Maven Dependencies Classpath Container efficiently adds dependencies specified in the Maven pom.xml file, and reliably and immedialtely reflects changes done to the  file both from within and outside Eclipse.  The plugin also a dedicated Maven  file editor and wizards/dialogs for easy Maven dependency management. Having tested the plugin for a week now in a project, I think it is really delivers what Sonatype promises: The "lynchpin" between Eclipse and Maven.
 [Soi-toolkit].  First a short introduction of the two tools and then an illustrated test run...  -[readmore]-  Soi-toolkit is an open source project initiated by Callista together with a number of its customers as a common place to share proven best practices for developing services and integrations based on Mule ESB.  Soi-toolkit can give new Mule ESB users a kick-start by providing answers to a number of classic getting-started questions, i.e. questions that needs to be answered before the actual development of services and integrations in Mule ESB can take place. Typical questions are:  - How to setup a minimalistic but sufficient development environment? - How to setup projects in a good way, file-structures, naming conventions, dependency management ? - How to handle logging, error handling  and property based configuration? - How to test, build, release and deploy the services and integrations?  In short soi-toolkit is taking care of all the boring parts and allowing the developer to focus on constructing the services and integrations in Mule ESB.  Soi-toolkit does this by a set of customizable source code generators that given a small set of input parameters can create both an initial setup of the projects and skeleton code for services and integrations following a set of predefined high-level patterns.  Example:  ![]  Soi-toolkit not only generates the Mule ESB configuration xml files but also sample transformers, test-classes , test-producers/consumers, log-settings and property files with appropriate properties for the generated code.  The generated code is immediately runnable  to meet the requirements of the specific project.  For more information of soi-toolkit the following links are recommended:  1. [soi-toolkit homepage] 2. [soi-toolkit overview] 3. [soi-toolkit tutorials] 4. [Callista-blog on soi-toolkit]  According to the [Mule Studio documentation]:  > Mule Studio is a user-friendly and powerful Eclipse-based tool that allows you to easily create Mule ESB flows, edit and test them quickly without a deep knowledge of Mule configuration.  Example:  ![]  That’s perfect from a soi-toolkt perspective since Mule Studio can take over exactly from the spot where soi-toolkit leaves the developer, i.e. refining the Mule ESB configuration files. With Mule Studio the developer can do this graphically without touching the underlying XML. If the developer wants he can at any time switch over to the XML and refine it by hand, i.e. the editor is two-way allowing the graphical view to be updated by manual changes in the XML.  For more information of Mule Studio the following links are recommended:  1. [MuleSoft blogs on Mule Studio] 2. [Mule Studio documentation] 3. [Mule Studio beta program]  Thit looks very promising, right?  Lets take it out for a spin!   **Note:** Mule Studio is currently in beta, i.e. not feature complete, and soi-toolkit’s current release, v0.4.1, is not Mule Studio aware. So the test run below is based on soi-toolkit code from trunk, far from complete when is comes to Mule Studio awareness. But the test run below clearly demonstrates what it will look like once Mule Studio and the new version of soi-toolkit are released.  Let’s start with an empty newly created project. It looks like:  ![]  Let’s create an integration that reads messages from one JMS queue, transforms the message and place it on another JMS queue.  We use the soi-toolkit generator for this and launch it using a soi-toolkit Eclipse-plugin from within Mule Studio:  ![]  We specify a one-way pattern as the base and JMS for both incoming and outgoing transport and finally name the integration  .  This results in a number of new source code files:  ![]  The most important files are:  - The Mule service configuration: -  -  - The transformer and its unit-test -  -  - Integration test and test-receiver -  -  -   Double-clicking on the  – file  opens up Mule Studio’s graphical editor:  ![]  Double-clicking on the elements in the graphical editor opens a property editor where the generated properties can be viewed and if required modified, e.g. for the inbound-jms-endpoint:  ![]  So we can for example see that the queue-name is generated as a configurable property available in the standard soi-toolkit property file, :  [![]  Finally clicking on the **Configuration XML** – tab shows the corresponding XML configuration generated by soi-toolkit.  [![]  Note the generated support for transaction handling, error handling and log-points.  There is more code of interest to go through, e.g. the generated Java transformer, the unit test and integration test classes and so on but to not make this blog endless I stop here for now.  Instead let’s try running the generated code!  First we simply run the unit test on the transformer and the integration tests of the whole integration. The integration test will do the following:  1. Start Mule embedded in the test that in turn start ActiveMQ  embedded as well. 2. Sending test messages to the in-jms-queue and wait for the asynchronous delivery of the outgoing message to the receiving teststub-component. 3. Compare the received message with an expected result and check log-queues and deadletter queue for expected results verifying correct error and retry - handling. 4. Both happy days and negative scenarios are covered by the generated integration tests.  The result of running the tests look like:  ![]  Now let’s start the Mule ESB server so that we can send messages to it manually.  We use the soi-toolkit generated Mule ESB Server, , it will by default start both the service but also its teststub-service that consumes the outgoing message .  ![]  Next we place a message  and sends it to the out-queue where the teststub consume the message and writes it out to the console.  ![]  Note the standardized .  Not bad for a few clicks in a wizard, right?  ...and a very good start for test driven development of integrations and services using soi-toolkit and Mule Studio!
 In fast evolving projects you are most certain to find some kind of technical debt. As architects and developers we are always looking for the best mix of flexibiltiy, clarity and maintainability in our code base. To maintain the quality of the code most project have a set of coding standards and architectural guidelines which the team should be aware of and follow. Nevertheless I've noticed that, and I'm sure most of you have seen or experienced this, when projects are working under hard time pressure these things tend to be put aside in favor of making the delivery on time.  In the best case this leads to post delivery activities in terms of cleaning up and documenting the code but my observations tell me that most often the code is left as it is, making it harder to maintain. In my opinion it is desirable to be able to coop with such problems at a much earlier stage.  Wouldn’t it be great to have a tool for helping you find and pinpoint potential problems early on, even integrated as a part of your continuous builds and also directly from inside your IDE?  Sonar is a software that analyzes your code base and displays parameters related to technical debt. Primarily it is built for Java projects but through the use of plugins it is also possible to analyze other languages as well. The software is very well suited for projects built with Maven but there are also ways of analyzing other types of projects.  If you are familiar with tools like Checkstyle you should rather quickly grasp the value of the tools that Sonar brings in terms of managing your code base and keeping it neat and clean.  The concept is that Sonar runs as a server application that collects data from your code through a Maven plugin. The analyzed data can then easily be viewed in the web GUI included with the server application or through other tools such as the Eclipse plugin. The Eclipse plugin integrates great with the IDE and lets the developer jump between the analysis and the code instantly which eases the work of correcting problems pointed out in the analysis.  The default setup covers the most common metrics of technical debt, such as coding standard alignment, code coverage of tests and a lot more. It is also possible to add plugins that measures additional parameters of the code. The Sonar site reports that there currently are more than 30 plugins available in addition to the default behavior.  Sonar currently integrates with the following continuous integration engines, which makes it easy to keep your analysis up to date down to each code change:  - Bamboo - Hudson - Continuum 1.2 - Cruise Control - TeamCity  For more information on setting up Sonar with your continuous integration engine read here: [http://docs.codehaus.org/display/SONAR/Collect+data#Collectdata-Continuousintegrationengines]  The Eclipse plugin is available at the following update site: http://dist.sonar-ide.codehaus.org/eclipse/  In Eclipse:   - Open the **Help** Menu and click on **Install New Software...** - In the field **Work with**, enter:  - Check the **Sonar integration for Eclipse** and follow the installation wizard.  After the installation, Eclipse is restarted and a new Sonar perspective is added to the Eclipse and you can associate projects with Sonar.  More information about the Eclipse plugin is available here: [http://docs.codehaus.org/display/SONAR/Sonar+Eclipse]  In this section I will explain a really quick way to get going with the Maven project of your own choice.  Download the latest version from [http://www.sonarsource.org/downloads/]  For this tutorial I use Sonar version 2.4.1  Make sure you are using the following:  - JDK version 1.5 or higher  - Maven of version 2.0.9 or higher   Sonar persists the collected analysis data to a database and there are built in support for most of the major db vendors. Fortunately we don’t have to bother about that choice for now since Sonar ships with a default configuration which uses an embedded Derby server which will work just perfect for the needs in this article.   1. Unpack the sonar zip to a folder of your choice  2. Enter the newly unpacked folder and run:  On windows   On other systems   Sonar requires the project to be built at least once before it can do the analysis with the plugin. So before starting the plugin that is performing the actual analysis we do a clean build. Since the plugin  runs all tests during the analysis we first run the clean build with tests disabled and then the tests will run when the plugin is activated.  Enter the folder of the Maven project you want to analyze and run the following:   Followed by the actual analysis, which also will run the tests:   Open a web browser and enter [http://localhost:9000] in the URL field, click on your project and enjoy your metrics.  Information in this article were found on [http://www.sonarsource.org/] where you also can find more information about Sonar.
 Speed is the key. I often need a web server in order to run a web application I developed to try things out. Setting up this infrastructure can often be quite tedious but if the only thing you need is a servlet container I often use the approach described in this article. We start out with nothing except Maven and Java installed.  -[readmore]-  Create a web application project:   This gives us a new directory  which is a Maven web application project. To run the web application, we configure the maven-jetty-plugin. Add the following configuration to project's .   Enter the  directory and do:   As soon as the server is started you can enter the following url in your browser.   As you can see, the server is started and listens on port 8080 by default. If you want to change this, it can easily be configured. Just extend the plugin with a configuration element and add a connector.   Assume you want to communicate in a secure way. The only thing you need to do is to add another connector element and specify a keystore containing the server's certificate. If you don't know how to create a certificate for your server, you can read my other blog post [Creating self-signed certificates for use on Android]. Simply add the following connector element and make sure the  is located in your  directory:   You can test this in a nice way using openssl to see what the server returns when you try to access it on port 9443.   Finally, if you for some reason want mutual authentication, you also need to specify a trust store in which the server keeps certificates of trusted clients. Extend the previous connector with the following information:   Now you have a web server up and running your web application with mutual authentication. The clients must provide a valid certificate in order to communicate with the server. At last I just want to add a final element to our configuration. Since TLS/SSL can be quite horrible to troubleshoot, I add the following configuration which gives a lot of nice output :)   Have fun!  * [Maven Jetty Plugin] * [Maven Webapp Archetype]
 This is the first part of a two-part series about [Testcontainers]. In this first part I will explain what it is, what problems it tries to solve, how it works and finally how you can use it in your own projects. In the second part we will see if we can reduce startup time for our Testcontainers.  -[readmore]-   * This line will be replaced with the toc below   Earlier this spring I held a skills meeting for my colleagues that was about [JUnit 5].  The full source code for my example project can be found here: [https://github.com/merikan/testcontainers-demo)] and it has two directories named  and .  So let's start with the obvious question; what is Testcontainers and what are the problems it is trying to solve? Or is it just another cool tool in our development stack?  On Testcontainers's home page you can read the following; > Testcontainers is a Java library that supports JUnit tests, providing lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in a Docker container.  How cool isn't that? This means that if the software can run inside a docker container we can use it in our tests.  Testcontainers was started as an open source project for the Java platform  and a few more. These are projects of varying quality but many of them can be used without problems in your applications.   _A personal note; After using Testcontainers a couple of years, for both my Java and Go projects, I do think they should restructure the Testcontainers main site and embrace the other projects and let them become first class citizens. Projects not ready for prime time could be incubators._    So back to the question that we should ask; What are the problems that Testcontainers could help us solve?  If we first take a look at different test strategies and different layers, usually explained as a [test-pyramide].  ![Test Pyramide]   At the bottom we have unit tests. Must of us can agree on what a [unit test].  If we look above the Unit test layer in the test-pyramid we will find some type of integration test which means that we will have more than one component and sometimes we need to integrate with an external resource, a.k.a [backing service]. A backing service is any service that our application or module consumes over the network and it is now Testcontainers shines. Before we look at Testcontainers, let's first discuss some of the problems we've had with external resources.  Over the years we have tried to solve the problem of interacting with external resources in different ways, depending on the type of backing service. To make these resources available it's a balance between cost  and the degree of automation and ease of setting them up and maintaining them. Then several issues arise such as; should we use a shared resource on a remote server, or a locally installed process or a embedded in-memory service? Each solution to our first problem has its own problems and challenges, below you can see some of these challenges.  **A shared resource on a remote server;** * is usually expensive as it requires one or more extra servers and the environment must be maintained. * it is shared by several people and maybe different projects which requires quite a lot of administration * you will probably need to provide the service in several versions which makes it more cumbersome  **Locally installed resource;** * can be cumbersome to install and maintain. Of course, we can use virtual servers locally to facilitate this work. * depending on the license, it can be expensive  **Embedded and In-Memory;** * usually more lightweight than other solutions but is only available for a few types * not the same codebase as the real resource, it only emulates the real target environment * we are not testing with real connections over the wire * not configured as target environment, security, cluster etc.  Each strategy has it's own pros and cons and today, embedded resource is probably the most widely used method. It solves many of the previous problems but unfortunately it introduces some new. Some problems that I have encountered from time to time are: It only *emulates* the target resource , usually with subtle differences, and the security settings are typically different from the target environment. We will of course find these differences in the end, but the earlier in the development process the better.   One great solution to many of these problems is [Testcontainers]. With Testcontainers we can start and stop one or more docker containers with the same resource as we use in our production environment, with a configuration as similar as possible. Later on I will show how easy it is to use Testcontainers in a Spring Boot application with Junit 5, but it can of course be used in any Java application.  Not everyone agrees that you should start up resources locally, but it usually depends on what conditions you have in your project. If your system landscape is based on microservice architecture and perhaps consists of hundreds of microservices where some are needed as a backing service, it is probably not conceivable to try to start these locally. However, most of us are not in that situation with so many microservices that form a system landscape, so for us Testcontainers probably works very well. If you are more interested in this topic, [Cindy Sridharan], which I really appreciate and find interesting.   Now we now that Testcontainers is a open source project that we can use in our tests to start and stop anything that can run in a docker container. Let's delve deeper into the subject.    ![modules]   Testontainers can be of two types: [GenericContainer]. GenericContainer is used as basis to create your own Testcontainer of any docker container. A module is a specialized container that is designed for a specific product such as MariaDb, Kafka etc. and we can see them as specialized Testcontainers with extended convenience methods and functionality. One module that differs from the others is DockerCompose, which allows you to start several containers at the same time and of the type you want.   All we need to do in our tests is to declare that we want to use one or more Testcontainers, of a certain type. The Testcontainers framework will download the image  and handle its lifecycle during the tests by starting the container, make sure it is ready and then stop the container after all tests have finished. This is done by annotating the test class with .   The next step is to add the  annotation to specify which Testcontainer we want to use and to be started by the Testcontainer framework. We have two options here; containers declared as static fields will be shared between test methods and containers declared as instance fields will be started and stopped for every test method. Which strategy you want to use depends on how independent your tests are, you must handle any side effects that are caused between the tests.  To make sure we have reproducible builds we should never rely on , instead we should always declare what version we want to use, such in the example below:  This is one way to start Testcontainers but later on I will show you another and, in my opinion, better way to start them.  Our containers are started with dynamic port numbers and some other default values. You can of course set these and other values yourself when you start a Testcontainer. You can then easily retrieve information about the started container through various methods such as:   When all the tests have finished our little friend, named [Ryuk], will make sure that our Testcontainers are taken down. Ryuk is started as a container at the same time as your other Testcontainers are started.   Earlier I said that there was a better way to start our Testcontainers instead of using the  annotation.  Using the  annotation is an easy way to add a Testcontainer but the problem is that your Testcontainers will be started and stopped for each test class. A better way is to use the singleton pattern and use an abstract class to start our Testcontainers. The singleton container is started only once when the base class is loaded and the container can then be reused by all inheriting test classes. At the end of the test suite the Ryuk container, that is started by Testcontainers core, will take care of stopping the singleton container.  Keep in mind that you have to deal with any side effects caused by multiple tests using the same resource. For example, by making your tests unaffected by the state of the resource or by setting the resource in the desired state for each test.  This is what the abstract class looks like to be used to start Testcontainers:   Each integration test class should then inherit from this abstract class to make use of Testcontainers.   If we now run our tests, we can see that our Testcontainers are only started once, which most likely means that we have shortened the time to run our test suites significantly.    In the following example I will be using Spring Boot, Junit 5 and Maven. You can find the source code on Github  which represents before and after our changes.  The first thing we need to do is to turn off any embedded resource started by Spring Boot and instead use Testcontainers.  Remove  as an embedded resource so that Spring Boot does not attempt to start it.   and add Testcontainers instead. Our project uses  so let's add MariaDb as a Testcontainer. Since we are using Junit 5 we will also add the  module for Testcontainers. First we will import Testcontainers BOM and then add our dependencies.  _Note: We are using version 1.15.0-rc2 of Testcontainers due to issues with Docker for Mac 2.4.0.0 in earlier versions._   Next we will create an abstract class to be used by all integration test classes.   Now it is time to make sure our integration test classes inherits from .   As you can see, we do not need to use the  annotation in our test classes as it is already declared in our abstract class. We can of course still use the annotation in our test class as desired to overlay certain functionality.   This is all we have to do to get started with Testcontainers in our Spring Boot projects. As you can see it's pretty straightforward and very easy to do. If you want to run the tests yourself you can do the following, assuming you have Git and Java already installed.   If, after running your tests, you are fast and run the `docker ps` command you can see that your containers might still be running together with the Ryuk container but after a short while they are all stopped.  This is not a Testcontainer problem but rather a side effect of the changes we did. Since we removed the support for embedded H2 database, which was used when we also ran the application locally with Maven, we now want to use a MariaDb instead and start it as a docker container. I have added a  file we can use to start MariaDb locally. When we then start our application with the , the spring profile named  will be activated, see property file .  These are the steps you have to take; start MariaDB, start the application, ctrl+c to stop the application and then stop the MariaDb container. Don't forget to give MariaDb some time to get started before running the application.   If your start a new terminal while the application is running you can curl to the endpoint like this.    In this blog post, I have explained what the [Testcontainers] framework is and what problems it tries to solve. I have also briefly explained how it works and how you can easily get started using it in your own projects.   In my next blog post, I will show a little more advanced usage of Testcontainers to, among other things, speed up your tests.  And finally; I'm just a human being, if you find any ambiguities, typos, errors or possible improvement, please let me know. If you like this blog post, please add a comment and feel free to share this blog post using your favorite social media platform! There should be a few icons below to get you started.  Until next time, keep coding and never stop learning new things.  * Testcontainers [home page] * The full source code for my example project can be found here: [https://github.com/merikan/testcontainers-demo)]
In the [last part], we implemented the Schema-per-tenant pattern, and observed that it will scale better than the Database-per-tenant implementation. There will still most likely be an upper limit on the number of tenants it supports, caused by the Database Migrations that has to be applied to each tenant.  In this part, we will redo the solution and implement the Shared database with Discriminator Column pattern using Hibernate Filters and some AspectJ magic.  -[readmore]-  [comment]: #  [Multi Tenancy]: https://whatis.techtarget.com/definition/multi-tenancy [SAAS]: https://en.wikipedia.org/wiki/Software_as_a_service [Spring Framework]: https://spring.io/projects/spring-framework [Spring Boot]: https://spring.io/projects/spring-boot [Spring Data]: https://spring.io/projects/spring-data [Hibernate]: https://hibernate.org/orm/ [JPA]: https://en.wikipedia.org/wiki/Jakarta_Persistence [Cross Cutting Concern]: https://en.wikipedia.org/wiki/Cross-cutting_concern [Aspect]: https://en.wikipedia.org/wiki/Aspect-oriented_software_development [AspectJ]: https://www.eclipse.org/aspectj/ [JIRA issue for discriminator-based multi-tenancy]: https://hibernate.atlassian.net/browse/HHH-6054 [invesdwin-instrument]: https://github.com/subes/invesdwin-instrument [Github repository]: https://github.com/callistaenterprise/blog-multitenancy [shared_database_hibernate branch]: https://github.com/callistaenterprise/blog-multitenancy/tree/shared_database_hibernate [multi-tenant-app-demo]: https://github.com/ramsrib/multi-tenant-app-demo  [comment]: #  [neighbours]: /assets/blogg/multi-tenancy-with-spring-boot/undraw_neighbors_ciwb.png [SingleDatabase]: /assets/blogg/multi-tenancy-with-spring-boot/SingleDatabaseMultiTenancy.png  - [Part 1: What is Multi Tenancy] - [Part 2: Outlining an Implementation Strategy for Multi Tenant Data Access] - [Part 3: Implementing the Database per Tenant pattern] - [Part 4: Implementing the Schema per Tenant pattern] - Part 5: Implementing the Shared database with Discriminator Column pattern using Hibernate Filters  - [Part 6: Implementing the Shared database with Discriminator Column pattern using Postgres Row Level Security] - Part 7: Summing up    The Database per Tenant and Schema per Tenant patterns provide a clean separation of data between tenants, but at the price of duplicating the database table definitions for each tenant. As we observed in the last part, this may cause scalability problems, since every Database Migration needed must be applied for every tenant. If Database Migrations are applied automatically on application startup , a large number of tenants will lead to long startup time.  In the Shared database with Discriminator Column pattern, this problem no longer exists. Placing the data for all tenants in one single database, we only have one single set of database to manage.  ![SingleDatabase][SingleDatabase]  In order to separate data between different tenants, we use a *Discriminator Column* in every table to hold the tenant information for each row in the table. We would hence need to populate the discriminator column with correct tenant information every time we store data, and we need to include the discriminator column as an extra condition every time we query for data. That is clearly a [Cross Cutting Concern] that we would like to capture in one single place. The data isolation guarantee between tenants  relies on us being able to prove that the discriminator column is properly used for all database access!  So let's implement the Shared Database with Discriminator Column pattern using Hibernate Filters!   Although Hibernate's  enumeration contains a  entry, this pattern is not yet supported in Hibernate as of version 5.4.x. It was scheduled for version 5, but never made it. There is an open [JIRA issue for discriminator-based multi-tenancy], but with no real progress since 2017. The JIRA issue however identifies the existing Hibernate mechanisms available roll our own implementation.  As we observed above, there are two capabilities needed:  * Population of the discriminator column of all entities with the correct tenant information when saving entities, and * Adding a  condition on all queries for entities  Luckily, there are existing mechanisms available for both these capabilities: Standard JPA  and Hibernate specific .   The standard JPA EntityListener mechanism allows a listener to be attached to the lifecycle of a JPA entity. It allows us to to populate the Discriminator column with the current tenant. Given an interface  that all entities implement, the following Listener will do the trick:    The standard Hibernate  mechanism allows us to define a Filter containing a clause that can be applied to all queries for entities upon which the Filter is attached:    We are now prepared to encapsulate the usage of a discriminator column, an EntityListener and a Filter as an abstract base class for our Entities:   All entities will need to extend  in order to have the multitenancy support applied, as for example:    That was a neat and self-contained mechanism! However, there is still one piece missing: Unfortunately, a Filter defined on an entity doesn't get automatically applied, it is only available to be applied. When a query is issued, the underlying Hibernate Session needs to be explicitly configured to use the filter. Since the Session object is created dynamically at runtime , we cannot apply the Filter once and for all at application startup. Instead we need an additional mechanism: an [Aspect].   [AspectJ] provides a mechanism to defined fine-grained *execution points* and intercept the execution at those points to inject additional behaviour. This is exactly what we need: A way to intercept the creation of a Hibernate Session, to make sure that our Filter is properly applied to every created Session. Note that we cannot to that with the light-weight built in Aspect functionality in Spring, since that mechanism can only be used for Spring-managed beans. The Hibernate Session object is not managed by Spring, and hence we need the full-fledged AspectJ support.  In order to do its magic , or at load-time using *load-time weaving*. The latter approach is less intrusive, and hence to be preferred in our case.  Configuring the AspectJ Load-Time Weaver is done using an  file in the classpath:   This configuration defines an aspect  and the classes to which it should apply . Note that the Aspect class itself must be part of the weaver classes, for technical reasons.  The  is reasonably straight-forward:   It defines an execution point , it injects the required setup to apply the Hibernate Filter.   Getting AspectJ load-time weaving to work in Spring Boot can be a bit complex, since the documentation is slightly misleading. First thing, we need the AspectJ weaver and Spring Boot aspect support in the classpath which is done easiest using a Spring Boot starter dependency:   Next step is to enable the AspectJ load time weaver, using the  annotation:   Finally, we need to use both Spring's instrumentation agent and AspectJ's aspectjweaver agent to be passed as -javaagent JVM arguments. The configuration of java agents will differ depending on deployment scenario. Using the Maven spring-boot plugin, the following configuration will to the work:   while running the application on the command line would look like this:    The above implementation is simple and self-contained. The systematic usage of EntityListeners and Hibernate Filters applied via an Aspect seems fairly robust. It will guarantee that each tenant's data is totally isolated from other tenants , won't it?  Unfortunately, there is one subtle leak: Hibernate's Filter mechanism is designed to apply to all Hibernate *queries*, but not to *direct fetching* via the Session object ` under the hood, and therefore will not be affected by the filter. Hence will allow fetching entities that belong to other tenants!  Fixing this problem is indeed easy, just override the findById with a proper JPQL query:   Simple, yes, but the problem is you have to know it must be done for each and every Repository used!   We now have a straight-forward implementation of the Shared Database with Discriminator Column pattern. Since we now use one single Database, the need for specific on-boarding logic and Migrations for tenants disappeared, as did most of the configuration. The use of AspectJ and load-time weaving is however a thing that not everyone may feel comfortable with.  A fully working, minimalistic example can be found in the [Github repository] for this blog series, in the [shared_database_hibernate branch].   The Shared Database with Discriminator Column pattern implementation overcomes the scalability issues we identified with the previous implementations. Hence we can assume this implementation will no practical limitation on the number of tenants .  The data separation guarantee between tenants however now becomes a challenge. The implementation is based on several cooperating mechanisms which may have leaks of their own or in combination with the other mechanisms. The burden of proof lies on us that there are no leaks.  In the [next part], we'll instead implement the critical Filter part of the solution using an advanced database mechanism: Row Level Security. Stay tuned!   The following links have been very useful inspiration when preparing this material:  [medium.com/@vivareddy/muti-tenant-with-discriminator-column-hibernate-implementation-a363f03b1d10]  [github.com/ramsrib/multi-tenant-app-demo]  [www.credera.com/insights/aspect-oriented-programming-in-spring-boot-part-3-setting-up-aspectj-load-time-weaving/]
Finally, it seems like Ed Burns, spec lead of JSF 2.0, has started to listen to the community .  For several years, there has been a massive critique against using JSP as rendering engine for JSF . The Facelets project provides an excellent alternative to JSP rendering, a both productive, elegant and clean solution. The only problem has been that Facelets is not standardized. On the contrary, the Facelets project has looked like more or less a one man show.  Now things are changing. The Facelets creator Jacob Hookom has joined the JSF expert group and been given the responsibility for the initial design work of what Ed Burns now calls the "top priority issue for JSF 2.0". In the Early Draft Review Goals in the issue tracker for the spec, one of 5 goals is to provide a "Page description language ".  This is great news! We have been holding back JSF usage in several projects, due to the inherent problems with JSF and JSP, in combination with the uncertain status and future of the Facelets project. With Facelets  becoming a part of the JSF 2.0 spec, that obstacle is removed. We hence seriously considering JSF with Facelets in an upcoming customer project.
 This is the second part of a two part series about [Testcontainers]. In this part I will among other things show you some tricks to get your Testcontainers start much faster.  -[readmore]-   * This line will be replaced with the    In the previous blog post, [Getting Started with Testcontainers] framework is and how to get started using it in your project. After using Testcontainers for a long time I learned that there are things that need to be adjusted to make it even better and faster. Below I will show some of the things that make a fantastic framework even better.  The full source code for my example project can be found here: [https://github.com/merikan/testcontainers-demo)] and the folder .    I described this in my [previous blog post] but it is well worth repeating. There may sometimes be a reason to use the  annotation to declare Testcontainers in our integration test classes. The disadvantage of this is that our Testcontainers will start and stop between each test class that is run, which can take a very long time. We can instead use the singleton pattern to make our Testcontainers start only once for all our test classes, and then be taken down when they are finished.  Keep in mind that you have to deal with any side effects caused by multiple tests using the same resource. For example, by making your tests unaffected by the state of the resource or by setting the resource in the desired state for each test.   To use the singleton pattern we can create an abstract class that handles the creation of Testcontainers.   Each integration test class should then inherit from this abstract class to make use of Testcontainers.   If we now run our tests, we will see that our Testcontainer is only started once, which means that we shorten the time to run our test suites significantly.    In my current projects I have multiple containers in my test suites and it will take some time to start, even if it's only done once. But with some small changes we can reduce the startup time.  Let us first create an example where we start several Testcontainers and then make some changes to see if we can reduce the startup time. In order not to get misleading results, we need to run it once first to ensure that all images are pulled down and available locally. To keep the example clean and clear I am not using any properties from our started containers.   If we now run one test we can see that that it takes a little less than 30 seconds to start all five Testcontainers.  Ok, time to make some changes. As we can see in the code above we are starting our testcontainers sequentially, what if we could start them in parallel instead? Let's try that. We can use the Stream class for this and it will look like this.   If we now run our test again we can see that it takes a little less than 12 seconds to start all five Testcontainers.  So we saved 18 seconds in startup time, it wasn't that bad, was it? Is there anything else we can do to make our tests faster? Yes there is, just keep reading and you will see yet another way.    One thing that repeatedly slows me down is when I work with one test and want to run it several times. It can take a long time and I want a fast feedback loop. So let's dig deeper and see how we can solve that.  Testcontainers comes with a [preview feature] will not be started, which in turn means that our Testcontainers will not be taken down after our tests are completed. This feature can really save time during the development process! All you have to remember is to manually stop all Testcontainers when you are done.  We have to do two things to enable this reuse. First we have to tell our Testcontainer that it should be reused, using the `.withReuse in our home directory and add the property . At the time of writing you can only have this property file in your home directory and not in the classpath, per project. But on the other hand, you probably do not want to impose such behavior on your team members or other environments, such as your CI / CD pipeline. This should definitely be optional and if it becomes possible to add it per project ,you should add it to  to prevent it from being propagated to your collegues and to the CI / CD pipeline.  This is how our abstract class will look like when enabling :   One [problem] that you probably will encounter is that the Testcontainer is not unique to this particular project. Since I have several Testcontainers with the same configuration in several projects, this posed a real problem for me. So let's fix this.  If we take a look at the [source code] as the value.   This is what it looks like in the abstract class when we activate reuse with a unique id to our Testcontainers.   If we start our test again, it took about 11 seconds to start our containers the first time, our Testcontainers will now be reused and this time it will only take about 1 second. That's not bad, is it?   This is really a performance boost to get faster feedback when running our tests. And now all we have to do, when we are done testing, is to stop our running Testcontainers.  To list all running Testcontainers:  and to stop all running Testcontainers:    One question that often comes up is this: If my tests now use docker containers, can the tests themselves run in a docker container? Yes they can, it is called [Docker in Docker] aka. DinD. With docker in docker, there will be new challenges such as privileged mode etc., but that's not something I intend to address here.  To run your tests locally in a docker container.   You can read more about Testcontainers and Docker in Docker [here]  In this blog post I have showed you some ways to drastically reduce the time to run your tests using Testcontainers.  And finally; I'm just a human being, if you find any ambiguities, typos, errors or possible improvement, please let me know. If you like this blog post, please add a comment and feel free to share this blog post using your favorite social media platform! There should be a few icons below to get you started.  Until next time, keep coding and never stop learning new things!  * Testcontainers [home page] * The full source code for my example project can be found here: [https://github.com/merikan/testcontainers-demo)] and the folder .
 **TODO: KORTARE MENINGAR**  Back in 2015-2017 I wrote a [blog series] and they asked me if I could write a hands-on book based on the blog series but updated and expanded a bit, also covering Kubernetes and Istio.  In September 2019, the book was published. In this blog post I will go through briefly what you can learn by reading it.  -[readmore]-  The book cover looks like:  <a href="https://www.packtpub.com/web-development/hands-on-microservices-with-spring-boot-and-spring-cloud"><img src="/assets/blogg/build-microservices-part-9/front-cover.png" width="400"></a>  The book covers both how to develop microservices but most important how to handle the challenges that comes with cooperating microservices. The following picture summarize capabilities required to handle the challenges:  <img src="/assets/blogg/build-microservices-part-9/challenges-with-microservices.png" width="600">  **TODO: ADD A LINE PER CHALLENGE**  In a large scaled system landscape with microservice, it must be possible to:  1. Keep track of all running microservices and its instances using a ***Discovery Server***. 2. Secure and route public APIs to the corresponding miorservcies using an ***Edge Server***. 3. Centralize management of the configuration of the microservices using a ***Configuration Server***. 4. Collect, store and analyse log output from the microservices. 5. Continuously check the health of the microservice instances and automatically restart unhealthy microservices, e.g. microservices instances that has hanged or crashed. 6. Collect and visualize metrics for resource usage per microservice isntance. 7. Observe the traffic that flows through the microservices, either synchronously API requests or as asynchronous message passing. 8. Manage how traffic is routed between the microservices, e.g. to be able to perform a rolling upgrade and if required a rollback if the upgrade failed. 9. Trace distributed call chains through the microservice landscape to be able to identify root causes of errors and find performance bottlenecks. 10. Minimize the effect of temporary networks errors to prevent that unnecessary large parts of the system landscape is affected using resilience mechanisms, e.g. timeouts, retries and circuit breakers.  The book consists of three sections:  **TODO**: Markera verktyg med *italic* ed?  In this section you will learn how to use Spring Boot 2.1 to build microservices that:  1. communicates using either RESTful APIs over HTTP or by sending events using a message broker like RabbitMQ or Kafka. **TOGETHER WITH SPRING CLOUD STREAM???** 1. document their exposed APIs using SpringFox to create Swagger definitions. 1. store their data in a database using Spring Data, either in MongoDB or MySQL. 1. are reactive, i.e. synchronous communication over the API is done using non blocking I/O and event passing is done asynchronously. ...using the Reactor project, Spring WebFlux and Spring Cloud Stream. 1. run as containers using Docker and uses Docker Compose to bring up a system landscape of cooperating microservices.  In this section you will learn how to make a system landscape of cooperating microservices scalable, resilient and manageable using Spring Cloud, Greenwich release. It covers: **TODO: MER TEXT**  1. service discovery using Netflix Eureka 1. hiding private APIs and exposing public APIs using Spring Cloud Gateway 1. protecting the public APIs using OAuth 2.0 and OpenID Connect 1. central management of microservices configuration using Spring Cloud Config Server 1. making the microservices resilient by using Resilience4J 1. distributed tracing using Spring Cloud Sleuth and Zipkin  In this section you will learn how to use Kubernetes as a *container orchestrator* and and Istio as a *service mesh*. Together they provide an excellent platform for deploying microservices in production. This section covers:  1. deploying and running microservices in Kubernetes 1. using the features in Istio for improved security, observability, traffic management. 1. replacing many of the the Spring Cloud services with standard functionality in Kubernetes and Istio, simplifying the system landscape 1. using Prometheus and Grafana for monitoring and alerts 1. using the EFK stack  for centralized log analysis   These tools together covers the required capabilities mentioned above. This is illustrated by the following picture:  <img src="/assets/blogg/build-microservices-part-9/capability-mappings.png" width="600">  As we can see some tool overlaps for some capabilities. For example, both Spring Cloud, Kubernetes and IStio comes with tools that can act as an edge server, while service management only is covered by Kubernetes. The book also cover how to reason what tool to choose to these capabilities.  **TODO** ADD TABLE HERE AND SOM EXPLANATION FROM THE PPT <img src="/assets/blogg/build-microservices-part-9/overlaps.png" width="600">  selections...  <img src="/assets/blogg/build-microservices-part-9/overlaps-selections.png" width="600">    **TODO** Each chapter builds on the previous chapters and add a new technology... the sample code in the book is based on four cooperating microservices:  <img src="/assets/blogg/build-microservices-part-9/p1.1.7-sample-microservice-landscape.png" width="400">  In the end after adding features from Spring CLoud, Kubernetes and Istio we will have a system landscape tat looks like the following:  **TODO**: Bakgrundsfärg, add Spring Cloud Sleuth! Size?  <img src="/assets/blogg/build-microservices-part-9/cadec-2020-overview.png" width="600">  **TODO**: Lägg in mängdlärabild där valen syns + tabellen!  If you are interested, the book is available at [Packt web site].  Happy Reading!   In the coming blog posts we will go through a bit more what we can learn from each section in the book!  For more blog posts on new ..., see the blog series - [building microservices]. 
    During SpringOne2GX I listened to Matt Stine´s presentation about Lattice, a stripped-down version of [Cloud Foundry] which really lowers the barrier to get started to develop and test cloud-native architectures...  -[readmore]-  ...literally speaking, Lattice enables a kickstart for developers to install micro-cloud environments on the their local desktops. Lattice reduces the overall footprint from an infrastructure perspective and allows to get started in just minutes.  Behind the scenes, we recognize the following [components] from Cloud Foundry:  - [Router] is responsible for load balancing traffic across running containers which can be updated dynamically as applications are launched or spun down. - [Diego] the Cloud Foundry's upcoming elastic runtime for containers. Diego is responsible for scheduling and running containerized workloads. - [Doppler/Loggregator] is responsible for streaming logs out of running containers.  Lattice consists of:  * Cluster Scheduling * Load Balancing  * Health Management * Log Aggregation  Lattice is installed easiest by using Vagrant :  $ curl https://lattice.s3.amazonaws.com/nightly/lattice-bundle-v0.4.3-osx.zip $ unzip lattice-bundle-v0.4.3-osx $ cd lattice-bundle-v0.4.3-osx/vagrant  $ vagrant up  Within minutes, a Lattice VM will be reachable at 192.168.11.11.  To be able to interact with Diego, Lattice offers a [CLI] containers.  The Lattice CLI is a downloaded as a part of the "lattice-bundle" and in order to connect to the Lattice VM, we are using the :  $ cd lattice-bundle-v0.4.3-osx $ ltc target 192.168.11.11.xip.io  To lists applications and tasks running on Lattice:  $ ltc list  Lattice makes deployment of applications very simple and straightforward. The following demo will utilize [Spring Cloud Config] to provide server and client support for externalized configuration.  In order to show different deployment options with Lattice, I let the server  will be deployed as a minimal Spring Boot application into a droplet container.  The client application also supports updating the properties dynamically. This is by adding a @RefreshScope annotation and a dependency to the Spring Boot Actuator.  Now, the next step is to install the  from a Docker Hub:  $ ltc create config-server springcloud/configserver --run-as-root --memory-mb 256 --env spring.cloud.config.server.git.uri=https://github.com/deibitsch/config-repo  Verify that the  application is up and running on Lattice:  $ ltc status config-server  This shows how to clone, build and deploy a minimal Spring Boot application into a .  $ cd ~/Documents/Development/git-repos $ git clone https://github.com/deibitsch/config-client.git $ cd config-client $ mvn clean install  Build  into a droplet using a CF buildpack:  $ ltc build-droplet config-client java -p ~/Documents/Development/git-repos/config-client/target/config-client-0.0.1-SNAPSHOT.jar --env spring.cloud.config.uri=http://config-server.192.168.11.11.xip.io  List existing droplets:  $ ltc list-droplets  Launch a the droplet as an app running on Lattice by:  $ ltc launch-droplet config-client config-client  Verify the  app on Lattice:  $ ltc status config-client  The client application has two endpoints:  * / returns greeting message  * /refresh enables to refresh properties from central repository  using "config-server".  You should be able to visit http://config-client.192.168.11.11.xip.io in your browser.  $ open http://config-client.192.168.11.11.xip.io  The property  can now be dynamically updated by applying a new value to  in central git-repo. The invocation  of the  endpoint will update property due to  annotation in the  application.  $ curl -X POST http://config-client.192.168.11.11.xip.io/refresh  The updated property should be updated if you refresh http://config-client.192.168.11.11.xip.io in your browser.  $ open http://config-client.192.168.11.11.xip.io  The Lattice CLI enables to stream logs from a specific application:  $ ltc logs config-client  Simply, invoke the "/refresh" endpoint resulting in log messages to be visible in the terminal:  $ curl -X POST http://config-client.192.168.11.11.xip.io/refresh  In order to scale up the number of instances of the client application:  $ ltc scale config-client 2  Verify that there are two instances running:  $ ltc status config-client
 This article covers how to setup your standard Eclipse environment with plugins from SpringSource for developing Groovy and Grails applications without using the full SpringSource Tool Suite.  -[readmore]-  It is more and more common that we see other languages than Java running on the JVM. One such language is Groovy which together with the Ruby on Rails inspired framework Grails forms a good platform for rapid development of simple web applications.  SpringSource Tool Suite is an extended Eclipse with support for a lot of interesting things. There’s is good support for developing Groovy and Grails applications in the SpringSource Tool Suite IDE, easy to setup via the built-in extension mechanism which is available in the Extensions tab on the STS Dashboard.  If however, we are interested in developing Groovy and Grails applications and don’t care so much about all the other stuff that SpringSource Tool Suite is packaging it is possible to pick out only the plug-ins that we need from the SpringSource Tool Suite and apply them to a clean Eclipse version.  This article presents a way of setting up a development environment for Groovy and Grails inside Eclipse.  Some of the functionalities in the Grails framework requires you to have a JDK installed.  Make sure that you have a JDK of version J2SE 1.6 installed and that your  environment variable points to the JDK and that Eclipse uses it on the build path of your projects, otherwise follow the instructions below.  - Go to the [JDK Download page] at Oracle and download Java SE Development Kit 6u23. - Follow the instructions in the installer to install the JDK on your computer. - Make sure that the  environment variable exists and points at your installed JDK `JAVA_HOME="c:\Program Files\Java\jdk1.6.0_23"` - Start Eclipse and open **Preferences/Java/Installed JREs** - Click **Add...**, Choose **Standard VM** and click **Next**. - Click **Directory** and choose the home folder of your JDK  - Click **Finish** to get back to the preferences. - Use the checkbox next to the JDK to activate it. - Click **OK** to save settings and close preferences.  We will start with a clean version of Eclipse 3.6.1 from Eclipse’s download page and install the plug-ins we need.  - Open a browser and go to [http://www.eclipse.org/downloads] - Download the package named: **Eclipse IDE for Java EE Developers** - Unpack the downloaded package in the location of your choice and start Eclipse. - Open up **Install New Software** under the Help menu. - Add two update sites 1. SpringSource update site for Eclipse 3.6 Release [http://dist.springsource.com/release/TOOLS/update/e3.6] 2. SpringSource update site for Eclipse 3.6 Release dependencies [http://dist.springsource.com/release/TOOLS/composite/e3.6] - Back in the **Install New Software** window, choose to work with: **SpringSource update site for Eclipse 3.6 Release** - Select **SpringSource Tool Suite Grails Support** under the category **Extensions / STS** - Press the **Next** button and follow the wizard to complete the installation. The installation might seem to be stuck at zero percent for a couple of minutes, just be patient. You will also get a warning that you are trying to install unsigned content which is normal in this case. - Restart Eclipse - You might get a question about uploading data to SpringSource with Spring User Agent Analysis, which you can answer in the way you want.  To make the plug-ins work we also need to download the Grails framework.  - Open up a browser and go to[ http://www.grails.org/Download] - Choose to download version 1.3.6 Binary Zip - Unpack the Grails framework where you like to store your development tools e.g.  - Add an environment variable  that points to your Grails installation folder   - Also update the  environment variable to point at    - In Eclipse, go to **Preferences/Groovy/Grails** and **Add…** - Enter **Grails** in the **Name** field. - Press **Browse...** and point out the the root folder of your unzipped your Grails framework e.g.  - Press **OK**.  Your Eclipse environment is now ready for developing Groovy and Grails applications.  Follow these steps to perform a simple test to see that Groovy now works inside Eclipse. As a side effect you also get a shortcut to a great tool for trying out groovy scripts, the Groovy Console.  You should now have a Grails perspective in Eclipse.  - Choose **Open Perspective/Other...** from the **Window** menu - Select **Grails** and click **OK**  - Right click in the **Project Explorer** and open **New/Other...** - Select **Groovy Project** and click **Next** - Add a project name e.g. **GroovyConsole** and click **Finish**  - Right click your project named **GroovyConsole** and open **New/Other...** - Select **Groovy Class** and click **Next** - Enter **GroovyConsole** in the name field, check the **Create Script** select box and click **Finish** - Open the newly created file  and insert the following code:   * Save the file.  Right click  and click on **Run As/Groovy Script**  - You shall now get a Groovy Console opened which verifies that Groovy is working inside Eclipse. You can also use the Groovy Console for testing groovy scripts.  For more information about the technologies and tools used in this article there is more to read on these locations:  - Groovy – [http://groovy.codehaus.org/] - Grails – [http://www.grails.org/] - SpringSource Tool Suite – [http://www.springsource.com/developer/sts] - Eclipse – [http://www.eclipse.org/]
This part of the [blog series] will deal with two fundamental pieces of a sound microservice architecture - service discovery and load-balancing - and how they facilitate the kind of horizontal scaling we usually state as an important non-functional requirement in 2017.  While load-balancing is a rather well-known concept, I think Service Discovery entails a more in-depth explanation. I'll start with a question:  _"How does Service A talk to Service B without having any knowledge about where to find Service B?"_  In other words - if we have 10 instances of Service B running on an arbitrary number of cluster nodes, someone needs to keep track of all these 10 instances. So when Service A needs to communicate with Service B, at least one proper IP address or hostname for an instance of Service B must be made available to Service A . In the continuously changing context of a microservice landscape, either approach requires _Service Discovery_ to be present. In its simplest form, Service Discovery is just a registry of running instances for one or many services.  If this sounds a lot like a DNS service to you, it kind of is. The difference being that service discovery is for use within your internal cluster so your microservices can find each other, while DNS typically is for more static and external routing so external parties can have requests routed to your service. Also, DNS servers and the DNS protocol are typically not well suited for handling the volatile nature of microservice environments with ever-changing topology with containers and nodes coming and going, clients often not honoring TTL values, failure detection etc.  Most microservice frameworks provides one or several options for service discovery. By default, Spring Cloud / Netflix OSS uses [Netflix Eureka] should also be mentioned in this crowd.  In this blog post, we'll primarily deal with the mechanisms offered by "Docker Swarm"  since we'll be doing service-to-service communication.  _Note: When referring to "Docker Swarm" in this blog series, I am referring to running Docker 1.12 or later in [swarm mode]" as a standalone concept was discontinued with the release of Docker 1.12._  In the realm of microservices, one usually differentiates between the two types of load-balancing mentioned above:  - Client-side: It's up to the client to query a discovery service to get actual address information  ecosystem that's backed by etcd. Some advantages of client-side load-balancing is resilience, decentralization and no central bottlenecks since each service consumer keeps its own registry of producer endpoints. Some drawbacks are higher internal service complexity and risk of local registries containing stale entries. ![client-side]  - Server-side: In this model, the client relies on the load-balancer to look up a suitable instance of the service it wants to call given a logical name for the target service. This mode of operation is often referred to as "proxy" since it functions both as a load-balancer and a reverse-proxy. I'd say the main advantage here is simplicity. The load-balancer and service discovery mechanism is typically built into your container orchestrator and you don't have to care about installing or managing those components. Also, the client  doesn't have to be aware of the service registry - the load-balancer takes care of that for us. Being reliant on the load-balancer to route all calls arguably decreases resilience and the load-balancer _could_ theoretically become a performance bottleneck. ![server-side]  Note that the actual _registration_ of producer services in the server-side example above is totally transparent to you as developer when we're using the _service_ abstraction of Docker in swarm mode. I.e - our producer services isn't even aware they are operating in a server-side load-balanced context . Docker in swarm mode takes care of the full registration/heartbeat/deregistration for us.  In the example domain we've been working with since [part 2] to integrate Go apps with Eureka including basic lifecycle management.  Let's say you want to build a custom-made monitoring application and need to query the _/health_ endpoint of every instance of every deployed service. How would your monitoring app know what IP's and ports to query? You need to get hold of actual service discovery details. If you're using Docker Swarm as your service discovery and load-balancing provider and need those IPs, how would you get hold of the IP address of each instance when Docker Swarm is keeping that information for us? With a client-side solution such as Eureka you'd just consume the information using its API. However, in the case of relying on the orchestrator's service discovery mechanisms, this may not be as straightforward. I'd say there's one primary option to pursue and a few secondary options one could consider for more specific use cases.  Primarily, I would recommend using the Docker Remote API - e.g. use the Docker APIs from within your services to query the Swarm Manager for service and instance information. After all, if you're using your container orchestrator's built-in service discovery mechanism, that's the source you should be querying. For portability, if that's an issue, one can always write an adapter for your Orchestrator of choice. However, it should be stated that using the Orchestrator's API have some caveats too - it ties your solution closely to a specific container API and you'd have to make sure your application can talk to the Docker Manager, e.g. they'd be aware of a bit more of the context they're running in and using the Docker Remote API does increase service complexity somewhat.  * Use an additional separate service discovery mechanism - i.e. run Netflix Eureka, Consul or similar and make sure microservices that wants to be made discoverable register/deregister themselves there in addition to the Docker swarm mode mechanics. Then just use use the discovery service's API for registering/querying/heartbeating etc. I dislike this option as it introduces complexity into services when Docker in swarm mode can handle so much of this for us more or less transparently. I almost consider this option an anti-pattern so don't do this unless you really have to. * Application-specific discovery tokens - in this approach, services that want to broadcast their existence can periodically post a "discovery token" with IP, service name etc. on a message topic. Consumers that needs to know about instances and their IPs can subscribe to the topic and keep its own registry of service instances up-to date. When we look at Netflix Turbine _without_ Eureka in a later blog-post, we'll use this mechanism to feed information to a [custom Turbine discovery plugin] I've created by letting Hystrix stream producers register themselves with Turbine using discovery tokens. This approach is a bit different as it doesn't really have to leverage the full service registry - after all, in this particular use-case we only care about a specific set of services.   Feel free to checkout the appropriate branch for the completed source code of this part from [github]:  git checkout P7  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._   We'll continue this part by taking a look at scaling our "accountservice" microservice to run multiple instances and see if we can make Docker Swarm automatically load-balance requests to it for us.  In order to know what instance that actually served a request we'll add a new field to the "Account" struct that we can populate with the IP address of the producing service instance. Open _/accountservice/model/account.go_:  type Account struct { Id string  Name string   // NEW ServedBy string  }  When serving an Account in the _GetAccount_ function, we'll now populate the _ServedBy_ field before returning. Open _/accountservice/service/handlers.go_ and add the _GetIp_ function as well as the line of code that populates the _ServedBy_ field on the struct:  func GetAccount {  // Read the 'accountId' path parameter from the mux map var accountId = mux.Vars["accountId"]  // Read the account struct BoltDB account, err := DBClient.QueryAccount  account.ServedBy = getIP // NEW, add this line ... }  // ADD THIS FUNC func getIP string { addrs, err := net.InterfaceAddrs if err != nil { return "error" } for _, address := range addrs { // check the address type and if it is not a loopback the display it if ipnet, ok := address. { if ipnet.IP.To4 != nil { return ipnet.IP.String } } } panic }  The _getIP_ function should go into some "utils" package since it's reusable and useful for a number of different occurrences when we need to determine the non-loopback IP-address of a running service.  Rebuild and redeploy our service by running _copyall.sh_ again from _$GOPATH/src/github.com/callistaenterprise/goblog_:  > ./copyall.sh  Wait until it's finished and then type:  > docker service ls ID NAME REPLICAS IMAGE yim6dgzaimpg accountservice 1/1 someprefix/accountservice  Call it using curl:  > curl $ManagerIP:6767/accounts/10000   Lovely. We see that the response now contains the IP address of the container that served our request. Let's scale the service up a bit:  > docker service scale accountservice=3 accountservice scaled to 3  Wait a few seconds and run:  > docker service ls ID NAME REPLICAS IMAGE yim6dgzaimpg accountservice 3/3 someprefix/accountservice  Now it says replicas 3/3. Let's curl a few times and see if get different IP addresses as _servedBy_.  curl $ManagerIP:6767/accounts/10000   curl $ManagerIP:6767/accounts/10000   curl $ManagerIP:6767/accounts/10000   curl $ManagerIP:6767/accounts/10000   We see how our four calls were round-robined over the three instances before 10.0.0.22 got to handle another request. This kind of load-balancing provided by the container orchestrator using the Docker Swarm "service" abstraction is very attractive as it removes the complexity of client-side based load-balancing such as Netflix Ribbon and also shows that we can load-balance without having to rely on a service discovery mechanism to provide us with a list of possible IP-addresses we could call. Also - from Docker 1.13 Docker Swarm won't route any traffic to nodes not reporting themselves as "healthy" if you have implemented the Healthcheck. This is very important when having to scale up and down a lot, especially if your services are complex and may take more than the few hundreds of milliseconds to start our "accountservice" currently needs.  It may be interesting to see if and how scaling our accountservice from one to four instances affects latencies and CPU/memory usage. Could there be a substantial overhead when the Swarm mode load-balancer round-robins our requests?  > docker service scale accountservice=4  Give it a few seconds to start things up.  Running the Gatling test with 1K req/s:  CONTAINER CPU % MEM USAGE / LIMIT accountservice.3.y8j1imkor57nficq6a2xf5gkc 12.69% 9.336 MiB / 1.955 GiB accountservice.2.3p8adb2i87918ax3age8ah1qp 11.18% 9.414 MiB / 1.955 GiB accountservice.4.gzglenb06bmb0wew9hdme4z7t 13.32% 9.488 MiB / 1.955 GiB accountservice.1.y3yojmtxcvva3wa1q9nrh9asb 11.17% 31.26 MiB / 1.955 GiB  Well well! Our 4 instances are more or less evenly sharing the workload and we also see that the three "new" instances stay below 10 mb of RAM given that they never should need to serve more than 250 req/s each.  First - the Gatling excerpt using one  instance: ![performance] Next - from the run with four  instances: ![performance]  The difference isn't all that great - and it shouldn't be - all four service instances are after all running on the _same_ virtualbox-hosted Docker Swarm node on the same underlying hardware . If we would add more virtualized instances to the Swarm that can utilize _unused_ resources from the host OS we'd probably see a much larger decrease in latencies as it would be separate logical CPUs etc. handling the load. Nevertheless - we do see a slight performance increase regarding the mean and 95/99-percentiles. We can safely conclude that the Swarm mode load-balancing has no negative impact on performance in this particular scenario.  Remember that Java-based _quotes-service_ we deployed back in [part 5]? Let's scale it up and then call it from the "accountservice" using its service name "quotes-service". The purpose of adding this call is to showcase how transparent the service discovery and load-balancing becomes when the only thing we need to know about the service we're calling is its logical _service_ name.  We'll start by editing _/goblog/accountservice/model/account.go_ so our response will contain a quote:  type Account struct { Id string  Name string  ServedBy string  Quote Quote  // NEW }  // NEW struct type Quote struct { Text string  ServedBy string  Language string  }  Note that we're using the [json tags] to map from the field names that the _quotes-service_ outputs to struct names of our own, _quote_ to _text_, _ipAddress_ to _ServedBy_ etc.  Continue by editing _/goblog/accountservice/service/handler.go_. We'll add a simplistic _getQuote_ function that will perform a HTTP call to _http://quotes-service:8080/api/quote_ whose return value will be used to populate the new _Quote_ struct. We'll call it from the main _GetAccount_ handler function.  First, we'll deal with a Connection: Keep-Alive issue that will cause load-balancing problems unless we explicitly configure the Go http client appropriately. In _handlers.go_, add the following just above the _GetAccount_ function:  var client = &http.Client  func init { var transport http.RoundTripper = &http.Transport{ DisableKeepAlives: true, } client.Transport = transport }  This init method will make sure any outgoing HTTP request issued by the _client_ instance will have the appropriate headers making the Docker Swarm-based load-balancing work as expected. Next, just below the _GetAccount_ function, add the package-scoped _getQuote_ function:  func getQuote { req, _ := http.NewRequest resp, err := client.Do  if err == nil && resp.StatusCode == 200 { quote := model.Quote bytes, _ := ioutil.ReadAll json.Unmarshal return quote, nil } else { return model.Quote, fmt.Errorf } }  Nothing special about it. That "?strength=4" argument is a peculiarity of the quotes-service API that can be used to make it consume more or less CPU. If there are some problem with the request, we return a generic error.  We'll call the new _getQuote_ func from the _GetAccount_ function, assigning the returned value to the _Quote_ property of the _Account_ instance if there were no error:  // Read the account struct BoltDB account, err := DBClient.QueryAccount account.ServedBy = getIP  // NEW call the quotes-service quote, err := getQuote if err == nil { account.Quote = quote }    If we would run the unit tests in _/accountservice/service/handlers_test.go_ now, they would fail! The GetAccount function under test will now try to do a HTTP request to fetch a famous quote, but since there's no quotes-service running on the specified URL  the test cannot pass.  We have two strategies to choose from here given the context of unit testing:  1) Extract the _getQuote_ function into an interface and provide one real and one mock implementation, just like we did in [part 4] for the Bolt client. 2) Utilize a HTTP-specific mocking framework that intercepts outgoing requests for us and returns a pre-determined answer. The built-in httptest package can start an embedded HTTP server for us that can be used for unit-testing, but I'd like to use the 3rd party [gock] framework instead that's more concise and perhaps a bit easier to use.  In _/goblog/accountservice/service/handlers_test.go_, add an init function above the _TestGetAccount instance is intercepted properly by gock:  func init { gock.InterceptClient }  The gock DSL provides fine-granular control over expected outgoing HTTP requests and responses. In the example below, we use New to tell gock to expect the _http://quotes-service:8080/api/quote?strength=4_ GET request and respond with HTTP 200 and a hard-coded JSON string as body.  At the top of _TestGetAccount_, add:  func TestGetAccount { defer gock.Off gock.New. Get. MatchParam. Reply. BodyString  _defer gock.Off_ will turn http intercept on which could potentially fail subsequent tests.  Let's assert that the expected quote was returned. In the innermost _Convey_-block of the _TestGetAccount_ test, add a new assertion:  Convey { So  account := model.Account json.Unmarshal So So  // NEW! So })  Try running all tests from the _/goblog/accountservice_ folder:  > go test ./... ? github.com/callistaenterprise/goblog/accountservice [no test files] ? github.com/callistaenterprise/goblog/accountservice/dbclient [no test files] ? github.com/callistaenterprise/goblog/accountservice/model [no test files] ok github.com/callistaenterprise/goblog/accountservice/service 0.011s  Rebuild/redeploy using _./copyall.sh_ and then try calling the _accountservice_ using curl:  > curl $ManagerIP:6767/accounts/10000 {"id":"10000","name":"Person_0","servedBy":"10.255.0.8","quote":  }  Scale the quotes-service to two instances:  > docker service scale quotes-service=2  Give it some time, it may take 15-30 seconds as the Spring Boot-based quotes-service is not as fast as our Go counterparts to start. Then call it again a few times using curl, the result should be something like:      We see that our own _servedBy_ is nicely cycling through the available _accountservice_ instances. We also see that the _ipAddress_ field of the _quote_ object has two different IPs. If we hadn't disabled the keep-alive behaviour, we'd probably be seeing that the same instance of _accountservice_ keeps serving quotes from the same _quotes-service_ instance.  In this part we touched upon the concepts of Service Discovery and Load-balancing in the microservice context and implemented calling of another service using only its logical service name.  In [part 8], we'll move on to one of the most important aspects of running microservices at scale - centralized configuration.
In this part of the Go microservices [blog series] for serving Account objects to our clients.  1. Overview 2. GraphQL 3. Schemas, Fields and Types with graphql-go 4. Resolver function 5. Queries 6. Unit tests 7. Serving over HTTP 8. Summary   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P14  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._   This part of the blog series won't introduce any new services, it will only add a new /graphql POST endpoint to the "accountservice" which will be wired to serve requests as defined by the [graphql schema] we'll define in Go code.   GraphQL was developed internally by Facebook and was publicly released in 2015. It provides an alternative query language to RESTful and other architectures for serving data from a server to a client. It's perhaps most unique feature is that GraphQL allows clients to define how the data requested shall be structured, rather than letting the server decide. This means that clients can fetch exactly the data required, mitigating the classic problems of fetching either too much or too little data for the use case at hand.  I recommend digging into the [official documentation] for more detailed explanations.   Let's take a quick look at setting up a GraphQL schema using key concepts such as types, field, resolve functions, root queries and the resulting schema.  The schema defines what types and fields that can be selected in a GraphQL query. GraphQL isn't tied to any particular DSL or programming language. Since this is a Go blog, I'll use Go GraphQL schemas according to the [graphql-go/graphql] project on github.  Here's the GraphQL type definition of the "AccountEvent" we introduced in part 13 of the blog series:  var accountEventType = graphql.NewObject( // Create new object graphql.ObjectConfig{ // Declare object config Name: "AccountEvent", // Name of the type Fields: graphql.Fields{ // Map declaring the fields of this type "id": &graphql.Field{ // Field declaration, "id" is its name Type: graphql.String, // of type string. }, "eventName": &graphql.Field{ Type: graphql.String, }, "created": &graphql.Field{ Type: graphql.String, }, }, } )  The type declaration above is about as simple as it gets with GraphQL, not that unlike a Go struct declaration.  However, it gets a bit more hairy when we introduce Resolver functions, arguments and link together several declared types into a composite object.  Here's a somewhat simplified type declaration for our "Account" type that pretty much mirrors the output struct we're currently using:  // accountType, includes Resolver functions for inner quotes and events. var accountType = graphql.NewObject(graphql.ObjectConfig{ Name: "Account", Fields: graphql.Fields{ "id": &graphql.Field{ // The id, name and servedBy fields should be familiar Type: graphql.String, }, "name": &graphql.Field{ Type: graphql.String, }, "servedBy": &graphql.Field{ Type: graphql.String, },  // continued...  This first part is very similar to the "accountEventType" we've already declared, just "primitive" fields on a type, no big deal.  However, the next part of the "accountType" declaration becomes much more complex when we declare that the "accountType" contains a _list_ of "accountEventType"s and a _Resolve_ function.  We'll see the Resolve function for actual Account objects later when we look at a GraphQL queries. In that context, the Resolve function is the piece of code that actually fetches Account structs  and stuffs that data into the GraphQL runtime that makes sure the outputted data conforms with the structure requested by the query.  The Resolve function below operates on already-fetched data  and performs filtering of each item's "events" using the "eventName" argument if the query has specified such:   "events": &graphql.Field{ // Here's how we declare that our "account" type can contain Type: graphql.NewList, // a list field. Declare "events" as type List of the accountEvent // type we declared in the last code sample. Args: graphql.FieldConfigArgument{ // Args declare _what_ fields we allow queries to use when filtering "eventName": &graphql.ArgumentConfig{ // this sublist in the context of the parent account type. Type: graphql.String, }, }, // Resolve functions on types allows us to use declared  // args in order to perform filtering of items from a sub-list. Resolve: func { account := p.Source. // Get the struct we're performing filtering on.  events := make // Create a new slice to return the wanted accountEvents in.  for _, item := range account.AccountEvents { // Iterate over all accountEvents on this account.  if item.EventName == p.Args["eventName"] { // Add to new list only if predicate is true events = append } } return events, nil // Return the new list. }, }, // truncated for brevity...  Resolve functions was the hardest part for me to get a grasp on, we'll see a bit more Resolve code in just a little bit when looking at the Resolve function for the actual account query.   In order for a client to be able to fetch account objects, we need to create a _schema_ consisting of a _SchemaConfig_ with a _RootQuery_ specifying queryable _Fields_.  Schema <- SchemaConfig <- RootQuery <- Field  In Go code this is declared like this:  rootQuery := graphql.ObjectConfig schemaConfig := graphql.SchemaConfig var err error schema, err = graphql.NewSchema  Deceptively simple. The actual complexity is in the _fields_ argument. We'll declare a single Field called "Account":  // Schema fields := graphql.Fields{ "Account": &graphql.Field{ Type: graphql.Type, // See accountType above Args: graphql.FieldConfigArgument{ "id": &graphql.ArgumentConfig{ Type: graphql.String, }, "name": &graphql.ArgumentConfig{ Type: graphql.String, }, }, Resolve: resolvers.AccountResolverFunc, }, }  This looks an awful lot like the stuff we've already declared, which is kind of the point.  My interpretation of what we're actually seeing is that the declared "Account" field is a query on the RootQuery.  * This "Account" field consists of a single GraphQL type "accountType" - e.g. exactly the type we defined above. * The "Account" defines two arguments that can be used for querying an account - id and name. * The "Account" defines a _Resolve_ func that is provided by a named function reference from another package.  I'd say the final schema with fields and types could be represented like this:  ![classdiagram]  If we wanted a GraphQL query that returns a list of Accounts, another field could be declared such as:  "AllAccounts": &graphql.Field{ Type: graphql.NewList, // List of accountType objects Args: graphql.FieldConfigArgument{ "name": &graphql.ArgumentConfig{ Type: graphql.String, }, }, Resolve: resolvers.AllAccountsResolverFunc, // Some function that returns all accounts },  That Field on the RootQuery specifies a List of accountType as its type. The single "name" argument could perhaps be implemented as a "like" search or similar in the specified "AllAccountsResolverFunc" function.   So, now that we have put our schema together, how do we actually tie our underlying data model to the Resolver functions declared in that "resolvers" parameter?   One of the sweet things about being able to pass Resolve functions that duck-types to the  functionName  signature is that we easily can provide different implementations for unit tests and real implementations. This is done using good ol' go interfaces and implementations:  // GraphQLResolvers defines an interface with our Resolver function type GraphQLResolvers interface { AccountResolverFunc }  // LiveGraphQLResolvers - actual implementation used when running outside of unit tests. type LiveGraphQLResolvers struct {  }  func  { account, err := fetchAccount if err != nil { return nil, err } return account, nil }  // TestGraphQLResolvers - implementation used in unit tests. type TestGraphQLResolvers struct {  }  func  { id, _ := p.Args["id"]. name, _ := p.Args["name"]. for _, account := range accounts { // The accounts slice is declared elsewhere in the same file as test data. if account.ID == id || account.NAME == name { return account, nil } } return nil, fmt.Errorf }  * The "live" implementation uses a "fetchAccount" function that actually talks to the other microservices  to fetch the requested account object. Nothing new there except some refactoring that makes sure our old _/accounts/_ HTTP endpoint uses the same code to get account objects as the new "fetchAccount" function used by our GraphQL resolve function. * The "test" implementation uses a hard-coded slice of Account objects and returns if matching either argument.  The resolver implementation used is simply whatever the calling code supplies. In unit tests:  func TestFetchAccount { initQL // Test implementation passed. ....  When started from the main func - i.e. either running standalone or when started within a Docker container), this line is invoked instead:  initQL   So far, we've only laid the groundwork. The purpose of GraphQL is after all facilitating those dynamic queries mentioned back in section #2 of this blog post.  GraphQL [queries] in their basic form simply asks for specific fields on objects declared in the schema. For example, if we want an Account object only containing the "name" and the events, the query would look like this:  query FetchSingleAccount { // Query and an arbitrary name for the query. This is optional!! Account { // We want to query the "Account" field on the RootQuery having id "123" name, events{ // Include the "name" and "events" fields in the response. eventName,created // On the "events", include only eventName and created timestamp. } } }  The response would look like:  { "data":{ "Account":{ "name":"Firstname-2483 Lastname-2483", "events":[{ "created":"2018-02-01T15:26:34.847","eventName":"CREATED" }] } } }  Note that we **must** specify what fields we want on the events, otherwise the following error would be returned:  "Field "events" of type "[AccountEvent]" must have a sub selection.",  We'll see a more complex example in the unit tests section below.  There's tons of stuff one can do using GraphQL queries, read up on fragments, parameters, variables etc. [here].    How do we assert that our schema is actually set up in a valid way and that our queries will work? Unit-tests to the rescue!  All the GraphQL code has gone into the file [/accountservice/service/accountql.go].  Let's start by specifying a GraphQL query as a multi-line string. The query uses variables, field selection and argument passing to the quote and events sub-fields.  var fetchAccountQuery = `query fetchAccount { Account { id,name,events { eventName },quote { quote },imageData } }`  Next, the test function:  func TestFetchAccount { initQL // #1 Init GraphQL schema with test resolvers Convey { vars := make // #2 Variables vars["accid"] = "123"  // #3 Create parameters object with schema, variables and the query string params := graphql.Params  Convey { r := graphql.Do // #4 Execute the query rJSON, _ := json.Marshal // #5 Transform the response into JSON  Convey {  // #6 Assert stuff... So So }) }) }) }  1. The very first thing we do is to call the _initQL_ func, passning our **test** Resolver implementation. The _initQL_ func is the one that we looked at in section #3, that sets up our schema, fields etc.. 2. We declare a String => interface map used to pass [variables] into the query execution. 3. The _graphql.Params_ contains the schema, variables and the actual query we want to execute. 4. The query is executed by passing the param object into the _graphql.Do_ func. 5. Transform response into JSON 6. Assert no errors and expected output.  The structure of the test above makes it quite simple to write queries and test them against your schema. The actual output will of course vary depending on what test data your TestResolvers are using and how they are treating arguments passed to them.   All this GraphQL stuff is rather useless unless we can provide the functionality to consumers of our service. It's time to wire the GraphQL functionality into our HTTP router!  Let's take a look at [/accountservice/service/routes.go] where a new _/graphql_ route has been declared:  Route{ "GraphQL", // Name "POST", // HTTP method "/graphql", // Route pattern gqlhandler.New(&gqlhandler.Config{ Schema: &schema, Pretty: false, }).ServeHTTP, },  Quite simple actually - the endpoint takes the query as POST body and the handler function is provided by a [graphql-go/handler] as argument.  We just need to make sure _initQL before initializing the routes:  func NewRouter *mux.Router {  initQL // HERE!!  router := mux.NewRouter for _, route := range routes { // rest omitted ...    __  To test our GraphQL stuff in a runtime environment, start your Docker Swarm mode server, make sure you have branch P14 checked out from git and run the _./copyall.sh_ script to build and deploy.  As always, deployment take a little while, but once everything is up and running, our brand new http://accountservice:6767/graphql endpoint should be ready for action.  Let's use curl to try it out! __  > curl -d '' -X POST -H "Content-Type: application/graphql" http://192.168.99.100:6767/graphql    Note that we're passing an appropriate Content-Type header.  I've also exposed the _/graphql_ endpoint in our Zuul EDGE server by adding an entry to the application.yaml. So we can call it through our reverse-proxy too including HTTPS termination:  > curl -k -d '' -X POST -H "Content-Type: application/graphql" https://192.168.99.100:8765/api/graphql  Note that the ID used in the queries above is for an Account already present in my CockroachDB Accounts database.  To obtain an account id to query with, I've added a two helper GET endpoints to the "dataservice" exposed at port 7070. First, a little _/random_ endpoint you can use to get hold of an Account instance, e.g:  > curl http://192.168.99.100:7070/random   If your database is empty , it should be possible to seed 100 accounts using another little utility endpoint _/seed_  > curl http://192.168.99.100:7070/seed    **Please note that running /seed removes all entries from your CockroachDB!!**   A very useful trait of GraphQL is it's capability to describe itself to clients using [introspection].  By doing queries on __schema and __type, we can obtain info about the schema we declared. This query returns all types in the schema:  { __schema { types { name } } }  Response:  { data": { "__schema": { "types": [ { "name": "Account" }, { "name": "__Type" }, { "name": "Boolean" }, { "name": "__DirectiveLocation" }, { "name": "AccountImage" }, ... omitted for brevity ... ], } } }  We can also take a closer look at the "Account" type, which after all is what we're usually dealing with in this API:  { __type { name fields { name type { name kind ofType { name kind } } } } }  Response:  { "data": { "__type": { "fields": [ { "name": "events", "type": { "kind": "LIST", "name": null, "ofType": { "kind": "OBJECT", "name": "AccountEvent" } } }, { "name": "id", "type": { "kind": "SCALAR", "name": "String", "ofType": null } }, ... Omitted for brevity ... },  The query lists available fields and their type on the "Account" type, including the type of list LIST kind, e.g. AccountEvent.   There's 3rd party GUIs that uses the introspection functionality to provide a GUI to explore and prototype queries, most notably [graphiql].  One can install GraphiQL into the cluster or run a local client. I'm using [graphiql-app] that's an Electron wrapper around Graphiql. To install on a Mac, use brew:  > brew cask install graphiql  Point the URL at our API running inside the local Docker Swarm mode cluster and enjoy full code-completion etc for writing queries or looking at the schema:  ![graphiql]   That's about it! In this part, we added support for querying our account objects using GraphQL. While our usage is pretty basic, it should get you started using GraphQL with Go. There's a lot more to explore when it comes to GraphQL, for further studies I recommend the [official introduction].  In the [next part], we'll finally get to adding support for monitoring using Prometheus endpoints.  Please help spread the word! Feel free to share this blog post using your favorite social media platform, there's some icons below to get you started.  Until next time,  // Erik 
In part 8 of the [blog series].  Centralizing something when dealing with Microservices may seem a bit off given that microservices after all is about decomposing your system into separate independent pieces of software. However, what we're typically after is isolation of processes. Other aspects of microservice operations should be dealt with in a centralized way. For example, logs should end up in your logging solution such as the [elk stack], we'll deal with externalized and centralized configuration using Spring Cloud Config and git.  Handling configuration for the various microservices that our application consists of in a centralized manner is actually quite natural as well. Especially when running in a containerized environment on an unknown number of underlying hardware nodes, managing config files built into each microservice image or from mounted volumes can quickly become a real headache. There are a number of proven projects to help deal with this, for example [etcd], a piece of software dedicated to provide exactly that.  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  The [Spring Cloud]. The Spring Cloud Config server can be viewed as a proxy between your services and their actual configuration, providing a number of really neat features such as:  * Support for several different configuration backends such as git  as stores. * Transparent decryption of encrypted properties. * Pluggable security * Push mechanism using git hooks / REST API and Spring Cloud Bus  to propagate changes in config files to services, making live reload of configuration possible.  For a more in-depth article about Spring Cloud Config in particular, take a look at my colleague Magnus recent [blog post].  In this blog post, we will integrate our "accountservice" with a Spring Cloud Config server backed by a public git repository on github, from which we'll fetch configuration, encrypt/decrypt a property and also implement live-reload of config properties.  Here's a simple overview of the overall solution we're aiming for:  ![configserver.png]   Since we're running Docker in Swarm mode, we'll continue using Docker mechanics in various ways. Inside the Swarm, we should run at least one  instances of Spring Cloud Configuration servers. When one of our microservices starts up, all they need to know about are the following:  - The logical service name and port of the config server. I.e - we're deploying our config servers on Docker Swarm as services, let's say we name that service "configserver". That means that is the only thing the microservices needs to know about addressing in order to make a request for its configuration. - What their name is, e.g. "accountservice" - What execution profile it is running, e.g. "dev", "test" or "prod". If you're familiar with the concept of _spring.profiles.active_, this is a home-brewn counterpart we can use for Go. - If we're using git as backend and want to fetch configuration from a particular branch, that needs to be known up front.   Given the four criteria above, a sample GET request for configuration could look like this in Go code:  resp, err := http.Get  I.e:  protocol://url:port/applicationName/profile/branch  For part 8, you'll probably want to clone branch P8 since it includes the source for the config server:  git clone https://github.com/callistaenterprise/goblog.git git checkout P8  You could probably set up and deploy the config server in other ways. However, for simplicity I've prepared a _/support_ folder in the root _/goblog_ folder of the [source code repository of the blog series] which will contain the requisite 3rd party services we'll need further on.  Typically, each required support component will either be a simple _Dockerfile_ for conveniently building and deploying components which we can use out of the box, or it will be .  _._  Let's get started with the config server, shall we?  What? Weren't we about to install Spring Cloud Configuration server? Well - that piece of software depends on having a message broker to propagate configuration changes using [Spring Cloud Bus] backed by RabbitMQ. Having RabbitMQ around is a very good thing anyway which we'll be using in a later blog post so we'll start by getting RabbitMQ up and running as a service in our Swarm.  I've prepared a [Dockerfile] inside _/goblog/support/rabbitmq_ to use a pre-baked image which we'll deploy as a Docker Swarm service.  We'll create a new bash  script to automate things for us if/when we need to update things.  In the root _/goblog_ folder, create a new file _support.sh_:  #!/bin/bash  # RabbitMQ docker service rm rabbitmq docker build -t someprefix/rabbitmq support/rabbitmq/ docker service create --name=rabbitmq --replicas=1 --network=my_network -p 1883:1883 -p 5672:5672 -p 15672:15672 someprefix/rabbitmq    Run it and wait while Docker downloads the necessary images and deploys RabbitMQ into your Swarm. When it's done, you should be able to open the RabbitMQ Admin GUI and log in using _guest/guest_ at:  open http://$ManagerIP:15672/#/  Your web browser should open and display something like this: ![rabbitmq]  If you see the RabbitMQ admin GUI, we can be fairly sure it works as advertised.  In _/support/config-server_ you'll find a Spring Boot application pre-configured to run the config server. We'll be using a [git repository] files.  Feel free to take a look at _/goblog/support/config-server/src/main/resources/application.yml_ which is the config file of the config server:  --- # For deployment in Docker containers spring: profiles: docker cloud: config: server: git: uri: https://github.com/eriklupander/go-microservice-config.git  # Home-baked keystore for encryption. Of course, a real environment wouldn't expose passwords in a blog... encrypt: key-store: location: file:/server.jks password: letmein alias: goblogkey secret: changeme  # Since we're running in Docker Swarm mode, disable Eureka Service Discovery eureka: client: enabled: false  # Spring Cloud Config requires rabbitmq, use the service name. spring.rabbitmq.host: rabbitmq spring.rabbitmq.port: 5672  We see a few things:  * We're telling the config-server to fetch configuration from our git-repo at the specified URI. * A keystore for encryption  * Since we're running in Docker Swarm mode, Eureka Service Discovery is disabled. * The config server is expecting to find a rabbitmq host at "rabbitmq" which just happens to be the Docker Swarm service name we just gave our RabbitMQ service.  The _Dockerfile_ for the config-server is quite simple:  FROM davidcaste/alpine-java-unlimited-jce  EXPOSE 8888  ADD ./build/libs/*.jar app.jar ADD ./server.jks /  ENTRYPOINT ["java","-Dspring.profiles.active=docker","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]    A few things of note here:  - We're using a base [docker image] based on Alpine Linux that has the Java unlimited cryptography extension installed, this is a requirement if we want to use the encryption/decryption features of Spring Cloud Config. - A home-baked keystore is added to the root folder of the container image.   To use encrypted properties later on, we'll configure the config server with a self-signed certificate. .  In the _/goblog/support/config-server/_ folder, run:  keytool -genkeypair -alias goblogkey -keyalg RSA \ -dname "CN=Go Blog,OU=Unit,O=Organization,L=City,S=State,C=SE" \ -keypass changeme -keystore server.jks -storepass letmein \ -validity 730  This should create _server.jks_. Feel free to modify any properties/passwords, just remember to update _application.yml_ accordingly!  Time to build and deploy the server. Let's create a shell script to save us time if or when we need to do this again. Remember - you need a Java Runtime Environment to build this! In the _/goblog_ folder, create a file named _springcloud.sh_. We will put all things that actually needs building  in there:  #!/bin/bash  cd support/config-server ./gradlew build cd ../.. docker build -t someprefix/configserver support/config-server/ docker service rm configserver docker service create --replicas 1 --name configserver -p 8888:8888 --network my_network --update-delay 10s --with-registry-auth --update-parallelism 1 someprefix/configserver  Run it from the _/goblog_ folder :  > ./springcloud.sh  This may take a while, give it a minute or two and then check if you can see it up-and-running using _docker service_:  > docker service ls  ID NAME MODE REPLICAS IMAGE 39d26cc3zeor rabbitmq replicated 1/1 someprefix/rabbitmq eu00ii1zoe76 viz replicated 1/1 manomarks/visualizer:latest q36gw6ee6wry accountservice replicated 1/1 someprefix/accountservice t105u5bw2cld quotes-service replicated 1/1 eriklupander/quotes-service:latest urrfsu262e9i dvizz replicated 1/1 eriklupander/dvizz:latest w0jo03yx79mu configserver replicated 1/1 someprefix/configserver  Try to manually load the "accountservice" configuration as JSON using curl:  > curl http://$ManagerIP:8888/accountservice/dev/master {"name":"accountservice","profiles":["dev"],"label":"master","version":"b8cfe2779e9604804e625135b96b4724ea378736", "propertySources":[ {"name":"https://github.com/eriklupander/go-microservice-config.git/accountservice-dev.yml", "source":  }] }  __  The actual configuration is stored within the "source" property where all values from the .yml file will appear as key-value pairs. Loading and parsing the "source" property into usable configuration in Go is the centerpiece of this blog post.  Before moving on to Go code, let's take a look inside the root folder of the P8 branch of the [configuration-repo]:  accountservice-dev.yml accountservice-test.yml  Both these files are currently very sparsely populated:  server_port: 6767 server_name: Accountservice TEST the_password:   The only thing we're configuring at this point is the HTTP port we want our service to bind to. A real service will probably have a lot more stuff in it.  One really neat thing about Spring Cloud Config is its built-in support for transparently decrypting values encrypted directly in the configuration files. For example, take a look at [accountservice-test.yml] where we have a dummy "the_password" property:  server_port: 6767 server_name: Accountservice TEST the_password: 'AQB1BMFCu5UsCcTWUwEQt293nPq0ElEFHHp5B2SZY8m4kUzzqxOFsMXHaH7SThNNjOUDGxRVkpPZEkdgo6aJFSPRzVF04SXOVZ6Rjg6hml1SAkLy/k1R/E0wp0RrgySbgh9nNEbhzqJz8OgaDvRdHO5VxzZGx8uj5KN+x6nrQobbIv6xTyVj9CSqJ/Btf/u1T8/OJ54vHwi5h1gSvdox67teta0vdpin2aSKKZ6w5LyQocRJbONUuHyP5roCONw0pklP+2zhrMCy0mXhCJSnjoHvqazmPRUkyGcjcY3LHjd39S2eoyDmyz944TKheI6rWtCfozLcIr/wAZwOTD5sIuA9q8a9nG2GppclGK7X649aYQynL+RUy1q7T7FbW/TzSBg='  By prefixing the encrypted string with __, our Spring Cloud configuration server will know how to automatically decrypt the value for us before passing the result to the service. In a running instance with everything configured correctly, a curl request to the REST API to fetch this config would return:  ... "source": { "server_port": 6767, "server_name": "Accountservice TEST", "the_password": "password" ....  Pretty neat, right? The "the_password" property can be stored as clear-text encrypted string on a public server  transparently decrypts the property into actual value 'password'.  Of course, you need to encrypt the value using the same key as Spring Cloud Config is using for decryption, something that can be done over the config server's HTTP API:  curl http://$ManagerIP:8888/encrypt -d 'password' AQClKEMzqsGiVpKx+Vx6vz+7ww00n...   Our Go-based configuration framework of choice is [Viper] functions. Very convenient, indeed.  Remember the picture at the top of this blog post? Well, if not - here it is again:  ![configserver.png]  We'll make our microservices do an HTTP request on start, extract the "source" part of the JSON response and stuff that into Viper so we can get the HTTP port for our web server there. Let's go!  As already demonstrated using curl, we can do a plain HTTP request to the config server where we just need to know our name and our "profile". We'll start by adding some parsing of flags to our "accountservice" _main.go_ so we can specify an environment "profile" when starting as well as an optional URI to the config server:  var appName = "accountservice"  // Init function, runs before main func init { // Read command line flags profile := flag.String configServerUrl := flag.String configBranch := flag.String flag.Parse  // Pass the flag values into viper. viper.Set viper.Set viper.Set }  func main { fmt.Printf  // NEW - load the config config.LoadConfigurationFromBranch( viper.GetString, appName, viper.GetString, viper.GetString initializeBoltClient service.StartWebServer // NEW, use port from loaded config }   The _config.LoadConfigurationFromBranch_ function goes into a new package we're calling _config_. Create _/goblog/accountservice/config_ and the following file named _loader.go_:  // Loads config from for example http://configserver:8888/accountservice/test/P8 func LoadConfigurationFromBranch { url := fmt.Sprintf fmt.Printf body, err := fetchConfiguration if err != nil { panic } parseConfiguration }  // Make HTTP request to fetch configuration from config server func fetchConfiguration { resp, err := http.Get if err != nil { panic } body, err := ioutil.ReadAll return body, err }  // Pass JSON bytes into struct and then into Viper func parseConfiguration { var cloudConfig springCloudConfig err := json.Unmarshal if err != nil { panic }  for key, value := range cloudConfig.PropertySources[0].Source { viper.Set fmt.Printf } if viper.IsSet { fmt.Printf } }  // Structs having same structure as response from Spring Cloud Config type springCloudConfig struct { Name string  Profiles []string  Label string  Version string  PropertySources []propertySource  }  type propertySource struct { Name string  Source map[string]interface  }  Basically, we're doing that HTTP GET to the config server with our appName, profile and git branch, then unmarshalling the response JSON into the _springCloudConfig_ struct we're declaring in the same file. Finally, we're simply iterating over all the key-value pairs in the _cloudConfig.PropertySources[0]_ and stuffing each pair into viper so we can access them whenever we want using _viper.GetString_ or another of the typed getters the Viper API provides.  Note that if we have an issue contacting the configuration server or parsing its response, we panic the entire microservice which will kill it. Docker Swarm will detect this and try to deploy a new instance in a few seconds. The typical reason for a behaviour such as this is when starting your cluster from cold and the Go-based microservice will start much faster than the Spring Boot-based config server does. Let Swarm retry a few times and things should sort themselves out.  We've split the actual work up into one public function and a few package-scoped ones for easier unit testing. The unit test for checking so we can transform JSON into actual viper properties looks like this using the GoConvey style of tests:  func TestParseConfiguration {  Convey { var body =   Convey { parseConfiguration  Convey { So }) }) }) }  Run from _goblog/accountservice_ if you want to:  > go test ./...   Given that we're loading the configuration from an external source, our service needs a hint about where to find it. That's performed by using flags as command-line arguments when starting the container and service:  _goblog/accountservice/Dockerfile_:  FROM iron/base EXPOSE 6767  ADD accountservice-linux-amd64 / ADD healthchecker-linux-amd64 /  HEALTHCHECK --interval=3s --timeout=3s CMD ["./healthchecker-linux-amd64", "-port=6767"] || exit 1 ENTRYPOINT ["./accountservice-linux-amd64", "-configServerUrl=http://configserver:8888", "-profile=test", "-configBranch=P8"]  Our ENTRYPOINT now supplies values making it possible to configure from where to load configuration.     You probably noted that we're not using 6767 as a hard-coded port number anymore, i.e:  service.StartWebServer  Use the _copyall.sh_ script to build and redeploy the updated "accountservice" into Docker Swarm  > ./copyall.sh  After everything's finished, the service should still be running exactly as it did before you started on this part of the blog series, with the exception that it actually picked its HTTP port from an external and centralized configuration server rather than being hard-coded into the compiled binary.  __  Let's take a look at the log output of our accountservice:  > docker logs -f [containerid] Starting accountservice Loading config from http://configserver:8888/accountservice/test/P8 Loading config property the_password => password Loading config property server_port => 6767 Loading config property server_name => Accountservice TEST Successfully loaded configuration for service Accountservice TEST  __    - "Oh, did that external service we're using for [some purpose] change their URL?" - "Darn. None told us!!"  I assume many of us have encountered situations where we need to either rebuild an entire application or at least restart it to update some invalid or changed configuration value. Spring Cloud has the concept of [@RefreshScope].  This figure provides an overview of how a push to a git repo is propagated to our Go-based microservices:  ![/assets/blogg/goblog/part8-springcloudpush.png]  In this blog post, we're using a github repo which has absolutely no way of knowing how to perform a post-commit hook operation to my laptop's Spring Cloud server, so we'll emulate a commit hook push using the built-in _/monitor_ endpoint of our Spring Cloud Config server.  curl -H "X-Github-Event: push" -H "Content-Type: application/json" -X POST -d '' -ki http://$ManagerIP:8888/monitor  The Spring Cloud Config server will know what to do with this POST and send out a _RefreshRemoteApplicationEvent_ on an [exchange]. If we take a look at the RabbitMQ admin GUI after having booted Spring Cloud Config successfully, that _exchange_ should have been created:  ![Exchange name]  How does an _exchange_ relate to more traditional messaging constructs such as publisher, consumer and queue?  Publisher -> Exchange ->  -> Queue -> Consumer  I.e - a message is published to an _exchange_, which then distributes message copies to _queue_ based on _routing_ rules and bindings which may have registered _consumers_.  So in order to consume _RefreshRemoteApplicationEvent_ messages , all we have to do now is make our Go service listen for such messages on the _springCloudBus_ exchange and if we are the targeted application, perform a configuration reload. Let's do that.  The RabbitMQ broker can be accessed using the AMQP protocol. There's a good Go AMQP client we're going to use called [streadway/amqp]. Most of the AMQP / RabbitMQ plumbing code should go into some reusable utility, perhaps we'll refactor that later on. The plumbing code is based on [this example] from the streadway/amqp repo.  In _/goblog/accountservice/main.go_, add a new line inside the _main_ function that will start an AMQP consumer for us:  func main { fmt.Printf  config.LoadConfigurationFromBranch( viper.GetString, appName, viper.GetString, viper.GetString initializeBoltClient  // NEW go config.StartListener service.StartWebServer }  Note the new _amqp_server_url_ and _config_event_bus_ properties, they're loaded from the [_accountservice-test.yml] configuration file we're loading.  The _StartListener_ function goes into a new file _/goblog/accountservice/config/events.go_. This file has a _lot_ of AMQP boilerplate which we'll skip so we concentrate on the interesting parts:  func StartListener { err := NewConsumer if err != nil { log.Fatalf }  log.Printf select  // Yet another way to stop a Goroutine from finishing... }  The NewConsumer function is where all the boilerplate goes. We'll skip down to the code that actually processes an incoming message:  func handleRefreshEvent { updateToken := &UpdateToken err := json.Unmarshal if err != nil { log.Printf } else { if strings.Contains { log.Println  // Consumertag is same as application name. LoadConfigurationFromBranch( viper.GetString, consumerTag, viper.GetString, viper.GetString } } }  type UpdateToken struct { Type string  Timestamp int  OriginService string  DestinationService string  Id string  }  This code tries to parse the inbound message into an _UpdateToken_ struct and if the destinationService matches our consumerTag , we'll call the same _LoadConfigurationFromBranch_ function initially called when the service started.  Please note that in a real-life scenario, the _NewConsumer_ function and general message handling code would need more work with error handling, making sure only the appropriate messages are processed etc.   Let's write a unit test for the _handleRefreshEvent_ function. Create a new test file _/goblog/accountservice/config/events_test.go_:  var SERVICE_NAME = "accountservice"  func TestHandleRefreshEvent { // Configure initial viper values viper.Set viper.Set viper.Set  // Mock the expected outgoing request for new config defer gock.Off gock.New. Get. Reply. BodyString  Convey { var body = ` ` Convey { handleRefreshEvent  Convey { So }) }) }) }  I hope the BDD-style of GoConvey conveys  how the test works. Note though how we use _gock_ to intercept the outgoing HTTP request for new configuration and that we pre-populate viper with some initial values.  Time to test this. Redeploy using our trusty _copyall.sh_ script:  > ./copyall.sh  Check the log of the _accountservice_:  > docker logs -f [containerid] Starting accountservice ... [truncated for brevity] ... Successfully loaded configuration for service Accountservice TEST <-- LOOK HERE!!!! ... [truncated for brevity] ... 2017/05/12 12:06:36 dialing amqp://guest:guest@rabbitmq:5672/ 2017/05/12 12:06:36 got Connection, getting Channel 2017/05/12 12:06:36 got Channel, declaring Exchange  2017/05/12 12:06:36 declared Exchange, declaring Queue  2017/05/12 12:06:36 declared Queue  2017/05/12 12:06:36 Queue bound to Exchange, starting Consume  2017/05/12 12:06:36 running forever  Now, we'll make a change to the _accountservice-test.yml_ file on my git repo and then fake a commit hook using the _/monitor_ API POST shown earlier in this blog post:  I'm changing _accountservice-test.yml_ and its _service_name_ property, from _Accountservice TEST_ to _Temporary test string!_ and pushing the change.  Next, use curl to let our Spring Cloud Config server know about the update:  > curl -H "X-Github-Event: push" -H "Content-Type: application/json" -X POST -d '' -ki http://192.168.99.100:8888/monitor  If everything works, this should trigger a _refresh token_ from the Config server which our _accountservice_ picks up. Check the log again:  > docker logs -f [containerid] 2017/05/12 12:13:22 got 195B consumer: [accountservice] delivery: [1] routingkey: [springCloudBus]  2017/05/12 12:13:22 Reloading Viper config from Spring Cloud Config server Loading config from http://configserver:8888/accountservice/test/P8 Loading config property server_port => 6767 Loading config property server_name => Temporary test string! Loading config property amqp_server_url => amqp://guest:guest@rabbitmq:5672/ Loading config property config_event_bus => springCloudBus Loading config property the_password => password Successfully loaded configuration for service Temporary test string! <-- LOOK HERE!!!!  As you can see, the final line now prints _"Successfully loaded configuration for service Temporary test string!"_. The source code for that line:  if viper.IsSet { fmt.Printf }  I.e - we've dynamically changed a property value previously stored in Viper during runtime without touching our service! This IS really cool!!  **Important note:** While updating properties dynamically is very cool, that in itself won't update things like the port of our _running_ web server, existing Connection objects in pools or  the active connection to the RabbitMQ broker. Those kinds of "already-running" things takes a lot more care to restart with new config values and is out of scope for this particular blog post.  __  Adding loading of configuration at startup shouldn't affect runtime performance at all and it doesn't. 1K req/s yields the same latencies, CPU & memory use as before. Just take my word for it or try yourself. We'll just take quick peek at memory use after first startup:  CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS accountservice.1.pi7wt0wmh2quwm8kcw4e82ay4 0.02% 4.102MiB / 1.955GiB 0.20% 18.8kB / 16.5kB 0B / 1.92MB 6 configserver.1.3joav3m6we6oimg28879gii79 0.13% 568.7MiB / 1.955GiB 28.41% 171kB / 130kB 72.9MB / 225kB 50 rabbitmq.1.kfmtsqp5fnw576btraq19qel9 0.19% 125.5MiB / 1.955GiB 6.27% 6.2MB / 5.18MB 31MB / 414kB 75 quotes-service.1.q81deqxl50n3xmj0gw29mp7jy 0.05% 340.1MiB / 1.955GiB 16.99% 2.97kB / 0B 48.1MB / 0B 30  Even with AMQP integration and Viper as configuration framework, we have an initial footprint of ~4 mb. Our Spring Boot-based _config server_ uses over 500 mb of RAM while RabbitMQ  uses 125 mb.  I'm fairly certain we can starve the config server down to 256 mb initial heap size using some standard JVM -xmx args but it's nevertheless definitely a lot of RAM. However, in a production environment I would expect us running ~2 config server instances, not tens or hundreds. When it comes to the supporting services from the Spring Cloud ecosystem, memory use isn't such a big deal as we usually won't have more than one or a few instances of any such service.  In this part of the Go microservices [blog series] we deployed a Spring Cloud Config server and its RabbitMQ dependency into our Swarm. Then, we wrote a bit of Go code that using plain HTTP, JSON and the Viper framework loads config from the config server on startup and feeds it into Viper for convenient access throughout our microservice codebase.  In the [next part], we'll continue to explore AMQP and RabbitMQ, going into more detail and take a look at sending some messages ourselves.
 Groovy and Grails support have long been a sad story in Eclipse. Most notable, running and debugging Grails Unit tests in Eclipse has been quite painful, partly due to the fact that the Groovy eclipse plugin didn't recognize the tests as being Unit tests .  With the [alpha release of V2 of the Groovy plugin], things are getting better. Most of the classpath related problems are resolved, and Groovy Unit test classes are now correctly recognized. This means Grails unit tests can now be executed directly in the IDE, without the need to run a grails command.  Some flaws with Grails Unit tests still exist, however: The Eclipse Groovy plugin is not aware of many of the Grails conventions . This means that the "magic" in terms of meta-programming that Grails performs on e.g. Entities is not done when executed directly in Eclipse, which cause most of the superb Grails mocking support to fail. Consider for instance the simple Employee entity below:   In a Unit test for the Employee, the constraint on  can be tested by mocking the Entity class:   This test runs as expected via the Grails command, but if executed directly within Eclipse, the Employee class is just an ordinary Groovy class that hasn't been meta-programmed by Grails, and the test fails with the following exception:   There are ways around these problems. Starting with Groovy 1.6, [**AST Transformations**] allow meta-programming in compile time. A local AST transformation is triggered by an Annotation on the class to be transformed, and the compiler automatically applies the transformation. Most Grails meta-programming enhancements are also available as AST annotations.  Hence if we mark the Employee entity with the  annotation  mechanism will work again.   One little caveat, though: The Eclipse Groovy plugin must be configured to use the correct classpath when doing AST transformations. Otherwise, it refuses to compile the annotated class, giving the following error:   By adding a file  with the contents as seen below to the root folder of the eclipse project, the Groovy compiler is configured to use the same classpath as the project : 
In this part of the Go microservices [blog series] O/R-mapper.  1. Overview 2. The CAP theorem 3. CockroachDB 4. Installing CockroachDB 5. The new "Dataservice" with GORM 6. Running and testing an endpoint 7. Load test and resilience 8. Summary   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P13  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  Data consistency vs availability in distributed systems is a very interesting topic. These days, traditional [ACID] often forms the basis of persistence in distributed microservice architectures.  Bounded contexts and eventual consistency can somewhat simplified be explained as:  - Bounded contexts are a central pattern in Domain-driven design, which is a very useful pattern when designing microservice architectures. For example - if you have an "Accounts" microservice and an "Orders" microservice, they should own their own data  in separate databases _without_ old-school foreign key constraints between them. Each microservice is solely responsible for writing and reading data from its own domain. If the "orders" microservice needs to know about the owning "account" for a given "order", the "orders" microservice must ask the "account" microservice for account data - the "orders" microservice may _not_ under any circumstance query or write directly to the tables or document stores of the "account" microservice. - Eventual consistency can be several things. It's primarily the concept of a data replication mechanism where a given data write will _eventually_ be replicated across the distributed storage system so any given read will yield the latest version of the data. One can also consider it a requisite of the bounded context pattern, e.g. for a "business transaction" write that appears atomic to an outside viewer, many microservices may be involved in writing data across several bounded contexts without any distributed mechanisms guaranteeing a global ACID transaction. Instead, _eventually_ all involved microservices will have performed their writes, resulting in a consistent state across the distributed system from the perspective of the business transaction. See a good comparison of ACID and BASE [here].  These days, many people turn to the NoSQL database [Apache Cassandra] when they require horizontally scalable data storage with automatic replication and eventual consistency. However, I'm a bit curious how a cutting edge "SQL" database such as CockroachDB works in our microservice context, so that'll be the focus of this blog post.  First, a few words about the [CAP theorem].  CAP is a three-letter acronymn for database systems that claims that no distributed database may ever fulfill all these three criterias at any one time:  - Consistent: A read is guaranteed to return the _most recent_ write. - Available: The choice is always between serving the data you have even though you can't guarantee that is the most recent version of it  OR you must deny serving data if you're not absolutely sure there's no inconsistent state of the requested data anywhere in the cluster. - Partition tolerant: If a database server goes down, the remaining nodes must continue to function and when the failed node recovers, consistent data must still be served.  A distributed database may only choose two of above, making them either "CAP-Available" . The main advantage of an AP database is better latencies since CP databases must coordinate writes and reads across nodes, while an AP system is allowed to possibly return inconsistent or missing data which is faster. In other words - AP databases favor speed while CP databases favors robustness.  Do note that it's fully possible to run a CAP-capable distributed database as long as there are no network or other problems. The problem is that there's always going to be network problems at some point, see [the fallacies of distributed computing]. This is especially relevant for microservices given that we're typically leaving the monolithic database of your enterprise behind, instead letting each microservice "own" their own domain of data - sometimes split over many databases, possibly even across multiple data centers.  CockroachDB is a CAP Consistent  from cockroachlabs.  CockroachDB was created by ex-Google employees that used to work on Google's [Cloud Spanner]. CockroachDB do - as prevoiusly stated - not claim to be a CAP-database, but claims full C and P, and a significant number of 9's for availability.  At it's core, CockroachDB is a distributed key-value store written in Go, but differs from its peers by having an ANSI-compliant SQL interface, behaving like a relational database in most, if not all, aspects. The authors are very transparent about CockroachDB still having some issues making it unsuitable for OLAP-like workloads. Essentially, JOIN operations are continuously being optimized but they still have quite a way to go until the JOIN performance is on par with old-school databases.  ![cockroachdb overview] _Source: Cockroachlabs_  A CockroachDB cluster _always_ consists of at least three database nodes, where the database will stay 100% operational if one node goes down. The underlying replication engine always makes sure any entry exists on at least two nodes with auto-replication if a node goes down. We'll get back to this claimed resilience a bit later where we'll stress test things while taking down a DB node, should be a fun exercise!  Time to get this database installed and up and running in our cluster. We're going to pull v1.1.3 directly from Docker Hub and start three nodes, each running one instance of CockroachDB on separate ports. Since each node needs it's own mounted storage we cannot  run three _instances_ of a CockroachDB _docker swarm mode service_, we need three separate services.  For development purposes, this is actually very easy. I've prepared a bash-script to set this up:  #!/bin/bash  # CoachroachDB master, will publish admin GUI at 3030, mapped from 8080 docker service rm cockroachdb1 docker service create --name=cockroachdb1 --network=my_network -p 26257:26257 -p 3030:8080 --mount type=volume,source=cockroach-data1,target=/cockroach/cockroach-data cockroachdb/cockroach:v1.1.3 start --insecure  # CoachroachDB docker service rm cockroachdb2 docker service create --name=cockroachdb2 --network=my_network --mount type=volume,source=cockroach-data2,target=/cockroach/cockroach-data cockroachdb/cockroach:v1.1.3 start --insecure --join=cockroachdb1  # CoachroachDB docker service rm cockroachdb3 docker service create --name=cockroachdb3 --network=my_network --mount type=volume,source=cockroach-data3,target=/cockroach/cockroach-data cockroachdb/cockroach:v1.1.3 start --insecure --join=cockroachdb1  Let's dissect the first _docker service create_ a bit:  - Ports: We're publishing port 26257 which actually isn't necessary unless we want to try to connect to the cluster from the outside. We're also mapping the admin GUI locally at port 8080 to port 3030. - Volume mounts. CockroachDB requires some persistent storage, so we're mounting a local folder as persistent storage using the _--mount_ flag. - Start command: _start --insecure_. We're supplying a _start_ command and the _--insecure_ argument  in order to run a local cluster without setting up certificates. Also note the _--join=cockroachdb1_ argument passed to the two "workers" telling them to form a cluster with their leader.  Startup may take a few minutes, after which the green and pleasant admin GUI should be available in your favorite browser at http://192.168.99.100:3030:  ![Overview] _The overview_  ![Nodes list] _List of server nodes_  Nice! Now we're ready to create some databases and users. For more details, please check the rich [documentation].  We're going to use the [built-in] SQL client to create two databases and a two users - one for each of our bounded contexts. Since our CockroachDB instances are running in Docker Containers, we can't use _cockroach sql_ directly. We must do it by connecting to a running container using a bit of docker wizardry:  > docker ps  CONTAINER ID IMAGE COMMAND 10f4b6c727f8 cockroachdb/cockroach:v1.1.3 "/cockroach/cockro..."  Find a container running the _cockroachdb/cockroach_ container and note the container ID. Then we'll use _docker exec_ to launch the SQL CLI:  > docker exec -it 10f4b6c727f8 ./cockroach sql --insecure  # Welcome to the cockroach SQL interface. # All statements must be terminated by a semicolon. # To exit: CTRL + D. # # Server version: CockroachDB CCL v1.1.3  # Cluster ID: 5c317c3e-5784-4d8f-8478-ec629d8a920d # # Enter \? for a brief introduction. # root@:26257/>  We're in!  I've prepared a .sql file whose contents we easily can copy-paste directly into the console. This is a one-time job for the purpose of this particular blog post. In a real-life scenario you'd obviously script this using some build automation tool.  CREATE DATABASE account; CREATE DATABASE image;  CREATE USER account_user WITH PASSWORD 'account_password'; CREATE USER image_user WITH PASSWORD 'image_password';  GRANT ALL ON DATABASE account TO account_user; GRANT ALL ON DATABASE image TO image_user;  Done! Now on to the wonderful world of Go and O/R-mapping!   First let's start with a brand new overview of what the microservice landscape will look like once this part is done:  ![New landscape overview]  Key stuff:  - The BoltDB is gone from the _accountservice_. - The new "dataservice" will access _accounts_ and _account events_ stored in a CockroachDB database named "account". - The existing "imageservice" will now store _image urls_ in another CockroachDB database named "image".  - The two databases above are both hosted in the three-node CockroachDB cluster. Data may exist on any two of three server nodes. - The _accountservice_ used to both act as a service aggregator AND account storage. It's purpose is now strictly orchestrating the fetching of account objects by talking to the Go-based "imageservice" and "dataservice" as well as the Java-based "quotes-service" and then aggregating them to a unified response. - The communication from our microservices to the CockroachDB cluster uses the [postgresql wire protocol].  GORM is an "[object-relational mapper] or similar, although perhaps not as mature or fully-featured. Still - > 7000 stars on github and over 120 contributors gives an indication of a well-liked and commonly used library.  CockroachDB uses the [postgresql wire protocol].  What about tables where we'll store and retrieve actual data? In this particular blog, we'll utilize the [AutoMigrate] to create our tables.  The AutoMigrate feature introspects Go structs with 'gorm'-tags and automatically creates tables and columns given these structs. Let's take a closer look how we declare primary keys, foreign keys and an index directly on the structs by using gorm [tags].  type AccountData struct { ID string `json:"" gorm:"primary_key"` Name string  AccountEvents []AccountEvent `json:"events" gorm:"ForeignKey:AccountID"` }  type AccountEvent struct { ID string `json:"" gorm:"primary_key"` AccountID string `json:"-" gorm:"index"` EventName string  Created string  }  Most of the GORM tags should be self-explanatory for people vaguely familiar with relational databases - e.g. "primary_key", "index" etc.  The _AccountData_ struct has a [has-many] that'll be served by our new _dataservice_. The "image data" - consisting of a single _AccountImage_ struct, will belong to its own bounded context and be served from the _imageservice_ microservice.  The generated tables looks like this from the CockroachDB GUI:  ![tables tables]  __  Dealing with Gorm requires surprisingly little boilerplate on the structs, but working with its DSL for querying and mutating data may take a little while getting used to. Let's take a look at a few basic use cases:  All interactions with the GORM API in these examples happen through "gc.crDB" which is my wrapping of a pointer to [gorm.DB], i.e:  type GormClient struct { crDB *gorm.DB }  var gc &GormClient  Below, we're opening the connection using _postgres_ SQL dialect and then calling the _AutoMigrate_ function to create tables.  var err error gc.crDB, err = gorm.Open // Addr is supplied from config server, of course if err != nil { panic }  // Migrate the schema gc.crDB.AutoMigrate // Note that we pass the structs we want tables for.   // Create an instance of our Account struct acc := model.AccountData{ ID: key, // A pre-generated string id Name: randomPersonName, // Some person name Events: accountEvents, // slice of AccountEvents }  gc.crDB.Create // Persist!  The code above will write both a row to the ACCOUNT_DATA table as well as any ACCOUNT_EVENT rows present in the Events slice, including foreign keys. Using the SQL client, we can try a standard JOIN:  root@:26257> use account; root@:26257/account> SELECT * FROM account_data AS ad INNER JOIN account_events AS ae ON ae.account_id = ad.id WHERE ad.id='10000'; +-------+----------+--------------------+------------+------------+---------------------+ | id | name | id | account_id | event_name | created | +-------+----------+--------------------+------------+------------+---------------------+ | 10000 | Person_0 | accountEvent-10000 | 10000 | CREATED | 2017-12-22T21:38:21 | +-------+----------+--------------------+------------+------------+---------------------+   We're seeding one AccountEvent per AccountData so the result is absolutely right!  It's of course possible to use the postgres driver and do standard SQL queries like the one above. However, to leverage GORM appropriately, we'll use the [query DSL] of GORM.  Here's an example where we load an AccountData instance by ID, [eagerly loading] any AccountEvents related to it.  func  { acc := model.AccountData // Create empty struct to store result in gc.crDB.Preload // Use the Preload to eagerly fetch events for // the account. Note use of ID = ? if acc.ID == "" { // Not found handling... return acc, fmt.Errorf } return acc, nil // Return populated struct. }  A more complex example - find all AccountData instances having a person whose name starts with 'Person_8' and count the number of AccountEvents for each entry.  func  {  rows, err := gc.crDB.Table. // Specify table including alias Select. // Select columns including count, see Group by Joins. // Do a JOIN Where. // Add a where clause Group // Group by name .Rows to execute the query  result := make // Create slice for result for rows.Next { // Iterate over returned rows pair := Pair // Pair is just a simple local struct rows.Scan // Pass result into struct fields result = append // Add resulting pair into slice } return result, err // Return slice with pairs. }  Note the fluent DSL with Select..Joins..Where..Group which is surprisingly pleasant to work with once you get used to it. Should be familiar if you've worked with similar APIs in the past such as [JOOQ]  Calling an endpoint exposing the query above yields:  [{ "Name": "Person_80", "Count": 3 }, { "Name": "Person_81", "Count": 6 }]  _Tidied up the response JSON for the sake of readability_  Regrettably, there doesn't seem to be an idiomatic and super-simple way to unit-test GORM interactions with the database. Some strategies do however exist, such as:  - Using [go-sqlite3] to boot a real light-weight database in unit tests. - Using [go-sqlmock]. - Using [go-testdb].  In all honesty, I havn't really examined any of the options above closely. Instead, I've wrapped the GORM db struct in a struct of my own, which implicitly implements this interface:  type IGormClient interface { QueryAccount QueryAccountByNameWithCount SetupDB SeedAccounts error Check bool Close }  Having an interface makes it very straightforward to use [testify/mock] to mock any interaction with methods on the struct wrapping the GORM db object.  If you've cloned the source and have installed CockroachDB, you can execute the _./copyall.sh_ script to build and deploy the updated microservices:  - accountservice - imageservice - dataservice  - vipservice  The configuration has been updated, including [.yaml-files] for the new "dataservice".  Once we're up and running, let's do a curl request to the "accountservice" _/accounts/_ endpoint:  > curl http://192.168.99.100:6767/accounts/10002 -k | json_pp { "imageData" : { "id" : "10002", "servedBy" : "10.0.0.26", "url" : "http://path.to.some.image/10002.png" }, "id" : "10002", "servedBy" : "10.0.0.3", "name" : "Person_2", "accountEvents" : [ { "ID" : "accountEvent-10002", "created" : "2017-12-22T22:31:06", "eventName" : "CREATED" } ], "quote" : { "ipAddress" : "eecd94253fcc/10.0.0.18:8080", "quote" : "To be or not to be", "language" : "en" } }  Looks good to me!  Let's get down to the business of testing whether our setup with CockroachDB is Consistent and Partition Tolerant, while providing acceptable levels of Availability.  Load- and resilience testing a microservice landscape with a distributed data store such as CockroachDB on a laptop running everything in virtualbox isn't that realistic perhaps, but should at least provide some insights.  For this purpose, I'm going to set up a landscape with the following characteristics:  - We'll bypass our EDGE server. We'll call the accountservice directly to remove TLS overhead for this particular test case. - 1 instance of the _accountservice_, _imageservice_, _dataservice_ respectively. - 2 instances of the _quotes-service_. - 3 CockroachDB instances each running as a Docker Swarm mode service.   I've pre-seeded the "account" database with about 15000 records, including at least one "account_event" per "account". First test runs a [gatling] that bombs away at the _/accounts/_ microservice to fetch our account objects with a peak rate of 50 req/s.  The test runs for 75 seconds with a 5 second ramp-up time.  ![gatling report 1] _Figure 7.1.1: Latencies _  Overall latencies are just fine, our microservices and the CockroachDB have no issue whatsoever handling ~50 req/s.  __  During the second run at approx. 20:10:00 in the test, I'm deliberately killing the "cockroachdb3" service. At 20:10:30, I restart the "cockroachdb3" service.  ![gatling report 2] _Figure 7.1.2.1: Service response time _  Killing one of the three cockroachdb nodes and restarting it ~30 seconds later has the following effects:  - No requests fail. This is probably a combination of the CockroachDB master quickly stopping handing over queries to the unavailable node as well as the retrier logic in our microservice which makes sure a failed call from the _accountservice_ to the _dataservice_ is retried 100 ms later. - Taking down the node just before 20:10:00 _probably_ causes the small latency spike at ~20:09:57_, though I'd say it is a very manageable little spike end-users probably wouldn't notice unless this was some kind of near-realtime trading platform or similar. - The larger much more noticable spike actually happens when the "cockroachdb3" node comes back up again. My best guess here is that the cockroachdb cluster spends some CPU time and possibly blocks operations when the node re-joins the cluster making sure it's put into a synchronized state or similar. - The mean service latency increased from 33 in run #1 to 39 in run #2, which indicates that while the "spike" at 20:10:30 is noticable, it affects relatively few requests as a whole causing just a slight adverse effect on the overall latencies of the test run.  We can look at the same scenarios from the perspective of the CockroachDB GUI where we can examine a plethora of different metrics.  In the graphs below, we see both scenarios in each graph - i.e. we first run the Gatling test _without_ taking down a CockroachDB instance, while we do the same "kill and revive"-scenario a minute later.  ![cockroachdb1] _Figure 7.1.3.1: CockroachDB queries per second over the last 10 seconds_  ![cockroachdb2] _Figure 7.1.3.2: CockroachDB 99th percentile latency over the last minute_  ![cockroachdb3] _Figure 7.1.3.3: CockroachDB live node count_  The graphs from CockroachDB are pretty consistent with what we saw in the Gatling tests - _taking down_ a CockroachDB node has hardly any noticable effect on latencies or availabilty, while _taking up_ a node actually has a rather severe - though short-lived - effect on the system.  A typical snapshot of Docker Swarm mode manager node CPU and memory utilization for a number of running containers during the first test:  CONTAINER CPU % MEM USAGE / LIMIT cockroachdb1.1.jerstedhcv8pc7a3ec3ck9th5 33.46% 207.9MiB / 7.789GiB cockroachdb2.1.pkhk6dn93fyr14dp8mpqwkpcx 1.30% 148.3MiB / 7.789GiB cockroachdb3.1.2ek4eunib4horzte5l1utacc0 10.94% 193.1MiB / 7.789GiB dataservice.1.p342v6rp7vn79qsn3dyzx0mq6 8.41% 10.52MiB / 7.789GiB imageservice.1.o7odce6gaxet5zxrpme8oo8pr 9.81% 11.5MiB / 7.789GiB accountservice.1.isajx2vrkgyn6qm50ntd2adja 17.44% 15.98MiB / 7.789GiB quotes-service.2.yi0n6088226dafum8djz6u3rf 7.03% 264.5MiB / 7.789GiB quotes-service.1.5zrjagriq6hfwom6uydlofkx1 10.16% 250.7MiB / 7.789GiB  We see that the master CockroachDB instance .  Another note is that our Go microservices - especially the "accountservice" - is using a substantial amount of CPU serving the load - in a more real-life scenario we would almost certainly have scaled the accountservice to several worker nodes as well. On a positive note - our Go-based microservices are still using very little RAM.  This test case will _write_ random account objects through a new POST API in the _accountservice_ to the databases while simultaneously performing a lot of reads. We'll observe behaviour as we put the system under moderate  load and finally see what happens when we pull the plug from one, then another, of the CockroachDB instances just like in 7.1.2 above.  This load-test that writes/reads things concurrently and acts upon newly created data is written in a simple Go [program]. We'll observe the behaviour by looking at the graphs in the CockroachDB admin GUI.  ![concurrent 1] _Figure 7.2.1: Queries per second and 99th percentile_  ![concurrent 2] _Figure 7.2.2: Node count_  ![concurrent 3] _Figure 7.2.3: Replicas_  What can we make of the above?  - CoachroachDB and our microservices seems to handle taking down and then up nodes during a read/write load quite well. - The main noticable latency spike we see happens at 10:18:30 in the timeline when we bring "cockroachdb3" back up. - Again - taking _down_ nodes are handled really well. - Taking _up_ "cockroachdb2" at 10:15:30 was hardly noticable, while taking up "cockroachdb3" at 10:18:30 affected latencies much more. This is - as previously stated - probably related to how CockroachDB distributes data and queries amongst cluster members. For example - perhaps the ~500 records written per minute while a node were down is automatically replicated to the node that was unavailable when it comes back up.  As you just saw, our cluster can handle when a CockroachDB worker node goes down, providing seamless balancing and failover mechanisms. The problem is that if we kill "cockroachdb1", things comes abruptly to a halt. This stems from the fact that our CoackroachDB cluster is running as three _separate_ Docker Swarm mode services - each having their own unique "cockroachdb1", "cockroachdb2" and "cockroachdb3" service name. Our _dataservice_ only knows about this connection URL:  postgresql://account_user:account_password@cockroachdb1:26257/account ^ HERE! ^  so if the service named "cockroachdb1" goes down, we're in deep s--t. The setup with three separate Docker Swarm mode services is by the way the [official] way to run CockroachDB on Docker Swarm mode.  Ideally, our "dataservice" should only need to know about a single "cockroachdb" service, but at this point I havn't figured out how to run three _replicas_ of a CockroachDB service which would make them a single adressable entity. The main issue seems to be mounting _separate_ persistent storage volumes for _each_ replica, but there may be other issues.  Anyway - my interrim **hacky** solution would probably be based around the concept of client-side load balancing , where our _dataservice_ would have to become Docker API-aware and use the Docker Remote API to get and maintain a list of IP-addresses for containers having a given label.  If we add _--label cockroachdb_ to our _docker service create_ commands, we could then apply a filter predicate for that label to a "list services" Docker API call in order to get all running CockroachDB instances. Then, it'll be straightforward to implement a simple round-robin client-side load balancing mechanism rotating connection instance to the CockroachDB nodes including circuit-breaking and housekeeping.  ![part13 - client side load balancer] _Figure 7.3_  I'd consider the above solution a hack, I'd much rather figure out how to run CockroachDB instances using replicas. Also - do note that running production databases inside containers with mounted storage is kind of [frowned upon] anyway, so in a production scenario you'd probably want to use a dedicated DB cluster anyway.  In this part of the blog series, we've added a "dataservice" that works with the CockroachDB database well suited to distributed operation, also using the Gorm O/R-mapper for Go for mapping our Go structs to SQL and back. While we've only scratched the surface of the capabilities of CockroachDB, our simple tests seems to indicate an open-source database that might be a really interesting candidate for systems that needs a SQL/ACID-capable relational database with horizontal scalability, consistency and high availability.  The [next part] ~~should deal with an issue that actually should be one of the first things to incorporate in a sound software architecture - security~~~ adds support for querying accounts using GraphQL. We'll get to security - promise!  Please help spread the word! Feel free to share this blog post using your favorite social media platform, there's some icons below to get you started.  Until next time,  // Erik 
Apache Avro, a Serialization framework originating from Hadoop, is rapidly becoming a popular choice for general Java Object Serialization in Apache Kafka based solutions, due to its compact binary payloads and stringent schema support. In its simplest form, it however lacks an important feature of a good Schema formalism: The ability to decompose a schema into smaller, reusable schema components. It can be accomplished, but requires some additional work or using an alternative Schema syntax. -[readmore]-  [comment]: #  [Serialization]: https://en.wikipedia.org/wiki/Serialization [Composition]: https://en.wikipedia.org/wiki/Object_composition [Data Binding]: https://en.wikipedia.org/wiki/Data_binding [Apache Avro]: https://avro.apache.org/ [Apache Hadoop]: https://hadoop.apache.org/ [Apache Kafka]: Https://kafka.apache.org/ [Confluent]: https://www.confluent.io/ [Schema Registry]: https://docs.confluent.io/current/schema-registry/docs/index.html [Event Driven Architectures]: https://martinfowler.com/articles/201701-event-driven.html [Google Protobuf]: https://developers.google.com/protocol-buffers/ [DRY principle]: https://en.wikipedia.org/wiki/Don%27t_repeat_yourself [Kafka Benchmark]: https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines  [comment]: #  [Avro-logo]: http://avro.apache.org/images/avro-logo.png   Data [Serialization] plays a central role in any distributed computing system, be it message-oriented or RPC-based. Ideally, the involved parties should be able to exchange data in a way that is both efficient and robust, and which can evolve over time. I've seen many data serialization techniques come and go during the last 20 years, shifting with the current technical trends: Fixed-position binary formats, tag-based binary formats, separator-based formats, XML, Json, etc. Early frameworks were usually backed by supporting tools , whereas more recent serialization frameworks usually provides a formal Schema language to enforce data correctness and enable contracts to evolve in a controlled way. The Schema formalism usually also provides a [Data Binding] mechanism to allow for easy usage in various programming languages.  In order to support non-trivial domain/information models, the Schema language should provide support for [Composition], where a complex Schema may be composed from smaller, resuable Schemas. This is usually achieved by some kind of *Include* mechanism in the the Schema formalism, and optionally additional build time configuration for any code generation Data Binding support.   [Event Driven Architectures] are becoming increasingly more popular, partly due to the challenges with tightly coupled micro services. When streaming events at scale, a highly scalable messaging backbone is a critical enabler. [Apache Kafka] is widely used, due to its distributed nature and thus extreme scalability. In order for Kafka to really deliver, individual messages needs to be fairly small . Hence verbose data serialization formats like XML or JSON might not be appropriate for event sourcing.  ![Apache Avro][Avro-logo]  While there are several serialization protocols offering compact binary payloads , [Apache Avro] is frequently used together with Kafka. While not necessarily the most elegant serialization framework, the [Confluent] Kafka packaging provides a [Schema Registry], which allows a structured way to manage message schemas and schema versions, and the Schema Registry is based on Avro schemas.  Suprisingly, while the formal support for managing Schema versioning  is really powerful, Vanilla Avro lacks a decent *include* mechanism to enable Compositional Schemas that adheres to the [DRY principle]. The standard JSON-based syntax for Avro Schemas allows for a composite type to refer to other fully-qualified types, but the composition is not enforced by the schema itself. Consider the following schema definitions, where the composite UserCarRelation is composed from the simpler User and Car schemas:     In order for the Avro Compiler to interpret and properly generate code for the UserCarRelation schema, it needs to be aware of the inclusions . The Avro maven plugin provides explicit support for this missing inlusion mechanism:   As seen, this inclusion is only handled by the Data Binding toolchain and not explicitly present in the Schema itself. Hence it won't work with e.g. the Kafka Schema Registry.   In more recent versions of Avro, there is however an alternative syntax for describing Schemas. *Avro IDL* is a custom DSL for describing datatypes and RPC operations. The toplevel concept in an Avro IDL definition file is a *Protocol*, a collection of operations and their associated datatypes. While the syntax at first look seems to be geared toward RPC, the RPC operations can be omitted, and hence a Protocol may be used to only define datatypes. Interestingly enough, Avro IDL do contain a standard *include* mechanism, where other IDL files as well as JSON-defined Avro Schemas may be properly included. Avro IDL originated as an experimental feature in Avro, but is now a supported alternative syntax.  Below is the same example as above, in Avro IDL:     Now the build system configuration can be correspondingly simplified:    Compositionality is an important aspect of a well-designed information or message model, in order to highlight important structural relationships and to eliminate redundancy. If Apache Avro is used as your Serialization framework, I believe Avro IDL should be the preferred way to express the Schema contracts.
In this part of the Go microservices [blog series].  1. Overview 2. The Circuit Breaker 3. Resilience through Retrier 4. Landscape overview 5. Go code - adding circuit breaker and retrier 6. Deploy & run 7. Hystrix Dashboard and Netflix Turbine 8. Turbine & Service discovery 9. Summary   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P11  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  Consider the following make-believe system landscape where a number of microservices handles an incoming request:  ![circuit breaker 1] _Figure 1 - system landscape_  What happens if the right-most service "Service Y" fails? Let's say it will accept incoming requests but then just keep them waiting, perhaps the underlying data storage isn't responsive. The waiting requests of the consumer services  waiting for their response. This may even cascade through the call chain all the way back to the entry point service, effectively grinding your entire landscape to a halt.  ![circuit breaker 2] _Figure 2 - cascading failure_  While a properly implemented [healthcheck] pattern comes in.   ![circuit breaker 3] _Figure 3 - circuit breaker_  Here we see how a _circuit breaker_ logically exists between Service A and Service Y . The concept of the circuit breaker comes from the domain of electricity. Thomas Edison filed a patent application back in 1879. The circuit breaker is designed to open when a failure is detected, making sure cascading side effect such as your house burning down or microservices crashing doesn't happen. The hystrix circuit breaker basically works like this:  ![circuit breaker 4] _Figure 4 - circuit breaker states_  1. Closed: In normal operation, the circuit breaker is _closed_, letting requests  pass through. 2. Open: Whenever a failure has been detected , the circuit _opens_, making sure the consumer service short-circuits instead of waiting for the failing producer service. 3. Half-open: Periodically, the circuit breaker lets a request pass through. If successful, the circuit can be closed again, otherwise it stays open.  There's two key take-aways with Hystrix when the circuit is closed:  1. Hystrix allows us to provide a _fallback_ function that will be executed _instead_ of running the normal request. This allows us to provide a fallback behaviour. Sometimes, we can't do without the data or service of the broken producer - but just as often, our fallback method can provide a default result, a well-structured error message or perhaps calling a backup service. 2. Stopping cascading failures. While the fallback behaviour is very useful, the most important part of the circuit breaker pattern is that we're immediately returning some response to the calling service. No thread pools filling up with pending requests, no timeouts and hopefully less annoyed end-consumers.   The circuit breaker makes sure that if a given producer service goes down, we can both handle the problem gracefully and save the rest of the application from cascading failures. However, in a microservice environment we seldom only have a single instance of a given service. Why consider the first attempt as a failure inside the circuit breaker if you have many instances where perhaps just a single one has problems? This is where the _retrier_ comes in:  In our context - using Go microservices within a Docker Swarm mode landscape - if we have let's say 3 instances of a given producer service, we know that the Swarm Load-balancer will automatically round-robin requests addressed to a given _service_. So instead of failing inside the breaker, why not have a mechanism that automatically performs a configurable number of retries including some kind of backoff?  ![retrier] _Figure 5 - retrier_  Perhaps somewhat simplified - the sequence diagram should hopefully explain the key concepts:  1. The retrier runs _inside_ the circuit breaker. 2. The circuit breaker only considers the request failed if all retry attempts failed. Actually, the circuit breaker has no notion of what's going on inside it - it only cares about whether the operation it encapsulates returns an error or not.  In this blog post, we'll use the [retries].  In this blog post and the example code we're going to implement later, we'll add circuit breakers to the _accountservice_ for its outgoing calls to the _quotes-service_ and a new service called _imageservice_. We will also install services running the Netflix Hystrix [Monitoring dashboard] hystrix stream aggregator. More on those two later.  ![overview] _Figure 6 - landscape overview_   Finally time for some Go code! In this part we're introducing a brand new underlying service, the _imageservice_. However, we won't spend any precious blog space describing it. It will just return an URL for a given "acountId" along with the IP-address of the serving container. It provides a bit more complexity to the landscape which is suitable for showcasing how we can have multiple named circuit breakers in a single service.  Let's dive into our "accountservice" and the _/goblog/accountservice/service/handlers.go_ file. From the code of the _GetAccount_ func, we want to call the underlying _quotes-service_ and the new _imageservice_ using go-hystrix and go-resilience/retrier. Here's the starting point for the quotes-service call:  func getQuote {  body, err := cb.CallUsingCircuitBreaker  // Code handling response or err below, omitted for clarity ... }  The _cb.CallUsingCircuitBreaker_ func is something I've added to our _/common/circuitbreaker/hystrix.go_ file. It's a bit on the simplistic side, but basically wraps the go-hystrix and retries libraries. I've deliberately made the code more verbose and non-compact for readbility reasons.  func CallUsingCircuitBreaker { output := make // Declare the channel where the hystrix goroutine will put success responses.  errors := hystrix.Go(breakerName, // Pass the name of the circuit breaker as first parameter.  // 2nd parameter, the inlined func to run inside the breaker. func error { // Create the request. Omitted err handling for brevity req, _ := http.NewRequest  // For hystrix, forward the err from the retrier. It's nil if successful. return callWithRetries },  // 3rd parameter, the fallback func. In this case, we just do a bit of logging and return the error. func error { logrus.Errorf circuit, _, _ := hystrix.GetCircuit logrus.Errorf return err })  // Response and error handling. If the call was successful, the output channel gets the response. Otherwise, // the errors channel gives us the error. select { case out := <-output: logrus.Debugf return out, nil  case err := <-errors: return nil, err } }  As seen above, go-hystrix allows us to name circuit breakers, which we also can provide fine-granular configuration for given the names. Do note that the hystrix.Go func will execute the actual work in a new goroutine, where the result sometime later is passed through the unbuffered  code snippet, which will effectively block until _either_ the _output_ or _errors_ channels recieves a message.  Next, the _callWithRetries_ func that uses the retrier package of go-resilience:  func callWithRetries error {  // Create a retrier with constant backoff, RETRIES number of attempts  with a 100ms sleep between retries. r := retrier.New  // This counter is just for getting some logging for showcasing, remove in production code. attempt := 0  // Retrier works similar to hystrix, we pass the actual work  in a func. err := r.Run error { attempt++  // Do HTTP request and handle response. If successful, pass resp.Body over output channel, // otherwise, do a bit of error logging and return the err. resp, err := Client.Do if err == nil && resp.StatusCode < 299 { responseBody, err := ioutil.ReadAll if err == nil { output <- responseBody return nil } return err } else if err == nil { err = fmt.Errorf }  logrus.Errorf return err }) return err }   I've created three unit tests in the _/goblog/common/circuitbreaker/hystrix_test.go_ file which runs the _CallUsingCircuitBreaker_ func. We won't go through all test code, one example should be enough. In this test we use _gock_ to mock responses to three outgoing HTTP requests, two failed and at last one successful:  func TestCallUsingResilienceLastSucceeds { defer gock.Off  buildGockMatcherTimes // First two requests respond with 500 Server Error  body := []byte buildGockMatcherWithBody request respond with 200 OK  hystrix.Flush // Reset circuit breaker state  Convey { Convey { // Call single time  bytes, err := CallUsingCircuitBreaker  Convey { // Assert no error and expected response So So So }) }) }) }  The console output of the test above looks like this:  ERRO[2017-09-03T10:26:28.106] Retrier failed, attempt 1 ERRO[2017-09-03T10:26:28.208] Retrier failed, attempt 2 DEBU[2017-09-03T10:26:28.414] Call in breaker TEST successful  The other [tests] asserts that hystrix fallback func runs if all retries fail and another test makes sure that the hystrix circuit breaker is opened if sufficient number of requests fail.   Hystrix circuit breakers can be configured in a variety of ways. A simple example below where we specifiy the number of failed requests that should open the circuit and the retry timeout:  hystrix.ConfigureCommand("quotes-service", hystrix.CommandConfig{ SleepWindow: 5000, RequestVolumeThreshold: 10, })  See the [docs] using this naming convention:  hystrix.command.[circuit name].[config property] = [value]  Example:   hystrix.command.quotes-service.SleepWindow: 5000    In the git branch of this part, there's updated microservice code and ./copyall.sh which builds and deploys the new _imageservice_. Nothing new, really. So let's take a look at the circuit breaker in action.  In this scenario, we'll run a little [load test] that by default will run 10 requests per second to the /accounts/ endpoint.  > go run *.go -zuul=false    Let's say we have 2 instances of the _imageservice_ and _quotes-service_ respectively. With all services running OK, a few sample responses might look like this:     If we kill the quotes-service:  > docker service scale quotes-service=0  We'll see almost right away  how the fallback function has kicked in and are returning the fallbackQuote:    What's a lot more interesting is to see how the application as a whole reacts if the quote-service starts to respond really slowly. There's a little "feature" in the quotes-service that allows us to specify a hashing strength when calling the quotes-service.  http://quotes-service:8080/api/quote?strength=4  Such a request is typically completed in about 10 milliseconds. By changing the _strength_ query-param to ?strength=13 the _quotes-service_ will use a LOT of CPU and need slightly less than a second to complete. This is a perfect case for seeing how our circuit breaker reacts when the system comes under load _and_ probably is getting CPU-starved. Let's use Gatling for two scenarios - one where we've disabled the circuit breaker and one with the circuit breaker active.  No circuit breaker, just using the standard _http.Get_: ![no circuit breaker] ![no circuit breaker 2]  The very first requests needs slightly less than a second, but then latencies increases, topping out at 15-20 _seconds_ per request. Peak throughput of our two _quotes-service_ instances .  Circuit breaker, with Timeout set to 5000 ms. I.e - when enough requests have waited more than 5000 ms, the circuit will open and the fallback Quote will be returned. ![with circuit breaker] ![with circuit breaker 2] __ ![with circuit breaker 3] In this diagram, we see the distribution of response time halfway through the test. At the marked data point, the breaker is certainly open and the 95%th percentile is 10ms, while the 99%th percentile is over 4 seconds. In other words, about 95% of requests are handled within 10ms but a small percentage  are using up to 5 seconds before timing out.  During the first 15 seconds or so, the greenish/yellowish part, we see that more or less all requests are linearily increasing latencies approaching the 5000 ms threshold. The behaviour is - as expected - similar to when we were running _without_ the circuit breaker. I.e. - requests can be successfully handled but takes a lot of time. Then - the increasing latencies trip the breaker and we immediately see how response times drops back to a few milliseconds instead of ~5 seconds for the majority of the requests. As stated above, the breaker lets a request through every once in a while when in the "half-open" state. The two _quotes-service_ instances can handle a few of those "half-open" requsts, but the circuit will open again almost immediately since since the _quotes-service_ instances cannot serve more than a few req/s before the latencies gets too high again and the breaker is tripped anew.  We see two neat things about circuit breakers in action here:  - The open circuit breaker keeps latencies to a minimum when the underlying quotes-service has a problem, it also "reacts" quite quickly - significantly faster than any healthcheck/automatic scaling/service restart will. - The 5000 ms timeout of the breaker makes sure no user has to wait ~15 seconds for their response. The 5000 ms configured timeout takes care of that.     One neat thing about Hystrix is that there's a companion Web application called _Hystrix Dashboard_ that can provide a graphical representation of what's currently going on in the circuit breakers inside your microservices.  It works by producing HTTP streams of the state and statistics of each configured circuit breaker updated once per second. The Hystrix Dashboard can however only read one such stream at a time and therefore _Netflix Turbine_ exists - a piece of software that collects the streams of _all_ circuit breakers in your landscape and aggregates those into one data stream the dashboard can consume:  ![Turbine] _Figure 7 - Service -> Turbine -> Hystrix dasboard relationship_  In Figure 7, note that the Hystrix dashboard _requests_ the _/turbine.stream_ from the Turbine server, and Turbine in it's turn requests _/hystrix.stream_ from a number of microservices. With Turbine collecting circuit breaker metrics from our _accountservice_, the dashboard output may look like this:  ![turbine 2] _Figure 8 - Hystrix dashboard_  The GUI of Hystrix Dashboard is definitely not the easiest to grasp at first. Above, we see the two circuit breakers inside _accountservice_ and their state in the middle of one of the load-test runs above. For each circuit breaker, we see breaker state, req/s, average latencies, number of connected hosts per breaker name and error percentages. Among things. There's also a thread pools section below, though I'm not sure they work correctly when the root statistics producer is the go-hystrix library rather than a hystrix-enabled Spring Boot application. After all - we don't really have the concept of thread pools in Go when using standard goroutines.  Here's a short video of the "quotes-service" circuit breaker inside the _accountservice_ when running part of the load-test used above: __  [![The video]  All in all - Turbine and Hystrix Dashboard provides a rather nice monitoring function that makes it quite easy to pinpoint unhealthy services or where unexpected latencies are coming from - in real time. Always make sure your inter-service calls are performed inside a circuit breaker.  There's one issue with using Netflix Turbine & Hystrix Dashboard with non-Spring microservices and/or container orchestrator based service discovery. The reason is that Turbine needs to know where to find those /hystrix.stream endpoints, for example _http://10.0.0.13:8181/hystrix.stream_. In an ever-changing microservice landscape with services scaling up and down etc, there must exist mechanisms that makes sure _which_ URLs Turbine tries to connect to to consume hystrix data streams.  By default, Turbine relies on [Netflix Eureka] and that microservices are registering themselves with Eureka. Then, Turbine can internally query Eureka to get possible service IPs to connect to.  In our context, we're running on Docker Swarm mode and are relying on the built-in service abstraction Docker in swarm mode provides for us. How do we get our service IPs into Turbine?  Luckily, Turbine has support for plugging in custom discovery mechanisms. I guess there's two options apart from doubling up and using Eureka in addition to the orchestrator's service discovery mechanism - something I thought was a pretty bad idea back in [part 7].  This solution uses the AMQP messaging bus  and a "discovery" channel. When our microservices having circuit breakers start up, they figure out their own IP-address and then sends a message through the broker which our custom Turbine plug-in can read and transform into something Turbine understands.  ![turbine with messaging] _Figure 9 - hystrix stream discovery using messaging_  The registration code that runs at _accountservice_ startup:  func publishDiscoveryToken { // Get hold of our IP adress  and build a discovery token. ip, _ := util.ResolveIpFromHostsFile token := DiscoveryToken{ State: "UP", Address: ip, } bytes, _ := json.Marshal  // Enter an eternal loop in a new goroutine that sends the UP token every // 30 seconds to the "discovery" channel. go func { for { amqpClient.PublishOnQueue time.Sleep } } }  Full source of my little _circuitbreaker_ library that wraps go-hystrix and go-resilience can be found [here].  An other option is to let a custom Turbine plugin use the Docker Remote API to get hold of containers and their IP-addresses, which then can be transformed into something Turbine can use. This should work too, but has some drawbacks such as tying the plugin to a specific container orchestrator as well as having run Turbine on a Docker swarm mode manager node.  The [source code] and some basic docs for the Turbine plugin I've written can be found on my personal github page. Since it's Java-based I'm not going to spend precious blog space describing it in detail in this context.  You can also use a pre-built [container image] I've put on hub.docker.com. Just launch as a Docker swarm _service_.  An executable jar file and a Dockerfile for the Hystrix dashboard exists in _/goblog/support/monitor-dashboard_. The customized Turbine is easiest used from my container image linked above.  I've updated my shell scripts to launch the custom Turbine and Hystrix Dashboards. In _springcloud.sh_:  # Hystrix Dashboard docker build -t someprefix/hystrix support/monitor-dashboard docker service rm hystrix docker service create --constraint node.role==manager --replicas 1 -p 7979:7979 --name hystrix --network my_network --update-delay 10s --with-registry-auth --update-parallelism 1 someprefix/hystrix  # Turbine docker service rm turbine docker service create --constraint node.role==manager --replicas 1 -p 8282:8282 --name turbine --network my_network --update-delay 10s --with-registry-auth --update-parallelism 1 eriklupander/turbine  Also, the _accountservice_ Dockerfile now exposes port 8181 so Hystrix streams can be read from within the cluster. You *shouldn't* map 8181 to a public port in your _docker service create_ command.  I don't know if Turbine is slightly buggy or what the matter is, but I tend to having to do the following for Hystrix Dashboard to pick up a stream from Turbine:  * Sometimes restart my _turbine_ service, easiest done using _docker service scale=0_ * Have some requests going through the circuit breakers. Unsure if hystrix streams are produced by go-hystrix if there's been no or no ongoing traffic passing through. * Making sure the URL one enters into Hystrix Dashboard is correct. _http://turbine:8282/turbine.stream?cluster=swarm_ works for me.  In part 11 of the [blog series] we've looked at circuit breakers and resilience and how those mechanisms can be used to build a more fault-tolerant and resilient system.  In the [next part], we'll be introducing two new concepts: The Zuul EDGE server and distributed tracing using Zipkin and Opentracing.
In April last year, Adobe announced that they planned to donate their Flex programming platform to Open Source. Some two weeeks ago, it finally happened.  Flex is a platform for Rich Internet Applications , Flex applications can also run locally, as desktop applications.  I attended an awesome demo of Flex and Air at the QCON conference this morning. Starting with a simple CRUD app, Christophe Coenraets gradually modified it into a sleek, iTunes lookalike.  The programming language for Flex is ActionScript, an ECMA-standardized script language. But on top of this imperative scripting language, Flex adds MXML, a declarative xml-based markup language quite similar to Microsoft's XAML:  Neat, isn't it?  Two things comes to my mind: Why doesn't the Java community try to catch up? The Flex programming model is far superior to any of the Java Web frameworks out there. Still most work and innovation within the Java Web application area seems to be obsessed with trying to achieve roughly the same thing using Ajax.  The second thing that puzzles me is Adobe's visions and plans with Flex. How will Adobe make money on Flex, when going Open Source? They probably realized that sticking to a closed, commercial license would prevent wide adoption. But what is their alternative plan? Where is the snag?
  c10k: Patterns for non blocking services in the land of callback hell  In the previous blog we demonstrated the benifits, from a scalability perspective, of writing non blocking services compared to traditional blocking servcies. This is specifically important if the services has wait for responses from external resources  before it can send a response back to ist caller. Using standards such as the Servlet 3.1 specification and frameworks like Spring MVC makes the seource code both simple to write and test and maybe most important portable, i.e. not locked into a specific web server product.  We were also introduced to a new programming model that comes with the non-blocks approach, the callback model. Instead of just waiting in our code, blocking a precois thread, for the externa resoruce to respond we setup a callback method that will be called  once the external resource is done. This way we don't need to block any thread dirig the wait for the responce of the external resource.  The callback model introduce a more complex programming model then the tradidional blocking model so this means that we have to be carefull avoiding writing code that bemoces wery costly to maintain over time.  In this blog we will look into three very common patterns were the non-blocking aproach have a great potential for imporoving scalability and we will compare different approaches for minimize the complexity introduced by the callback model.  ...   In the previous blog we described how to develop non-blocking REST services using Spring MVC and Spring Boot. In this blog we will look into consequences of programming model based on callbacks that is imposed by the asynchronous programming model.  Three common patterns that we will implement in a asynchronous way using non-blocking I/O:  1. Routing 2. Aggregation 3. Routing Slip   ![]     ![]         ![]  ![] 
This week, Google's announcement of Google App Engine  got a lot of attention. The initial 10 000 test accounts ran out in a few hours.  With GAE you can run your own web application on Google's infrastructure. Initially they support applications written in Python, but more languages are already considered for future versions. Data can be persisted using the DataStore API, and there is also an API for integrating with Google Accounts. Of course, there are some limitations on what your webapp can do, this blogpost gives you a good overview of what they are.  A lot of people have already compared GAE to Amazon Elastic Compute Cloud , even though they are quite different. Amazon EC2 gives you your own VM where you can run basically whatever you want, whereas GAE only lets you run Python web applications in a sand box. The similarity, of course, lies in the fact that they both offer a way to scale your application on demand, by using infrastructure already available and proven.  GAE is certainly not as flexible as EC2; but on the other hand it seems very easy to get something up and running with GAE. Not to mention the fact that it's free. Google lets you use up to 500MB of persistent storage and enough CPU and bandwidth to serve about 5 million page views a month. If you want to scale beyond that, you'll have to start paying. Currently it is not official how much it will cost, the preview version released so far only covers the limited free accounts.  For small web applications, traditionally hosted in shared environments, the GAE approach is very appealing, if you're not too afraid of the lock in and can live with the limitations.  It will be interesting to watch the evolvment of GAE as well as EC2 in the future.
 This post is the first in a series of two blog posts where I will try to give some useful tips on how to make your spring applications more configurable. Too often I see application configuration files embedded somewhere deep down in the application referenced only by its classpath location.  -[readmore]-   By injecting the configurations this way they will be packaged inside the application in and you will have to rebuild and deploy it if you need to make any changes. Considered you have environment specific configurations you will also have to build an artifact for each and every environment.  A better way to handle the configuration files is to put them outside of the application in a separate location on the server. Changing configuration using this setup will at most force you to restart your application to make Spring reload them. I normally put the configuration files in a separate folder under my home catalog, i.e. . In the spring config file you will inject the configurations using:    is a standard JVM system property which is resolved by Spring using the  placeholder to the home catalog of the user running the server. You can even make the configuration location configurable through an environment variable:   But what if you want to package some default configurations within you application but still give the opportunity to manipulate them without the need to rebuild and make it possible to choose your own config location? No problem use:   Spring will read and merge the configuration files in the order they appear and will override duplicated configurations. This means that you can override the default configurations by adding them in .  Ok, now I showed how to configure the property files but what if you want to configure the Spring application context? What if you want to configure the way your application is put together? Lets take an example: If an application uses JNDI to retrieve the database connection you will first need to configure your server with a database connection and a JNDI-name. Secondly you will need to create a datasource bean in you application context which is injected into your DAO:s, i.e:  The dao:   The spring config file :   Using JNDI will force you to configure the server to before you can run the application. But as a developer of you don't change the JNDI configuration on your server for every application you work on. Instead it would be great if you could create a direct connection using Spring, this would remove the need of server configuration. Here is two example showing the datasource bean configured to use JNDI in the first case and a direct connection in the second case:  JNDI - :   Direct - :   In the  it is now possible choose which one of the two datasources I would like to use by importing the corresponding file. When deploying on the dev-environment use:   and when deploying to production use:   It could be quite annoying having to switch like this, but luckily Spring has a solution for this. Using the  placeholder we can inject which type of datasource connector we would like to use, we could even give it a default value incase we don't inject a connector type. This is done by adding a : in the placeholder like this:   The  placeholder picks up parameters from System properties, environment variables and the property files we injected earlier. So this mean that we Now have an application context that is externally configurable. By setting a property named  we could easily change the application to use a direct connection to the database instead of retrieve it from JNDI.  Great we are done? No, unfortunately the  placeholder is unable to pick up an property from a property file injected to a property-placeholder bean if it is used in an import-tag, why I will explain in the next blog post. This will limit us to set the datasource connector using either a System property  or an environment variable. To be really happy want to keep all configuration parameters in  but in the way Spring works this is not possible out of the box. In my next blog post I will explain why this isn't possible and I will also show a solution to the problem.
 For sure, Java is a great language. I wrote my first Java app 1996 and have had lots of fun with it ever since. But for the past 4-5 years my interest in dynamic programming languages as Python, Groovy and Clojure has increased more and more. When working with other languages you discover a bunch of nice programming features you don't want to be without.  Already said, Java is a great language, but it is verbose. For example, when creating maps and lists with values/objects in Java, you need to write horribly many lines of code to achieve a very simple task. In Groovy for example, you can do it with a clear and simple syntax.   In Java, there is no good way doing this. The standard procedure is:   There are alternatives, such as making an anonymous inner class with an instance initializer :   Which probably should be written like this if you follow the Java stylish way :   However, I'm not too fond of the previous approach because what you end up with is a subclass of ArrayList which has an instance initializer, and that class is created just to create one object. And further on, the instance of the anonymous class that you have created contain a synthetic reference to the enclosing object. If you serialize the collection you will also serialize everything in the outer class. The approach is also incompatible with one popular way to implement the  method. Assume the class Example having this method:   Then, objects created by "double brace initialization" will never be equal to objects created without it. So this approach should never be uses for any class that needs a non trivial  method. Collection classes should be fine though.  To achieve a one-liner initialization of a list there is an idiomatic alternative that doesn't require such heavy handed use of anonymous inner classes.   This approach is especially useful when setting up mocks and stubs in your unit tests. It 's fairly clean and understandable, and it helps you keeping down the number of lines of code:   Can be written like this:   Which is three lines of code less  What would be nice is if the Collection Literals proposal for [Project Coin] is accepted, so we can have list literals in Java 7. Imagine writing lists and maps in this way:   Alternatively a slightly more verbose syntax:   Both these syntaxes is far better than what you can do today. If I'm going to vote for an alternative I would pick the shorter syntax. Why? It's cleaner, simpler and shorter.  But there are certainly some difficulties to solve such as:   It should be a really simple thing adding list and map literals to Java. But the devil is in the details. When looking into these details you will discover that Java's static types and generics combine will cause some difficulty. It will be interesting to see how it will be resolved in the future. If it will not be solved, there is a risk that Java will lose some of its attractiveness as a first choice language for many developers. Young developers are considering Java the new Cobol - a dying old-fashioned language that is no fun to work in.  Finally, wouldn't be nice to have a simpler syntax for generics where you don't have to write the generics type twice in a generics declaration. Today you need to write this:   When Java 7 is out you can, hopefully, write something like this: 
 In the world of information architecture, UML still has a strong foothold. Therefore class diagrams are still desired, even when designing RESTful APIs. I have therefore sought a way to represent the pieces that make up an API as UML and found that although this is somewhat intuitive, there is not one set way of doing this. This post describes our way of using UML to create a hybrid web service model/information model.  -[readmore]-  For our project we on the one hand had a need for a web service model capturing resources and sub-resources and the relations or paths between them, and on the other hand we needed a information model to aid in designing the web services and underlying database by capturing the datamodel and the attributes, datatypes and subcomponents that made up each resource or sub-resource. For this we extended the UML-notatin so that we were able to capture all of this in one class diagram.  Inspired by the [guide to designing RESTful web services] we started of with making a basic web service model of the solution.  Step one was to make classes identifiable as resources. This was done by writing “Resource” after the class name:  ![UML1.jpg]  Perhaps a better way to do this would have been to create a stereotype called Resource, because then you get the nice . So here we go:  ![UML2.jpg]  Then we needed to capture the relation between two or more resources. To do this we used a simple association, and gave it a name in the direction which indicated the path used to reach it:  ![UML3.jpg]  Again, perhaps a steteotype would have been expected, and then using a one way arrow helps to show the direction of the relation. The way to do this would be like this:  ![UML4.jpg]  But here we felt we lost something by leaving out the cardinality of the relation. This might be of little interest for a consumer of the API, but it helps the developer of the underlying database to make the right assumptions about the composition of classes.  We initially played around with the idea of making some relations go in both directions making both resources each others sub-resources. In the final design instead modeled our api like a database having a relation from class A to B and adding the unique identifier of A to B, much like a foreign key. More about this in the section “Addressing the information model needs”.  As we relied on a convention of making all resources available as both top level resources of their own even if they primarily served as sub-resources we skipped linking them to a top level application. We later introduced this in a separate web service model for clarity, but during the development phase it served no need.  We would have ended up with a lot of lines if all classes had to connect to a top level class representing the application like this:  ![UML5.jpg]  As we relied on the convention of naming all resources in plural names the relations between resources and sub-resources were given plural names. Also we used the convention of retrieving a list of all objects by the url  and retrieving one object by it's identity .  This allowed us to skip having one class representing the whole collection and one class representing one entity. This is great for when you finish up ypur project and want to document the final web service model:  ![UML6.jpg]  But during the development phase of our project we kept it simple like this:  ![UML7.jpg]  As all resources were “read only” and only supported GET we simply left out specifying the method. Also we relied on the convention to make all attributes in the response searchable we also didn’t have to specify specific query-attributes per resource:  ![UML8.jpg]  It doesn’t hurt to add methods, but they make things less agile as things might move around in the model. Here the methods are included:  ![UML9.jpg]  Here is where we step away from the traditional UML representation of an API by exposing the attributes that make up each resource in the same model. The “proper” place for this would be as output from the get-method of each resource, but this made datatype, identifyers, cardinality and sub-components invisible in the model, so why not incorporate some of the conventions of representing xml-schemas in UML into the diagram to capture more in a single view?  In this example the output of a get method of one healthcareProvider returns a healthcareProviderId, we can then specify that it’s an int and we let the “unique” flag convey that it’s the identifier of this resource:  ![UML10.jpg]  The foreign key convention governing this design leads us to  having a sub-resource reached through   The sub-resource qualityMeasurments then has an attribute named like the resource and with a modifier showing it is the healthcareProviderId that is returned. In the final xml this is just a simple .  The output of a resource isn’t just a straight list of attributed, so we needed to find a way to represent more complex structures. We had the example of needing to embed the author in the output instead of making it a resource of it’s own. For this we used compositon-associations in a manner similar to that used when modeling XML-schemas in UML:  ![UML11.jpg]  Using this approach made it really easy to get an overview of the entire domain in one view. This enabled us to make changes in the design and still ensuring that - Attributes were not forgotten when merging or splitting resources - Naming conventions of attibutes were followed. - All relations were working in the directions intended  It also made it easy to alter between having something embedded in the output and having it a resource of it’s own, we simply just changed the relation.  Though primarily serving as a model for aiding in the design of the underlying datamodel and for developing the web services, this model also made it quite easy to create the required documentation for the domain. - It made it easy to construct a web service model as all the information was already captured in this model. - It made it easy to specify the output of each resource as we just did a cut and paste of the resource together with it’s compositions.  However some things were not addressed as it was out of scope for our domain, such as aliases for quick searches, redirects and having to incorporate external systems into our design.
 In this blog post I will describe how to set up a local Kubernetes cluster for test purposes with a minimal memory usage and fast startup times, using [Docker in Docker] instead of traditional local virtual machines.  -[readmore]-  This blog post is part of the blog series - [Trying out new features in Docker].   For a background on how [Docker in Docker].  This blog post is not an introduction to Kubernetes and the components that builds up a Kubernetes cluster. For an introduction of the concepts used in Kubernetes see: [kubernetes.io/docs/concepts/].  We are going to use the GitHub project [Mirantis/kubeadm-dind-cluster].  ![kubernetes] **Source:** [http://nishadikirielle.blogspot.se/2016/02/kubernetes-at-first-glance.html]    First, you need to have [Docker for Mac] installed, I'm on version .  Next, you also need to have  and  installed to be able to follow my instructions below. If you use [Homebrew], they can be installed with:  brew install jq brew install md5sha1sum  Finally, clone the Git repo  from GitHub and jump into the  folder:  git clone https://github.com/Mirantis/kubeadm-dind-cluster.git cd kubeadm-dind-cluster/fixed  We are good to go!   Start up a Kubernetes v1.8 cluster requesting 3 worker nodes in the cluster :  NUM_NODES=3 ./dind-cluster-v1.8.sh up  The first time the  command is executed it will take a few minutes and produce lot of output in the terminal window...  ...in the end it should say something like:  NAME STATUS ROLES AGE VERSION kube-master Ready master 2m v1.8.4 kube-node-1 Ready <none> 1m v1.8.4 kube-node-2 Ready <none> 1m v1.8.4 kube-node-3 Ready <none> 47s v1.8.4 * Access dashboard at: http://localhost:8080/ui  > **Note:** If you start up the cluster again later on, it will only take a minute.  Verify that you can see the master and worker nodes as ordinary containers in Docker for Mac:  docker ps  It should report something like:  CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 766582a93d1f mirantis/kubeadm-dind-cluster:v1.8 "/sbin/dind_init s..." 9 hours ago Up 9 hours 8080/tcp kube-node-3 e1fc6bec1f23 mirantis/kubeadm-dind-cluster:v1.8 "/sbin/dind_init s..." 9 hours ago Up 9 hours 8080/tcp kube-node-2 b39509b9db77 mirantis/kubeadm-dind-cluster:v1.8 "/sbin/dind_init s..." 9 hours ago Up 9 hours 8080/tcp kube-node-1 a01be2512423 mirantis/kubeadm-dind-cluster:v1.8 "/sbin/dind_init s..." 9 hours ago Up 9 hours 127.0.0.1:8080->8080/tcp kube-master   Ok, so let's see if we actually have a Kubernetes cluster up and running:  kubectl get nodes  It should result in a response like:  NAME STATUS AGE VERSION kube-master Ready 2m v1.8.4 kube-node-1 Ready 55s v1.8.4 kube-node-2 Ready 1m v1.8.4 kube-node-3 Ready 1m v1.8.4  Also try out Kubernetes Dashboard at: [localhost:8080/ui]  Click on the "Nodes" - link in the menu to the left and you should see something like:  ![k8s-dashboard]   Now, let's deploy a service and try it out!  I have a very simple Docker image   that creates some random quotes about successful programming languages.  We will create a [Deployment].  > **Note:** In more production like environment we should also set up an external load balancer, like HAProxy or NGINX in front of the Kubernetes cluster to be able to expose one single entry point to all services in the cluster. But that is out of scope for this blog post and left as an exercise for the interested reader :-)  First, switch to the default [namespace]:  kubectl config set-context $ --namespace=default  The default namespace should only contain one pre-created object, run the command:  kubectl get all  It should report:  NAME CLUSTER-IP EXTERNAL-IP PORT AGE svc/kubernetes 10.96.0.1 <none> 443/TCP 5h  Create a file named  with the following command:  cat <<EOF > quotes.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: quotes labels: app: quotes-app spec: replicas: 1 selector: matchLabels: app: quotes-app template: metadata: labels: app: quotes-app spec: containers: - name: quotes image: magnuslarsson/quotes:go-22 ports: - containerPort: 8080 ---  apiVersion: v1 kind: Service metadata: name: quotes-service spec: type: NodePort selector: app: quotes-app ports: - port: 8080 targetPort: 8080 nodePort: 31000 EOF  Create the Deployment and Service objects with the following command:  kubectl create -f quotes.yml  Verify that we got the expected objects created, using the following command:  kubectl get all  Expect output:  NAME READY STATUS RESTARTS AGE po/quotes-77776b5bbc-5lll7 1/1 Running 0 45s  NAME CLUSTER-IP EXTERNAL-IP PORT AGE svc/kubernetes 10.96.0.1 <none> 443/TCP 5h svc/quotes-service 10.105.185.117 <nodes> 8080:31000/TCP 45s  NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/quotes 1 1 1 1 45s  NAME DESIRED CURRENT READY AGE rs/quotes-77776b5bbc 1 1 1 45s  > **Note:** In the output above short names are used for object types: > > * : Pod > * : Service > * : Deployment > * : Replica Set  We can now try it out using  from one of the worker nodes:  docker exec kube-node-2 curl localhost:31000/api/quote -s -w "\n" | jq  Output should look like:  { "ipAddress": "quotes-77776b5bbc-5lll7/10.192.3.4", "quote": "In Go, the code does exactly what it says on the page.", "language": "EN" }  The most interesting part of the response from the service is actually the field , that contains the hostname and ip address of the pod that served the request,  in the sample response above.   This can be used to verify that scaling of a service actually works. In the output from a scaled service we expect different values in the  - field from subsequent requests, indicating that the request is load balanced over the available pods.  Let's try it out, shall we?  First, start a loop that use  to sends one request per second to the  and prints out the  - field from the response:  while true; do docker exec kube-node-2 curl localhost:31000/api/quote -s -w "\n" | jq -r .ipAddress; sleep 1; done  Initially the output should return one and the same hostname and IP address, since we only have one pod running in the service:  quotes-77776b5bbc-5lll7/10.192.3.4 quotes-77776b5bbc-5lll7/10.192.3.4 quotes-77776b5bbc-5lll7/10.192.3.4 quotes-77776b5bbc-5lll7/10.192.3.4  Now, scale the  by adding 8 new pods to it :  kubectl scale --replicas=9 deployment/quotes  Verify that you can see all 9  pods and also to what node they are deployed:  kubectl get pods -o wide  Expected output:  NAME READY STATUS RESTARTS AGE IP NODE quotes-77776b5bbc-42wgk 1/1 Running 0 1m 10.192.4.9 kube-node-3 quotes-77776b5bbc-c8mkf 1/1 Running 0 1m 10.192.3.8 kube-node-2 quotes-77776b5bbc-dnpm8 1/1 Running 0 25m 10.192.3.4 kube-node-2 quotes-77776b5bbc-gpk85 1/1 Running 0 1m 10.192.2.8 kube-node-1 quotes-77776b5bbc-qmspm 1/1 Running 0 1m 10.192.4.11 kube-node-3 quotes-77776b5bbc-qr27h 1/1 Running 0 1m 10.192.3.9 kube-node-2 quotes-77776b5bbc-txpcq 1/1 Running 0 1m 10.192.2.9 kube-node-1 quotes-77776b5bbc-wb2qt 1/1 Running 0 1m 10.192.4.10 kube-node-3 quotes-77776b5bbc-wzhzz 1/1 Running 0 1m 10.192.2.7 kube-node-1  > **Note:** We got three pods per node, as expected!  You can also use the Dashboard to see what pods that run in a specific node:  ![k8s-dashboard-pods-per-node]  Now, the output from the curl - loop should report different hostnames and ip addresses as the requests are load balanced over the 9 pods:  quotes-77776b5bbc-gpk85/10.192.2.8 quotes-77776b5bbc-42wgk/10.192.4.9 quotes-77776b5bbc-txpcq/10.192.2.9 quotes-77776b5bbc-txpcq/10.192.2.9 quotes-77776b5bbc-wb2qt/10.192.4.10 quotes-77776b5bbc-txpcq/10.192.2.9  Great, isn't it?   Now, let's expose the container orchestrator, i.e. Kubernetes, to some problems and see if it handles them as expected!   First, let's shut down some arbitrary pods and see if the orchestrator detects it and start new ones!  > **Note**: We will actually kill the container that runs within the pod, not the pod itself.  Start a long running command, using the  flag, that continuously reports changes in the state of the Deployment object:  kubectl get deployment quotes --watch  Initially, it should report:  NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE quotes 9 9 9 9 1d  > **Note:** The command hangs, waiting for state changes to be reported  To keep things relatively simple, let's kill all  running on the first worker node:  CIDS=$ docker exec kube-node-1 docker rm -f $CIDS  The command should respond with the ids of the killed containers:  e780545ddd17 ddd260ba3f73 b4e07e736028  Now, go back to the "*deployment watch*" - command and see what output it produces!  It should be something like:  quotes 9 9 9 8 1d quotes 9 9 9 7 1d quotes 9 9 9 6 1d quotes 9 9 9 7 1d quotes 9 9 9 8 1d quotes 9 9 9 9 1d  The output shows how Kubernetes detected that it got short of available pods and compensated that by scheduling new containers for the affected pods.   Now, let's make it even worse by removing a worker node, simulating that it is taken off line for maintenance work. Let's mark  as no longer accepting either existing pods or scheduling of new pods:  kubectl drain kube-node-3 --ignore-daemonsets  The command reports back what pods that was evicted from the node:  pod "quotes-77776b5bbc-jlwtb" evicted pod "quotes-77776b5bbc-7d6gc" evicted pod "quotes-77776b5bbc-cz8sp" evicted  Kubernetes will however automatically detect this and start new ones on the remaining nodes:  kubectl get pods -o wide  Reports back:  NAME READY STATUS RESTARTS AGE IP NODE quotes-77776b5bbc-28r7w 1/1 Running 0 11s 10.192.2.10 kube-node-1 quotes-77776b5bbc-7hxd5 1/1 Running 0 11s 10.192.3.10 kube-node-2 quotes-77776b5bbc-c8mkf 1/1 Running 0 7m 10.192.3.8 kube-node-2 quotes-77776b5bbc-dnpm8 1/1 Running 0 31m 10.192.3.4 kube-node-2 quotes-77776b5bbc-gpk85 1/1 Running 0 7m 10.192.2.8 kube-node-1 quotes-77776b5bbc-grcqn 1/1 Running 0 11s 10.192.2.11 kube-node-1 quotes-77776b5bbc-qr27h 1/1 Running 0 7m 10.192.3.9 kube-node-2 quotes-77776b5bbc-txpcq 1/1 Running 0 7m 10.192.2.9 kube-node-1 quotes-77776b5bbc-wzhzz 1/1 Running 0 7m 10.192.2.7 kube-node-1  > **Note:** The three pods with an age of 11 sec are the new ones.  We can also see that the node is reported to being unavailable for scheduling of pods:  kubectl get node  Reports:  NAME STATUS AGE VERSION kube-master Ready 1d v1.8.4 kube-node-1 Ready 1d v1.8.4 kube-node-2 Ready 1d v1.8.4 kube-node-3 Ready,SchedulingDisabled 1d v1.8.4  Great!  Let's wrap up by making the node available again:  kubectl uncordon kube-node-3  The node is now reported to be back on line:  kubectl get node  Results in:  NAME STATUS AGE VERSION kube-master Ready 1d v1.8.4 kube-node-1 Ready 1d v1.8.4 kube-node-2 Ready 1d v1.8.4 kube-node-3 Ready 1d v1.8.4  But none of the existing pods are automatically rescheduled to the node:  kubectl get pods -o wide  Still reports that all pods runs on node 1 and 2:  NAME READY STATUS RESTARTS AGE IP NODE quotes-77776b5bbc-28r7w 1/1 Running 0 4m 10.192.2.10 kube-node-1 quotes-77776b5bbc-7hxd5 1/1 Running 0 4m 10.192.3.10 kube-node-2 quotes-77776b5bbc-c8mkf 1/1 Running 0 11m 10.192.3.8 kube-node-2 quotes-77776b5bbc-dnpm8 1/1 Running 0 36m 10.192.3.4 kube-node-2 quotes-77776b5bbc-gpk85 1/1 Running 0 11m 10.192.2.8 kube-node-1 quotes-77776b5bbc-grcqn 1/1 Running 0 4m 10.192.2.11 kube-node-1 quotes-77776b5bbc-qr27h 1/1 Running 0 11m 10.192.3.9 kube-node-2 quotes-77776b5bbc-txpcq 1/1 Running 0 11m 10.192.2.9 kube-node-1 quotes-77776b5bbc-wzhzz 1/1 Running 0 11m 10.192.2.7 kube-node-1  We can, however, manually rebalance our pods with the commands:  kubectl scale --replicas=6 deployment/quotes kubectl scale --replicas=9 deployment/quotes  Verify:  kubectl get pods -o wide  Reports the expected three pod per node again:  NAME READY STATUS RESTARTS AGE IP NODE quotes-77776b5bbc-2q26w 1/1 Running 0 1s 10.192.4.13 kube-node-3 quotes-77776b5bbc-bbhcb 1/1 Running 0 1s 10.192.4.14 kube-node-3 quotes-77776b5bbc-c8mkf 1/1 Running 0 13m 10.192.3.8 kube-node-2 quotes-77776b5bbc-dnpm8 1/1 Running 0 37m 10.192.3.4 kube-node-2 quotes-77776b5bbc-gpk85 1/1 Running 0 13m 10.192.2.8 kube-node-1 quotes-77776b5bbc-qr27h 1/1 Running 0 13m 10.192.3.9 kube-node-2 quotes-77776b5bbc-trrdh 1/1 Running 0 1s 10.192.4.12 kube-node-3 quotes-77776b5bbc-txpcq 1/1 Running 0 13m 10.192.2.9 kube-node-1 quotes-77776b5bbc-wzhzz 1/1 Running 0 13m 10.192.2.7 kube-node-1   That's it, let's remove the Kubernetes cluster:  ./dind-cluster-v1.8.sh down  If you start up the cluster again with the  command, it will start up much faster than the first time!  If you don't want to start up the cluster again, at least in any near time, you can also clean up some data created for the cluster:  ./dind-cluster-v1.8.sh clean  If you start up the cluster again after a  command you are back to the very long startup time.   For more blog posts on new features in Docker, see the blog series - [Trying out new features in Docker].
 [Gradle] for more information.  -[readmore]-  Some large open source projects already have migrated to Gradle: the [Grails] contains both build scripts for Maven and Gradle. Seems like there is a momentum building up for Gradle...  Recently I was involved in a project where one of the deliverables was a small Android demo app and we decided to give Gradle a try to replace the Maven scripts we usually struggle with. The result looked very promising . This blog is based on the experiences from this project and describes how you can develop a minimal Android app and use Gradle as a build system not only for building the deployable apk-file but also deploy it on an Android device. The blog also briefly demonstrates the tight integration between Gradle and the new upcoming IntelliJ based Android Studio IDE.  If you want to check out the code sample and deploy it to your own Android device you need to have Java SE 7 and Git installed . Then perform:   This should result in a tree structure containing nine files, five related to Gradle and four related to the Android app itself:   For this blog I have developed the simplest possible Android app I could think of that is one level above a "_Hello world!_" example.  I decided to try out the brand new [Date and Time API] in Java SE 8 by calculating the date of a given number of days from today. According to the spec this should now be able to be calculated as simply as:   One problem with this is of course that the Android SDK is based on Java SE 7 :-(. But since the new Date and Time API is based on an already existing open source project, [Joda-Time], this can easily be solved by adding a dependency to Joda-Time in my build file.  The user interface is based on a layout with an input field for the number of days, a button for performing the calculation and an output field for the result. Something like:  ![]  The source code for this Android app is very simple:  This is a standardized manifest file that declares to the Android OS how the app works. For this minimal app it declares:  1. Defines a unique identifier for the application  2. What versions of Android that the apps runs on  3. Tells Android how to start the app    This is the core part of the app:  1. Displays the view  when the app is created 2. Listen for clicks on the Calculate - button 3. When clicked it grabs the input from the input-field, calculates that result and displays in the output-field.   Holds a static layout of the user interface:   Finally a XML based file for string values when required:   Ok, so now we have an Android app, but how do we build it and deploy it?  Time for Gradle...  The Gradle build file, , looks like:   1. The first thing we see is a  directive. Gradle use this to find dependencies for the build script it self, e.g. plugins. In our case Gradle tooling for Android. 2. Next we can see how the Android tooling is applied using the  directive. 3. Thirdly are some Android tooling specific  properties that relates how the Android plugin should compile and build the apk-file. 4. After that we can recognize the Gradle version of how to specify  and  for our own source code. Here we can see that our code depends on joda-time v2.3 and that is should be downloaded from Maven Central Repository. 5. Finally we see a `task wrapper`construct we are not used to in the Maven world. This is used to define what version of Gradle that should be used to execute the build script and also directs a Gradle wrapper  before running the build script. This means that you don't have to pre-install Gradle!  This is something else then Maven's bloated XML syntax, right?  If you still are in doubt then take some time to study the examples available at [the guides on spring.io]  Ok, let's take it out for a spin, shall we?  First warm up by asking what tasks it can process:   That's useful information if you are new to Gradle, right?  Now let's build the deployable apk-file:   You can now find the file in the  - folder.  To deploy it to a Android test device simply attach it to your computer with a USB cable and give the command:   The execution should end with something like:   **Note:** You Android device has to be in [developer mode] to accept installation of an app this way.  Head over to you Android device and look for the app. You should find it like:  ![]  Start it and you should see something similar to:  ![]  Fill in some number and hit the Calculate button:  ![]  It works, Gradle successfully built and deployed the Android app!  Let's wrap up this introduction with a few words on IDE integration. Gradle comes , is under development. Android Studio supports Gradle natively out of the box. Just start its "import command" and point out your  file:  ![]  Wait a few seconds and then you have everything setup as specified in the  file and you can start working, e.g. refining the layout with the drag and drop tooling that comes with Android Studio:  ![]  This looks very promising indeed! There are some aspects still to be investigated, such as handling of multi module projects and release handling, but we have decided to step up and use Gradle in a full scale project. If that succeeds I guess it's time to leave Maven behind. I'll be back with a follow up on that. Meanwhile I would like to encourage all Maven and Ant users out there to carefully look into Gradle, hopefully you will upgrade as well!
Service Component Architecture  moves on both in its adoption and in its evolution:  BEA has announces support for SCA v. 1.0 in its Project Genesis preview. This is the first offering from the major SOA infrastructure suppliers. IBM did announce prototypical support for SCA 0.9 in a SOA support-pack for WebSphere Application Server a while ago. This was based on a very unstable, and feature-lacking version of the Apache SCA project labeled Tuscany. No tooling was included. BEA releases a preview of a fully integrated SCA implementation, including tooling, as part of Web Logic 10.3.2, targeting GA for second half of 2008.  The spec it self has reached an important milestone, in that a draft transaction policy has been. There is now a technology-independent model for declaring transaction semantics on operations of SCA services, including Java-based annotations. This will then become a unified annotation model across WS-*, EJB 3 and Spring transaction annotations.
I'm currently working on a project where we, despite its [many drawbacks] users: BankId, Nordea and Telia. The whole solution is sometimes referred to as _electronic identification_, or simply e-id.  Fortunately, the server side parts have been packaged together into one common interface, in our case hosted by a third party. Even so we still have to handle the fact that the three solutions use different clients, which means generation of different client side code. Nordea and Telia uses different installed PKI-clients whereas BankId currently uses an applet . In all cases, multiple steps are needed to gather the information needed by the authentication service:  1. Ask the user which of the three solutions he/she uses. 2. For some clients: call the authentication service to generate a "challenge". 3. Present the appropriate client side code to the user . 4. Gather user input and send authentication request to the authentication service.  This means no simple one-step, form-based authentication.  The application is built on WebSphere Portal, so one of our main questions right now is how to best integrate the e-id authentication into WebSphere? My initial thought was that this must surely have been done by someone else, before us. If not with WebSphere Portal, then at least with WebSphere application server. But no such luck; even if someone indeed has done it we haven't found anyone willing to share their solution.  Currently we have three different tracks which we are discussing. None of them have been verified, so they are all more or less hypothetical at the moment.   We implement a simple Java-based solution on top of the Portal instead of integrating with WebSphere's security features.  Pros:  - Straightforward, we know how to do this and we can reuse the general code examples provided for us.  Cons:  - No full integration means we can't use portal specific roles, i.e. we can't configure which portal pages and portlets a specific user can see depending on his/her role. Even though this is a common way to use a portal our particular application won't use those features anyway so this is ok by us. - We have to manually handle storing and retrieving information about the logged in user in the session . - If we need it, we have to manually handle SSO between the portal and the backend systems. Currently our users aren't actors in any of the backend systems anyway  but there might be cases like that in the future.   WebSphere security is based on the [JAAS standard], so integration could be done by implementing a custom JAAS-login module for e-id.  Pros:  - JAAS is a Java standard so the solution might be reusable elsewhere.  Cons:  - The solution still requires WebSphere specific code and configuration. - Implementing a custom JAAS login module and integrating it with WebSphere Portal isn't trivial. In fact, we haven't yet verified that this is a possible solution. For example we don't know how to handle the generation of the different client side code .   We use a separate solution altogether for authentication .  Pros:  - Other applications can take advantage of the same solution. - There are already integrations  available for some of the available SSO reverse proxy products on the market. - Implementing SSO against the backend systems will probably be easier than with our own custom solution .  Cons:  - The integration is a WebSphere specific solution, i.e. we cannot reuse the TAI to integrate with another  server. - The project/organisation has to cover the cost of another  product and get the specific competence needed for configuring it. - We have to integrate the e-id authentication with the reverse proxy. Nothing says that this is any simpler than integrating with WebSphere directly. However, it might be worth it considering others can reuse the solution once it's there.  From a "Keep it Simple, Stupid"-perspective, alternative one is the most appealing. It's the simplest and probably cheapest solution that still covers all of our  requirements and the ability to reuse proven code examples is a big plus.  However, currently we are leaning slightly towards alternative number three, mainly because there seem to be other applications who also want to use e-id for authentication. Having a separate and reusable solution in place might be the cheapest solution in the long run. Since we haven't started looking seriously at any of the products on the market yet, we still have quite a long way ahead of us.
 While Web Services  them back again and/or setting up huge memory heaps on their servers and so on…  On top of that, many companies still use FTP for sending files unencrypted and resulting in a lot of usernames/passwords cluttered all over the system landscape, where instead SFTP can provide a secure transport and more easy managed authentication through its use of SSH and PKI.  In this blog I will describe how I have been working with some of our customers to help them find inexpensive and simple , to achieve large file transfer and transformation.  -[readmore]-  It turned out during this work that the current status of some of the open source components we decided to use did not meet all our requirements. But since we are talking about open source there is a very good solution to that, add the missing pieces yourself  and bring it back to the open source project!  The open source projects mentioned below lives in the [MuleForge], a hosting site and community for development and sharing of open source extensions to Mule ESB and that is where my contributions ended up.  In late 2009, I was involved in a customer project using Mule ESB. In this project we needed to transfer large files but we didn’t want to use FTP. In fact we strived to replace FTP with SFTP.  We started to evaluate the SFPT transport in Mule ESB . We found out that the SFTP transport, thanks to its streaming capabilities, could handle large files very well but there were a number of other features required by the customer that were missing in the transport, such as archive functionality, handling of duplicate file names, file-size checks to determine if a file is completely created before consuming it and so on.  To address these shortcomings I decided to join the SFTP-transport project. A couple months later the project released a new version of the transport that solved the mentioned concerns. Since then, our customer has been using the SFTP-transport in production.  Below is a very basic example of a file transfer from one SFTP-server to another SFTP-server applying a Java based transformation of the content. If you download the source code of the transport and take a look in the integration tests and their [mule-config-files], you can find many other examples of its use.  ![]  **Note #1:** The example assumes usage of PKI-keys and not old style username/password.  **Note #2:** In a real-world case attributes such as address would not be hardcoded but instead be provided as a configurable property.  Over time the SFTP-transport has been used more and more widely and in early 2011, MuleSoft decided to add the SFTP-transport as a core transport of the Mule ESB product. Since Mule ESB v3.1.2 the SFTP-transport is part of the Mule ESB distribution.  …and my work as a MuleForge committer was over for the time being…  Earlier this year !  The way to integrate Smooks in Mule ESB is to use the [Smooks Module for Mule] at the MuleForge, i.e. Smooks support in Mule is not a core part of the Mule ESB product.  Using the Smooks Module you can for example easily replace the Java based transformer in the SFTP-transport example above with a much more versatile Smooks based transformer by replacing the line:   with:   A problem I found out working with Smooks and Mule was that the Smooks Module for Mule was not yet released for Mule 3, only for Mule 1.x and Mule 2.x. However, all changes required for Mule 3 was already developed and committed but no one have had time to do the final testing and release management for a Mule 3 compatible release.  …so I decided to join in again, this time with the goal to release a Mule 3 compatible version of the Smooks Module!  Last weekend we released a release candidate for the next version, v1.3-RC1, which brings in compatibility with Mule 3.1.x. See [Smooks for Mule v1.3-RC1 released] for details!  A very good example of what the Smooks framework is capable of doing for you is the example in the Smooks distribution called **freemarker-huge-transform**. It demonstrates a setup where Smooks transforms very large XML-files containing order heads and order lines. An outer streaming SAX-parser is setup to stream through the order head elements and delegates the processing of the order lines to a DOM-parser. This means that the memory intensive DOM models only exist for one order line at the time resulting in a very small memory footprint even for transforming very large files. Freemarker is used as the templating language to declare what the actual transformation should do with each order head and order line.  Copy of the [smooks-config – file] from the example, comments removed to keep the size down:  ![]  Going through the example in detail is out of the scope for this blog  but some interesting aspects are worth mentioning:  - The outer sax-parser that read order lines with a streaming approach:   - The two Freemarker based templates that work together. The outer template handling transformation of order-heads  and the inner template transforming order-lines one-by-one using a traditional DOM-parser. - The Freemarker templates declare the outline of the transformed outgoing message format and the variables  refers to elements in the incoming message and declares where their corresponding values should be placed in the outgoing transformed message.  By combining a couple of open source products and making some contributions of your own, if required, you can construct very cost-effective, low-complex but still very robust solutions to problems that traditionally are concerned to be very hard, complex and expensive to solve.  **Feel the power of open source!**
 If you tried out the earlier blog posts in our [blog series] regarding building microservices I guess you are tired of starting the microservices manually at the command prompt? Let's dockerize our microservices, i.e. run them in Docker containers, to get rid of that problem!  -[readmore]-  I assume that you already heard of Docker and the container revolution? If not, there are tons of introductory material on the subject available on Internet, e.g. [Understanding Docker].  This blog post covers the following parts:  1. Install and configure Docker 1. Build Docker images automatically with Gradle 1. Configure the microservices for a Docker environment using Spring profiles 1. Securing access to the microservices using HTTPS 1. Starting and managing you Docker containers using Docker Compose 1. Test the dockerized microservices 1. Happy days 2. Scale up a microservice 3. Handle problems 1. Summary 1. Next Step   Install Docker by following the [instructions for your platform].  > We used [Boot2Docker v1.6.2 for Mac] when we wrote this blog post.   First, since we will start a lot of fine grained Java based microservices we will need some more memory  than the default value in the Linux virtual server that Boot2Docker creates for us.  The simplest way to do that is to stop Boot2Docker and recreate the virtual server with new memory parameters. If you upgraded Boot2Docker from an older version you should also perform a download of the virtual server to ensure that you have the latest version.  Execute the following commands to create a new virtual server with 4GB memory:  $ boot2docker stop $ boot2docker download $ boot2docker delete $ boot2docker init -m 4096 $ boot2docker info  $ boot2docker start  The start command might ask you to set a number of environment variables like:  export DOCKER_HOST=tcp://192.168.59.104:2376 export DOCKER_CERT_PATH=/Users/magnus/.boot2docker/certs/boot2docker-vm export DOCKER_TLS_VERIFY=1  Add them to you preferred config-file, e.g.  as in my case.  Now you can try Docker by asking it to start a CentOS based Linux server:  $ docker run -it --rm centos  Docker will download the corresponding Docker image the first time it is used, so it takes some extra time depending on your Internet connection.  Just leave the server, for now, with an  command:  [root@fd5773461402 /]# exit  > If you try to start a new CentOS server with the same command as above you will experience the magic startup time of a brand new server when using Docker, typically a sub-second startup time!!!  Secondly, add a line in your  - file for easier access to the Docker machine. First get the IP address of the Linux server that runs all the Docker containers:  $ boot2docker ip 192.168.59.104  Add a line in your  file like:  192.168.59.104 docker  Now you can, for example,  your docker environment like:  $ ping docker PING docker : 56 data bytes 64 bytes from 192.168.59.103: icmp_seq=0 ttl=64 time=0.822 ms 64 bytes from 192.168.59.103: icmp_seq=1 ttl=64 time=4.341 ms 64 bytes from 192.168.59.103: icmp_seq=2 ttl=64 time=1.454 ms   We have extended the Gradle build files from the earlier [blog posts].  As before we use Java SE 8 and Git so to get the source code perform:   The most important additions to the Gradle build-files are:  buildscript { dependencies { classpath 'se.transmode.gradle:gradle-docker:1.2'  ...  apply plugin: 'docker'  ...  group = 'callista' mainClassName = 'se.callista.microservises.core.review.ReviewServiceApplication'  ...  distDocker { exposePort 8080 setEnvironment 'JAVA_OPTS', '-Dspring.profiles.active=docker' }  docker { maintainer = 'Magnus Larsson <magnus.larsson.ml@gmail.com>' baseImage = 'java:8' }  **Notes:**  1. First we declare a dependency to the  and apply the  - plugin.  2. Next, we setup a variable, , to give the Docker images a common group-name and another variable, , to declare the main-class in the microservice.  3. Finally, we declare how the Docker image shall be built in the  and  declarations. See [plugin documentation] for details.  4. One detail worth some extra attention is the declaration of the  environment variable that we use to specify what Spring profile that the microservice shall use, see below for details.  We have also added the new task  to the build commands in .  Execute the  file and it will result in a set of Docker images like:  $ ./build-all.sh ... lots of output ...  $ docker images | grep callista callista/turbine latest 8ea25912aad7 43 hours ago 794.6 MB callista/monitor-dashboard latest f443c2cde704 43 hours ago 793 MB callista/edge-server latest b32bb74788ac 43 hours ago 826.6 MB callista/discovery-server latest 8eceaff6cc6b 43 hours ago 838.3 MB callista/auth-server latest 90041b13c564 43 hours ago 766.1 MB callista/product-api-service latest 5081a18b9cac 44 hours ago 801.9 MB callista/product-composite-service latest c200820d6cdf 44 hours ago 800.6 MB callista/review-service latest 1796c14c2a5a 44 hours ago 786.1 MB callista/recommendation-service latest 4f4e490cb409 44 hours ago 786.1 MB callista/product-service latest 5ed6a9620bce 44 hours ago 786.1 MB   To be able to run in a Docker environment we need to change our configuration a bit. To keep the Docker specific configuration separate from the rest we use a [Spring Profile] called  in our -files, e.g.:  --- # For deployment in Docker containers spring: profiles: docker  server: port: 8080  eureka: instance: preferIpAddress: true client: serviceUrl: defaultZone: http://discovery:8761/eureka/  **Notes:**  1. Since all microservices will run in their own Docker container  we don't need to care about port collisions. Therefore we can use a fixed port, , instead of dynamically assign ports as we have done until now.  2. Register our microservices to Eureka using hostnames in a Docker environment will not work, they will all get one and the same hostname. Instead we configure them to use its IP address during registration with Eureka.  3. The discovery service will be executing in a Docker container known under the name , se below for details, so therefore we need to override the setting of the .   Docker runs the containers in a closed network. We will expose as few services outside of the internal Docker network as possible, e.g. the OAuth Authorization server and the Edge server. This means that our microservices will not be accessible directly from the outside. To protect the communication with the exposed services we will use HTTPS, i.e. use server side certificates, that will protect the OAuth communication from unwanted eavesdropping. The OAuth Authorization server and the Edge server uses a [self-signed certificate] that comes with the source code of this blog post.  > Don't use this self-signed certificate for anything else than trying out our blog posts, it is not secure since its private part is publicly available in the source code!  Our API - microservice needs to communicate with the OAuth Authorization server to get information regarding the user . Therefore it needs to be able to act as a HTTPS client, validating the certificate that the OAuth Authorization service presents during the HTTPS communication. The API - microservice uses a trust store that comes with the source code of this blog post for that purpose.  For the OAuth Authorization server and the Edge server you can find the  private certificate, , in the folder  and the  file in the same folder contains the SSL - configuration, e.g:  server: ssl: key-store: classpath:server.jks key-store-password: password key-password: password  The API - microservice has its trust store, , and its configuration file in the folder  as well. The SSL configuration looks a bit different since it only will act as a HTTPS-client:  server: ssl: enabled: false # Problem with trust-store properties? # # Instead use: java -Djavax.net.debug=ssl -Djavax.net.ssl.trustStore=src/main/resources/truststore.jks -Djavax.net.ssl.trustStorePassword=password -jar build/libs/*.jar # # trust-store: classpath:truststore.jks trust-store: src/main/resources/truststore.jks trust-store-password: password  As you can see in the comment above we have experienced some problems with defining the trust store via the configuration file. Instead we use the  environment variable to specify it. If you look into the  - file of the API - microservice you will find:  distDocker { ... setEnvironment 'JAVA_OPTS', '-Dspring.profiles.active=docker -Djavax.net.ssl.trustStore=truststore.jks -Djavax.net.ssl.trustStorePassword=password'   To be able to start up and manage all our services with single commands we use [Docker Compose].  > We used [Docker Compose v1.2.0] when we wrote this blog post.  With Docker Compose you can specify a number of containers and how they shall be executed, e.g. what Docker image to use, what ports to publish, what other Docker containers it requires to know about and so on...  For a simple container that don't need to know anything about other containers the following is sufficient in the configuration file, :  rabbitmq: image: rabbitmq:3-management ports: - "5672:5672" - "15672:15672"  discovery: image: callista/discovery-server ports: - "8761:8761"  auth: image: callista/auth-server ports: - "9999:9999"  This will start up RabbitMQ, a discovery server and a OAuth Authorization server and publish the specified ports for external access.  To start up services that need to know about other containers we can use the  - directive. e.g. for the API microservice:  api: image: callista/product-api-service links: - auth - discovery - rabbitmq  This declaration will result in that the /etc/hosts file in the API container will be updated with one line per service that the API microservice depends on, e.g.:  172.17.0.23 auth 172.17.0.27 discovery 172.17.0.25 rabbitmq   Ok, we now have all the new bits and pieces in place so we are ready to give it a try!  For an overview of the microservice landscape we are about to launch see previous blog posts, specifically [Part 1].  Start the microservice landscape with the following command:  $ docker-compose up -d  **Note:** We have, a few times, noticed problems with downloading Docker images . But after recreating the Boot2Docker virtual server as described in *1.1 Configuration when using Boot2Docker* the download worked again.  This will start up all ten Docker containers. You can see its state withe the command:  $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------------------- blogmicroservices_api_1 /product-api-service/bin/p ... Up 8080/tcp blogmicroservices_auth_1 /auth-server/bin/auth-server Up 0.0.0.0:9999->9999/tcp blogmicroservices_composite_1 /product-composite-service ... Up 8080/tcp blogmicroservices_discovery_1 /discovery-server/bin/disc ... Up 0.0.0.0:8761->8761/tcp blogmicroservices_edge_1 /edge-server/bin/edge-server Up 0.0.0.0:443->8765/tcp blogmicroservices_monitor_1 /monitor-dashboard/bin/mon ... Up 0.0.0.0:7979->7979/tcp blogmicroservices_pro_1 /product-service/bin/produ ... Up 8080/tcp blogmicroservices_rabbitmq_1 /docker-entrypoint.sh rabb ... Up 0.0.0.0:15672->15672/tcp, 0.0.0.0:5672->5672/tcp blogmicroservices_rec_1 /recommendation-service/bi ... Up 8080/tcp blogmicroservices_rev_1 /review-service/bin/review ... Up 8080/tcp  You can monitor log output with the command:  $ docker-compose logs ... rec_1 | 2015-06-01 14:20:55.295 cfbfc65f-8a5f-41cc-8710-51856105bf62 recommendation INFO XNIO-2 task-13 s.c.m.c.r.s.RecommendationService:53 - /recommendation called, processing time: 0 rec_1 | 2015-06-01 14:20:55.296 cfbfc65f-8a5f-41cc-8710-51856105bf62 recommendation INFO XNIO-2 task-13 s.c.m.c.r.s.RecommendationService:62 - /recommendation response size: 3 ...   ...and as usual you can see the registered microservices in our discovery service, Eureka, using the URL :  ![system-landscape]    Let's try a happy days scenario, shall we?  First we need to access a OAuth Token using HTTPS :  $ curl https://acme:acmesecret@docker:9999/uaa/oauth/token \ -d grant_type=password -d client_id=acme \ -d username=user -d password=password -ks | jq . { "access_token": "d583cc8d-5431-4241-afbf-6c6e686899d8", "token_type": "bearer", "refresh_token": "cf3e2136-6fb3-4c23-b3ce-0d5118b5d538", "expires_in": 43199, "scope": "webshop" }  Store the Access Token in an environment variable as before:  $ TOKEN=d583cc8d-5431-4241-afbf-6c6e686899d8  With the Access Token we can now access the API, again over HTTPS:  $ curl -H "Authorization: Bearer $TOKEN" \ -ks 'https://docker/api/product/1046' | jq . { "productId": 1046, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ] }  Great!   Ok, let's spin up a second instance of one of the microservices. This can be done using the  -command:  $ docker-compose scale rec=2  **Note:**  is the name we gave the  microservice in the docker-compose configuration file, .  This command will start up a second instance of the  microservice:  $ docker-compose ps rec Name Command State Ports --------------------------------------------------------------------------- blogmicroservices_rec_1 /recommendation-service/bi ... Up 8080/tcp blogmicroservices_rec_2 /recommendation-service/bi ... Up 8080/tcp  If you call the API several times with:  $ curl -H "Authorization: Bearer $TOKEN" \ -ks 'https://docker/api/product/1046' | jq .  If you run the `docker-compose logs` command in a separate window, you will notice in the log-output, after a while, that the two -services take every second call.  rec_2 | 2015-06-01 14:20:54.357 cb6ad766-c385-442b-afbe-de9222221a23 recommendation INFO XNIO-2 task-2 s.c.m.c.r.s.RecommendationService:53 - /recommendation called, processing time: 0 rec_2 | 2015-06-01 14:20:54.358 cb6ad766-c385-442b-afbe-de9222221a23 recommendation INFO XNIO-2 task-2 s.c.m.c.r.s.RecommendationService:62 - /recommendation response size: 3  ...  rec_1 | 2015-06-01 14:20:55.295 cfbfc65f-8a5f-41cc-8710-51856105bf62 recommendation INFO XNIO-2 task-13 s.c.m.c.r.s.RecommendationService:53 - /recommendation called, processing time: 0 rec_1 | 2015-06-01 14:20:55.296 cfbfc65f-8a5f-41cc-8710-51856105bf62 recommendation INFO XNIO-2 task-13 s.c.m.c.r.s.RecommendationService:62 - /recommendation response size: 3  Good, not let's cause some problems in the microservice landscape!   Let's wrap up the tests with introducing an error and see how our circuit breaker introduced in [Part 2] acts in an Docker environment.  We have a backdoor in the review microservice that can be used to control its response times. If we increase the response time over the timeout configured in the circuit-breaker it will kick in...  The problem is that we can't access that backdoor from the outside . To access the backdoor we need access to a sever that runs inside the closed Docker container network. Let's start one!  $ docker run -it --rm --link blogmicroservices_rev_1:rev centos [root@bbd3e4154803 /]#  That wasn't that hard, was it?  Ok, now we can use the backdoor to set the response time of the review microservice to 10 secs:  [root@bbd3e4154803 /]# curl "http://rev:8080/set-processing-time?minMs=10000&maxMs=10000"  Exit the container and retry the call to the API. It will respond very slowly  and will not contain any review information:  [root@bbd3e4154803 /]# exit  $ curl -H "Authorization: Bearer $TOKEN" -ks \ 'https://docker/api/product/1046' | jq . { "productId": 1046, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": null }  If you retry the call once more you will again see a very long response time, i.e. the circuit breaker has not opened the circuit yet. But if you perform two requests directly after each other the circuit will be opened.  > We have configured the circuit breaker to be very sensitive, just for demo purposes.  This can be seen in the circuit breakers dashboard like:  ![system-landscape]  **Note:** URL to the circuit breaker: [http://docker:7979/hystrix/monitor?stream=http%3A%2F%2Fcomposite%3A8080%2Fhystrix.stream]  If you retry a call you will see that you get an **immediate response**, still without any review information of course, i.e. the circuit breaker is now open.  Let's heal the broken service! If you start a new container and use the backdoor to reset the response time of the review microservice to 0 sec and then retry the call to the API everything should be ok again, i.e. the system is self-healing!  $ docker run -it --rm --link blogmicroservices_rev_1:rev centos [root@bbd3e4154803 /]# curl "http://rev:8080/set-processing-time?minMs=0&maxMs=0" [root@bbd3e4154803 /]# exit  $ curl -H "Authorization: Bearer $TOKEN" \ -ks 'https://docker/api/product/1046' | jq . { "productId": 1046, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ] }  **Note:** The circuit breaker is configured to probe the open circuit after 30 seconds to see if the service is available again, i.e. to see if the problem is gone so it can close the circuit again. Probing is done by letting one of the incoming request through, even though that the circuit actually is open.   We have seen how we with very little effort can dockerize our microservices and run our microservices as Docker containers. Gradle can help us to automatically build Docker images, Spring profiles can help us to keep Docker specific configuration separate from other configuration. Finally Docker Compose makes it possible to start and manage all microservices, used for example by an application, with a single command.   We have already promised to demonstrate how the ELK stack can be used to provide centralized log management of our microservices. Before we demonstrate that we however need to consider how to correlate log event written by various microservices to its own log-files. So that will be the target for the next blog post in the [Blog Series - Building Microservices], stay tuned...
 In part 9 of the Go microservices [blog series], we'll examine messaging between Go microservices using RabbitMQ and the AMQP protocol.  Microservices is all about separating your application's business domain into bounded contexts with clearly separated domains, running with process separation where any persistent relations across domain boundaries has to rely on eventual consistency rather than ACID-like transactions or foreign key constraints. A lot of these concepts comes from or has been inspired by [Domain-driven design]. That's yet another huge topic one could write a blog series about.  In the context of our Go microservice [blog series] and microservice architecture in general, one pattern for accomplishing loose coupling between services is to use messaging for inter-service communication that doesn't need a strict request/response message interchange or similar. That said, using messaging is just one of many strategies one can adopt to facilitate loose coupling between services.  In Spring Cloud, RabbitMQ seems to be the message broker of choice, especially since the Spring Cloud Config server has RabbitMQ as a runtime dependency as we saw in [part 8] of the blog series.  For this part of the blog series, we'll make our "accountservice" place a message on a RabbitMQ _Exchange_ whenever a particular account object has been read. This message will be consumed by a brand new microservice we'll write in this blog post. We'll also deal with reusing Go code across multiple microservices by putting them in a "common" library we can import into each service.  Remember the system landscape image from Part 1? Here's an image of what it'll look like after this part has been finished:  ![part9 overview]  There's still a _lot_ of stuff missing until we're done. Don't worry, we'll get there.  There will be a lot of new source code for this part and not all of it will be included in the blog text. For the complete source, clone and switch to the branch for part 9:  git checkout P9  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  We'll implement a simple make-believe use case: When certain "VIP" accounts are read in the "accountservice", we want to notify an "VIP offer" service that under certain circumstances will generate an "offer" for the account holder. In a properly designed domain model, the accounts objects and VIP offer objects are two independent domains that should have as little knowledge of each other as possible.  ![separation]  I.e - the accountservice should **never** access the storage of the VIP service  directly. In this case, we're passing a message to the "vipservice" over RabbitMQ fully delegating both business logic and persistence to the "vipservice".  We'll do all communication using the [AMQP], just like in part 8 when we consumed configuration updates.  Let's repeat how _exchanges_ in AMQP relates to _publishers_, _consumers_ and _queues_:  ![amqp]  I.e - a message is published to an _exchange_, which then distributes message copies to _queue on Quora for a good explanation.  Since we'll want to use our new messaging code as well as our existing code for loading configuration from Spring Cloud config in both our existing _accountservice_ and the new _vipservice_, we'll create our first shared library.  Start by creating new folders under _/goblog/_ called _common_ to keep our new reusable stuff:  > mkdir -p common/messaging > mkdir -p common/config  We'll put all AMQP-related code in the _messaging_ folder and the configuration stuff in the _config_ folder. You can copy the contents of _/goblog/accountservice/config_ into _/goblog/common/config_ - remember that this will require us to update the _import_ statements previously importing our config code from within the _accountservice_. Just take a look at the [finished source] to see how it's supposed to be.  The messaging code will be encapsulated in a single file that will define both the interface our applications will use to connect, publish and subscribe as well as the actual implementation. In all honesty, there is a lot of boilerplate code required for AMQP-messaging using streadway/amqp so don't get bogged down in the details.  Create a new .go file in _/goblog/common/messaging_: [messagingclient.go]  Let's have a look at the important stuff  // Defines our interface for connecting, producing and consuming messages. type IMessagingClient interface { ConnectToBroker Publish error PublishOnQueue error Subscribe error SubscribeToQueue error Close }  The snippet above defines our messaging interface. This is what our "accountservice" and "vipservice" will deal with when it comes to messaging, hopefully abstracting away most complexity. Note that I've chosen two variants of "Produce" and "Consume" to use with _topics_ and _direct/queue_ messaging patterns.  Next, we'll define a struct which will hold a pointer to an amqp.Connection and that we will attach the requisite methods to so it  implements the interface we just declared.  // Real implementation, encapsulates a pointer to an amqp.Connection type MessagingClient struct { conn *amqp.Connection }  The implementations are quite verbose so let's limit ourselves to two of them - _ConnectToBroker_:  func  { if connectionString == "" { panic }  var err error m.conn, err = amqp.Dial if err != nil { panic } }  This is how we get hold of the connection pointer, e.g. _amqp.Dial_. If we're missing our config or cannot contact our broker, we'll panic our microservice and let the container orchestrator try again with a fresh instance. The passed connection string looks like:  amqp://guest:guest@rabbitmq:5672/  Note that we're using the Docker Swarm mode _service_ name of the rabbitmq broker.  The _PublishOnQueue, though I've simplified it a bit with fewer parameters. To publish a message to a named queue, all we need to pass is:  - body in the form of a byte array. Could be JSON, XML or some binary. - queueName - name of the queue you want to send your message to.  For more details about exchanges, see the [RabbitMQ docs].   func  error { if m.conn == nil { panic } ch, err := m.conn.Channel // Get a channel from the connection defer ch.Close  // Declare a queue that will be created if not exists with some args queue, err := ch.QueueDeclare( queueName, // our queue name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments )  // Publishes a message onto the queue. err = ch.Publish( "", // use the default exchange queue.Name, // routing key, e.g. our queue name false, // mandatory false, // immediate amqp.Publishing{ ContentType: "application/json", Body: body, // Our JSON body as []byte }) fmt.Printf return err }   A bit heavy on the boilerplate, but should be easy enough to understand. Declare the queue  for an example.  Moving on, the actual user of our "MessageClient" will be _/goblog/accountservice/service/handlers.go_, so we'll add a field for that and the hard-coded "is VIP" check that will send a message if the requested account was has id "10000":  var DBClient dbclient.IBoltClient var MessagingClient messaging.IMessagingClient // NEW  func GetAccount { ...  a bit further down  ... notifyVIP // Send VIP notification concurrently.  // If found, marshal into JSON, write headers and content data, _ := json.Marshal writeJsonResponse }  // If our hard-coded "VIP" account, spawn a goroutine to send a message. func notifyVIP { if account.Id == "10000" { go func { vipNotification := model.VipNotification data, _ := json.Marshal err := MessagingClient.PublishOnQueue if err != nil { fmt.Println } } } }  Taking the opportunity to showcase an inlined anonymous function that we're calling on a new goroutine, i.e. using the _go_ keyword. Since we have no reason whatsoever to block the "main" goroutine that's executing the HTTP handler while sending a message, this is a perfect time to add a bit of concurrency.  _main.go_ also needs to be updated so it initializes the AMQ connection on startup using configuration loaded and injected into Viper.  // Call this from the main method. func initializeMessaging { if !viper.IsSet { panic }  service.MessagingClient = &messaging.MessagingClient service.MessagingClient.ConnectToBroker service.MessagingClient.Subscribe }  No big deal - we're assigning the _service.MessagingClient_ instance by creating an empty messaging struct and the calling ConnectToBroker using a property value fetched from Viper. If our configuration doesn't contain a _broker_url_, we panic as we don't want to be running without even the possibility to connect to the broker.  We added the _amqp_broker_url_ property to our .yml config files back in part 8, so that's already been taken care of.  broker_url: amqp://guest:guest@192.168.99.100:5672 __  broker_url: amqp://guest:guest@rabbitmq:5672 __  Note that for the "test" profile, we're using the Swarm Service name "rabbitmq" instead of the LAN IP address of the Swarm as seen from my dev laptop. .  As for having clear-text usernames and passwords in configuration files that's not recommended, in a real-life scenario one could typically use the built-in encryption feature of the Spring Cloud Config server we looked at in Part 8.  Naturally, we should at least write a unit test that makes sure our _GetAccount_ function in _handlers.go_ does try to send a message whenever someone requests the magical and very very special account identified by "10000".  For this - we need a mock implementation of the IMessagingClient and a new test case in _handlers_test.go_. Let's start with the mock. This time we'll use the 3rd party tool [mockery] to generate a mock implementation of our IMessagingClient interface: __  > go get github.com/vektra/mockery/.../ > cd $GOPATH/src/github.com/callistaenterprise/goblog/common/messaging > ./$GOPATH/bin/mockery -all -output . Generating mock for: IMessagingClient  Now we have a mock file _IMessagingClient.go_ in our current folder. I don't like the name of the file nor the camelcasing, so we'll rename it to something that makes it evident that it's a mock and follows the conventions for file names used in the blog series:  mv IMessagingClient.go mockmessagingclient.go  It's possible you'll need to adjust the imports somewhat in the generated file, removing the import aliases. Other than that, we'll use a black-box approach to this particular mock - just assume it'll work when we start writing tests.  Feel free to examine the [source] of the blog series.  Moving on to _handlers_test.go_ we're adding a new test case:  // declare mock types to make test code a bit more readable var anyString = mock.AnythingOfType var anyByteArray = mock.AnythingOfType // == []byte   func TestNotificationIsSentForVIPAccount { // Set up the DB client mock mockRepo.On DBClient = mockRepo  mockMessagingClient.On MessagingClient = mockMessagingClient  Convey { req := httptest.NewRequest resp := httptest.NewRecorder Convey { NewRouter Convey { So time.Sleep // Sleep since the Assert below occurs in goroutine So }) })}) }  For details, follow the comments. I don't like that artificial 10 ms sleep just before asserting numberOfCalls, but since the mock is called in a goroutine separate from the "main thread" we need to allow it a tiny bit of time to complete. Hope there's a better idiomatic way of unit-testing when there's goroutines and channels involved.  I admit - mocking this way is more verbose than using something like Mockito when writing unit-tests for a Java application. Still, I think it's quite readable and easy enough to write.  Make sure that the test passes:  go test ./...  If you havn't, run the the _springcloud.sh_ script to update the config server. Then, run _copyall.sh_ and wait a few seconds while our "accountservice" is updated. We'll use curl to fetch our "special" account.  > curl http://$ManagerIP:6767/accounts/10000   If things went well, we should be able to open the RabbitMQ admin console and see if we've gotten a message on a queue named _vipQueue_.  open http://192.168.99.100:15672/#/queues  ![rabbitmq with 1 message in queue]  At the very bottom of the screenshot above, we see that the "vipQueue" has 1 message. If we use the "Get Message" function within the RabbitMQ admin console, we can look at this message:  ![the message]  Finally, it's time to write a brand new microservice from scratch that we'll use to showcase how to consume a message from RabbitMQ. We'll make sure to apply the patterns we've learned in the blog series up until now, including:  - HTTP server - Health check - Centralized configuration - Reuse of messaging code  If you've checked out the P9 source you can already see the "vipservice" in the root _/goblog_ folder.  I won't go through every single line of code here as some parts are repeated from the "accountservice". Instead we'll focus on the consuming the message we just sent. A few things to note:  - Two new .yml files added to the config-repo, _vipservice-dev.yml_ and _vipservice-test.yml_ - _copyall.sh_ has been updated so it builds and deploys both the "accountservice" and our new "vipservice".  We'll use the code from _/goblog/common/messaging_ and the _SubscribeToQueue_ function, e.g:  SubscribeToQueue error  Of most note here is the that we're supposed to provide:  - the name of the queue  - a consumer name  - a handler function that will be invoked with a received delivery - very similar to what we did when consuming config updates in part 8.  The implementation of _SubscribeToQueue_ that actually binds our callback function to a queue isn't that exciting, check the [source] if you want the details.  Moving on, a quick peek at an excerpt of the vipservice's _main.go_ shows how we're setting things up:  var messagingClient messaging.IMessagingConsumer  func main { fmt.Println  config.LoadConfigurationFromBranch initializeMessaging  // Makes sure connection is closed when service exits. handleSigterm { if messagingClient != nil { messagingClient.Close } }) service.StartWebServer }  // The callback function that's invoked whenever we get a message on the "vipQueue" func onMessage { fmt.Printf }  func initializeMessaging { if !viper.IsSet { panic } messagingClient = &messaging.MessagingClient messagingClient.ConnectToBroker  // Call the subscribe method with queue name and callback function err := messagingClient.SubscribeToQueue failOnError  err = messagingClient.Subscribe failOnError }  Looks familiar, right? We'll probably repeat the basics of how to setup and boot each microservice we add.  The _onMessage_ function just logs the body of whatever "vip" message we receive. If we would implement more of our make-believe use case it would have invoked some fancy logic to determine if the account holder was eligible for the _"super-awesome buy all our stuff   Not much to add. Except this snippet that allows us to clean up whenever we press Ctrl+C or when Docker Swarm thinks it's time to kill a service instance:  func handleSigterm { c := make signal.Notify signal.Notify go func { <-c handleExit os.Exit } }  Not the most readable piece of code, what is does is that it registers the channel "c" as listener for os.Interrupt and syscall.SIGTERM and a goroutine that will block listening for message on "c" until either of the signals are received. This allows us to be pretty sure that the _handleExit_ function we supplied will be invoked whenever the microservice is being killed. How sure? Ctrl+C or docker swarm scaling works fine. _kill_ does too. _kill -9_ doesn't. So please don't stop stuff using _kill -9_ unless you have to.  It will call that _Close_ func we declared on the IMessageConsumer interface, which in the implementation makes sure the AMQP conn is properly closed.  The _[copyall.sh]_ script has been updated, so if you're following along make sure it's up-to date with branch P9 on github and run it. When everything's done, _docker service ls_ should print something like this:  > docker service ls ID NAME REPLICAS IMAGE kpb1j3mus3tn accountservice 1/1 someprefix/accountservice n9xr7wm86do1 configserver 1/1 someprefix/configserver r6bhneq2u89c rabbitmq 1/1 someprefix/rabbitmq sy4t9cbf4upl vipservice 1/1 someprefix/vipservice u1qcvxm2iqlr viz 1/1 manomarks/visualizer:latest  :  ![dvizz]  Since the _docker service logs_ feature is marked as experimental in 1.13.0, we have to look at the "vipservice" logs the old-school way. First, run _docker ps_ to figure out the container id:  > docker ps CONTAINER ID IMAGE a39e6eca83b3 someprefix/vipservice:latest b66584ae73ba someprefix/accountservice:latest d0074e1553c7 someprefix/configserver:latest  Pick the CONTAINER ID for the vipservice and check its logs using _docker logs -f_:  > docker logs -f a39e6eca83b3 Starting vipservice... 2017/06/06 19:27:22 Declaring Queue  2017/06/06 19:27:22 declared Exchange, declaring Queue  2017/06/06 19:27:22 declared Queue  Starting HTTP service at 6868  Open another command shell and curl our special Account object.  > curl http://$ManagerIP:6767/accounts/10000  If everything works, we should see a message being consumed in the log of the original window.  Got a message:   A pattern for distributing work across multiple instances of a service is to utilize the concept of [work queues]. Each "vip message" should be processed by a single "vipservice" instance.  ![workqueue]  So let's see what happens when scale our "vipservice" to two instances using the _docker service scale_ command:  > docker service scale vipservice=2  A new instance of "vipservice" should be available within a few seconds.  Since we're using the _direct/queue_ approach in AMQP we expect round-robin behaviour. Use _curl_ to trigger four VIP account lookups:  > curl http://$ManagerIP:6767/accounts/10000 > curl http://$ManagerIP:6767/accounts/10000 > curl http://$ManagerIP:6767/accounts/10000 > curl http://$ManagerIP:6767/accounts/10000  Check the log of our original "vipservice" again:  > docker logs -f a39e6eca83b3 Got a message:  Got a message:   As expected, we see that the first instance processed two of the four expected messages. If we'd do _docker logs_ for the other "vipservice" instance we'd see two messages there as well. Promise.  Actually - I havn't really come up with an attractive way to unit test the AMQP consumer without spending a ridiculous amount of time mocking the amqp library. There's a test in [messagingclient_test.go] that tests the subscriber loop that waits indefinitely for incoming messages to process, but that's it.  For more thorough testing of messaging, I'll **probably** return to that topic in a future blog post about _integration testing_ Go microservices using _go test_ with the Docker Remote API or Docker Compose. The test would boot supporting services such as RabbitMQ it can use to send and receive actual messages in test code.  Won't do performance tests this time around, a quick peek at memory use after sending and receiving some messages will have to suffice:  CONTAINER CPU % MEM USAGE / LIMIT vipservice.1.tt47bgnmhef82ajyd9s5hvzs1 0.00% 1.859MiB / 1.955GiB accountservice.1.w3l6okdqbqnqz62tg618szsoj 0.00% 3.434MiB / 1.955GiB rabbitmq.1.i2ixydimyleow0yivaw39xbom 0.51% 129.9MiB / 1.955GiB  The above is after serving a few requests. The new "vipservice" is not as complex as the "accountservice" so it's expected it uses less RAM after startup.   That was probably the longest part of the [series] this far! We've accomplished:  - Examined RabbitMQ and the AMQP mechanics in more depth. - Added a brand-new "vipservice". - Extracted messaging  code into a reusable sub-project. - Publish / Subscribe of messages using the AMQP protocol. - Mock code generation with mockery.  In [part 10] provider. 
 I recently participated in Gartner’s ”Application architecture, development & integration” conference, and here I want to share a couple of impressions from that event.  First I want to say that it was a really good conference, both considering the organisation and the speakers, where the latter was a clear step up from The Rich Web Experience, which I attended a year ago. The agenda was mostly targeted at architects and other types of decision-makers, but developers with an eye on ”the bigger picture” could also find a lot of interrest. The architectural view was throughout the conference kept pretty technical and connected to real-world problems.  Microservices are gradually given a more defined place in the larger architectural picture. The view that a microservice is created when needed, to solve problems concerning agility and scalability, seems to be the dominating one, and there was less talk about ”microservice architecture” than ”microservices in the architecture”. Anne Thomas, in her presentation ”Microservices: the future of SOA and PaaS”, delivered the strategic planning assumption that ”By 2017, more than 90% of organizations that try microservices will find the paradigm too distruptive and use miniservices instead”. . The reason for this seemingly negative assumption is that microservices are a highly complex architecture which requires unfamiliar patterns and new infrastructure. Microservices may be the solution to your problems, but you need to make sure you have the right expertise!  SOA was once presented as the solution to the mess created by point-to-point integration. You’ve seen the images: Systems along the edges, and the center filled with arrows representing the integrations. Spaghetti. Enter SOA architecture, services defined on a central ESB, and voila! These days, the services are depicted as the new spaghetti, and they seem to have moved out of the ESB as they are spread out all over the place. Enter the API control gateway  and a separation of inner and other API:s. This brings the order back. It’s magic! Sarcasm aside, part of the reason for this change is that what is defined as a service has changed. It has shifted from mainly beeing the interface definition to including the implementation of the functionality. Services used to access backend systems, now they access data.
 In this blog post, we'll use [Golang] to boot and test a microservice environment.  -[readmore]-  Sometimes testing a microservice environment presents challenges not typically encountered when developing more traditional applications of the monolith kind. For example, our modern microservice environment may consist of a number of containers  has published on this blog and on github as our sample Microservice landscape. It consists of one public-facing service backed by several internal microservices and no less than six supporting services:  - Edge server [Zuul] - OAuth server [Spring] - Discovery server [Eureka] - Config server [Spring] - Monitoring [Hystrix] - Messaging [RabbitMQ]  Want to try/run this yourself? Clone the [source code]!  Part of the challenge here is to make all these components work together, something that requires a non-trivial amount of configuration. Also, making sure things stays that way over time is equally important. Typically, you have a CI/CD pipeline to detect regressions, but with container technology it is quite easy to bootstrap all components on your developer laptop !   This program was actually inspired by a bash shell script and it is certainly just as easy to do this using whatever console-friendly language you prefer. I like Go due to its relative simplicity, [quick compile time], portability and rich set of libraries.   It’s actually a rather trivial task, divided into a number of distinct steps:  1. Define how to test your landscape, we'll use a [YAML] file. 2. Build and launch the Go program, specifying the YAML file you just filled in. 3. Await the outcome. 4. Live long and prosper.  Let’s start by having a look at a sample [YAML] file for specifying the environment we want to boot and test:  --- title: Microservices sample test file # docker_compose_root: /Users/myuser/projects/microservices-workshop/solution-7 # docker_compose_file: docker-compose.yml # services: # - http://192.168.99.100:8761 - http://192.168.99.100:8761/eureka/apps/edgeserver - http://192.168.99.100:8761/eureka/apps/product - http://192.168.99.100:8761/eureka/apps/productapi - http://192.168.99.100:8761/eureka/apps/productcomposite - http://192.168.99.100:8761/eureka/apps/recommendation - http://192.168.99.100:8761/eureka/apps/review oauth: # url: https://192.168.99.100:9999/uaa/oauth/token client_id: acme client_password: acmesecret scope: webshop grant_type: password username: user password: password token_key: access_token endpoints: # - url: https://192.168.99.100/api/product/1046 auth_method: TOKEN - url: https://192.168.99.100/api/product/1337 auth_method: TOKEN - url: https://192.168.99.100/api/product/7331 auth_method: TOKEN  The YAML files defines some environment specifics such as title, docker project root directory and file, what microservices to query Eureka for _before_ fetching OAuth token and testing services.  1. Title. Used used in some console output. 2. Absolute path to where the docker-compose.yml files lives. 3. Name of the docker-compose yml file. Usually we stick with the default 'docker-compose.yml' 4. List of microservice status endpoints provided by our discovery server, in this case Eureka. All URLs must respond with a HTTP status code indicating success for a GET request before retrieving the OAuth token and commencing service testing. 5. OAuth settings. client_id and client_password are used as HTTP Basic auth for retrieving an OAuth token for username/password. scope, grant_type and token_key are OAuth parameters. 6. List of service endpoints to actually test. Currently only GET is supported, should add support for POST/PUT/REMOVE, request bodies, parsing etc. Note that auth_method TOKEN and NONE are the currently supported ones.   So, how does our little golang program tie this together? I would recommend starting by taking a look directly at the main.go source file main method:  func main { CallClear  // 1. Check required command-line tools are present // docker-compose etc. CheckDocker  // 2. Load service specification from yaml t := LoadSpecification  // 3. Start using docker-compose up -d, then use defer to make sure it's torn down afterwards. DockerComposeUp defer DockerComposeDown  // 4. Then wait for specified microservices awaitServicesHasStarted  // 5. When all are started, get and store OAuth token StoreOAuthToken  // 6. execute list of endpoint HTTP calls.  runEndpoints }   _CallClear, will return to that later.   _exec.LookPath_ is a convenience method for checking if a certain executable is present on the PATH, in this case 'docker-compose'. If not present, we use the log.Fatal to log and exit.  func CheckDocker { _, err := exec.LookPath if err != nil { log.Fatal } fmt.Printf }   func LoadSpecification { spec := parseSpecFile dir, _ := os.Getwd dat, _ := ioutil.ReadFile var t TestDef yaml.Unmarshal  fmt.Println  return t }  Not too verbose - get the .yaml file cmd-line arg, current directory, read the file and pass a reference to an uninitialized TestDef struct into the yaml.Unmarshal method. The yaml library does all the parsing for us, allowing us to return the populated struct to the main method.   func DockerComposeUp { cmd := exec.Command cmd.Dir = t.DockerComposeRoot env := os.Environ env = append cmd.Env = env cmd.Run fmt.Println }  The _exec.Command_ prepares a Cmd using varargs separated arguments to the docker-compose executable. '-f' specifies the file name and '-d' tells it to run in a daemon mode. Eventually, we'll call Run on the _cmd_ instance.  Note how we specify the docker-compose file using a value from our testDef and how we append a PROJECT_ROOT environment variable to the env of the Cmd. Each Cmd is only active for a given Cmd so it's _not_ possible in go to first do this:  // WON'T WORK cmd1 := exec.Command cmd1.Run cmd2 := exec.Command cmd2.Run // WON'T WORK!!  In the faulty snippet above, the 'export' won't carry over to cmd2, making use of such a environment variable in the docker-compose.yml yield nil, hence the use of the environment trick in the correct example further up.   How do we know when all infrastructure and services have started and is ready for testing? Our best bet is to utilize Eureka - remember these?  services: # - http://192.168.99.100:8761 - http://192.168.99.100:8761/eureka/apps/config-server ... - http://192.168.99.100:8761/eureka/apps/product  The first URL is simply the root of the Eureka service - if Eureka isn't up we can't check the state of services that has registered with the discover service. Under _http://192.168.99.100:8761/eureka/apps/[service name]_ Eureka publishes status pages  for each registered service. For our purpose, we're OK as long as a "OK" HTTP status code is returned.  func awaitServicesHasStarted { wg := sync.WaitGroup  fmt.Println consoleRow++ fmt.Println consoleRow++  for _, service := range t.Services { wg.Add consoleRow++ go PollService } wg.Wait has been called within PollService.  // Fix cursor position after all services have started consoleRow+=3 fmt.Printf // Move cursor to row }  Here we see some fun golang features in action, namely the [WaitGroup].  func PollService {  Cprint Cprint  req, _ := http.NewRequest var DefaultTransport http.RoundTripper = &http.Transport{ TLSClientConfig: &tls.Config, }  for { resp, err := DefaultTransport.RoundTrip if err != nil || resp.StatusCode > 299 { Cprint Cprint } else { Cprint Cprint wg.Done return } time.Sleep } }  Ignore all the Cprint calls for now, they're just for the VT100-style console output. The important part is the HTTP GET requests performed using net/http's DefaultTransport. DefaultTransport? Why not use the standard net/http client golang provides? Well - the easiest way of having golang's net/http client to accept servers with self-signed certificates is by setting the TLSClientConfig seen in the code.  Apart from the cert fix , the program will keep looping issuing one request per second to whatever Url that was specified in the _service_ parameter until a < 299 HTTP status is returned.  For example, _http://192.168.99.100:8761_ - which in our microservice landscape is the Eureka start page - won't return 200 OK until the Eureka service is up and running. Somewhat later, the "Product service" status can be queried by asking Eureka it's state at the _http://192.168.99.100:8761/eureka/apps/product_. So remember, it is not the actual service endpoint we are checking, we are just making sure it has booted and registered itself with its discovery service.  Actual endpoint testing is probably much more complex in a real-world scenario than here - you'd want full support for other HTTP methods, POST bodies, headers, response parsing etc. - which is out of scope for this little exercise. The service testing code  function yielded:  req := BuildHttpRequest var DefaultTransport http.RoundTripper = &http.Transport{ TLSClientConfig: &tls.Config, } if endpoint.Auth_method == "TOKEN" { req.Header.Add }  We then try each service up to 10 times before giving up:  retries := 10 for i := 0; i < retries; i++ {  resp, err := DefaultTransport.RoundTrip  if err != nil || resp.StatusCode > 299 { Cprint Cprint } else { Cprint Cprint wg2.Done break } time.Sleep  if  { wg2.Done Cprint Cprint } }  Again, we use a WaitGroup to make sure each goroutine can finish before the program exits.  Finally, the little CPrint method used to output program state to the console window:  var l sync.Mutex  func Cprint { l.Lock fmt.Printf fmt.Print l.Unlock }  We're simply using Printf with the VT100 escape sequence for positioning the cursor and row/col and then printing text. Since each goroutine may call this code to update console output at any time we need to use a lock mechanism or the output will become really garbled. Alternatively, we could have used a sync go channel instead, see bottom of this blog post for an example.  Below, we have a sample output from a run.  > go run src/github.com/eriklupander/mstest/*.go spec.yml  Starting up... docker-compose installed OK Loaded specification 'Microservices sample test file' Docker starting up using /Users/eriklupander/privat/blog-microservices/docker-compose.yml ... Waiting for all microservices to start...   http://192.168.99.100:8761 done http://192.168.99.100:8761/eureka/apps/edgeserver done http://192.168.99.100:8761/eureka/apps/product done http://192.168.99.100:8761/eureka/apps/productapi done http://192.168.99.100:8761/eureka/apps/productcomposite done http://192.168.99.100:8761/eureka/apps/recommendation done http://192.168.99.100:8761/eureka/apps/review done  Getting OAuth token ... OK  https://192.168.99.100/api/product/1046 ... OK https://192.168.99.100/api/product/1337 ... OK https://192.168.99.100/api/product/7331 ... OK  All done. Docker shutting down...      Bonus snippet of code using Go channels for handling  console printing without explicit use of mutexes and/or locks.  var consoleChannel chan CText // Declare privately scoped channel that we can pass console print // statements to func init { // Maybe some other stuff  go pollConsoleChannel // Start a goroutine that will handle receives on the channel }  func pollConsoleChannel { consoleChannel = make channel  // Will iterate until main program exits, blocks at <-. for { msg := <- consoleChannel // Blocks here until a message is received. fmt.Printf // Position cursor fmt.Print // Prints text at cursor position } }  /* Application invokes this func to print to console */ func ChPrint {  msg := CText{ // Build a simple instance of the CText struct row, col, text, } consoleChannel <- msg // Submit message on channel }  /* Simple struct for encapsulating row, col and text to log */ type CText struct { Row int Col int Text string }
 Last week I passed the [CKAD exam]. In this blog post, I will share some preparations I did to be able to solve the tasks in the exam, specifically on how I prepared for the time constraints in the exam.  -[readmore]-   I have been working with Kubernetes since its inception in 2014, and I decided recently to manifest my experience by taking the CKAD exam.  The CKAD exam in summary:  * The exam certifies that users can design, build, configure, and expose cloud native applications for Kubernetes. * The exam is performance-based, it consists of 19 tasks with problems to solve within two hours using the command line in a Linux terminal accessible using Chrome. Your score must exceed 66% to pass the exam. * The only allowed help is an extra tab in Chrome, where you are allowed to access information from: * <https://kubernetes.io/docs/>​ * ​<https://github.com/kubernetes/>​ * <https://kubernetes.io/blog/​> * You can take the exam remotely, e.g. from your home, but the exam is proctored using screen sharing, webcam, and the microphone on your computer.  From reading the exam curriculum, I realized that I was used to most of the concepts but lacked experience from using a few of them. So I browsed through an on-line course to catch up on the missing parts. Being rather self-confident, I started to go through the tests at the end of the course. I then realized that even if I had experiences from using most of the concepts in the exam, I was not fast enough to execute the exercises given the time available. It took me too long time to use  and , i.e. the main tools used during the exam.  > I guess you can use any text editor available in the Linux terminal, but  was the one I had some previous experience with.  So I had to practice a lot to be fast enough to pass the exam!   Many of the exam tasks can be solved by using the following steps:  1. **Creating a  file containing start material for the task** Use  imperative commands, e.g.  and  with the options `--dry-run -o yaml > n.yaml` to create the  file .  2. **Adding the final parts to the  file using an editor like ** The required syntax for the missing pieces can be found in either the links listed above or by using the `kubectl explain` command.  1. **Create the Kubernetes resources** Use the `kubectl apply -f n.yaml` command to create the resources in the Kubernetes cluster.  1. **Verify the expected result** Use  commands `get -o yaml`, , , and  to verify that you got the expected result.  Sometimes a HTTP endpoint inside the Kubernetes cluster needs to be verified. This can be done using either `kubectl port-forward` and a local  command in the Linux terminal or by launching a Kubernetes pod that can run the  command inside the cluster, e.g.:  kubectl run --image=curlimages/curl --restart=Never -i --rm curl-pod -- curl $service:$port -s -m1   To be able to solve the tasks in the exam fast enough, I recommend the following preparation steps:  1. Take a course to ensure that you learn all parts of Kubernetes required for the exam. I suggest the course [Kubernetes Certified Application Developer  provided by KodeKloud at Udemy.  2. Read through all material available at [CKAD Certification Exam Candidate Resources], i.e. the Candidate Handbook, Exam Tips, and FAQ  3. Bookmark favorite links to the Kubernetes documentation. You are allowed to use bookmarks in Chrome to your favorite pages in the links listed above. Get used to use these bookmarks when looking up information.  4. Setup alias and autocomplete according to [kubectl-autocomplete]. Use it both when you practice and during the exam!  Using  as shorthand alias for  and enabling autocomplete for  commands will save you a lot of time!  > Set up a bookmark in Chrome to this link!  5. Practice on all options available for the imperative  commands for creating start material for the  files. Specifically the `kubectl run` command contains a lot of useful options for creating a , e.g. , , , , , and `-- [COMMAND] [args...]`.  The CKAD environment has recently been upgraded to Kubernetes v1.18, where deprecated variants of the `kubectl run` command have been removed. This means that the  command only can be used to create  and that the  command has to be used to create , , and .  If you still want to use all the useful options in the  command for a  when creating a , you have to execute both the  command and the `create deployment` command. Next, you need to paste the result from the  command into the result of the `create deployment` command. The content under the  section shall be replace with the  and  sections from the  command. Don't forget to indent the output from the  command correctly . Practice, practice, practice...  6. Learn other useful  commands: 1. The commands , , , and  can be useful for some tasks 2. Practice rolling out and rolling back upgrades of a deployment using the `kubectl rollout` command. 3. To get a good overview of available labels on a set of Kubernetes objects, use the  option of the `kubectl get ` command. 4. If you are requested to update or fix a problem of an existing resource, use the `kubectl get ... -o yaml > n.yaml` command to get started.  7. Learn how to configure  to edit  files efficiently. My recommendation is to create a  file in the home folder with the command `vi ~/.vimrc` and enter:  set number set tabstop=2 shiftwidth=2 expandtab  Explanations to the configuration:  * `set number` makes  show line numbers, very handy if  complains about an error on a specific line in a  file. * `set tabstop=2 shiftwidth=2 expandtab` makes  expand TAB characters to two spaces and sets indentation to two characters, perfect when editing  files.  8. Learn critical  keystrokes to manipulate  files, specifically: 1. [Goto a specific line in the text file] 2. [Copy and paste lines of code] 3. [Indent lines of code]  9. Once you feel confident with the tests provided by the course you took, I recommend that you sign up for the CKAD Simulator provided by [killer.sh].  It provides you with an environment similar to the exam environment. When you sign up, you are allowed to run the tests in two separates sessions. If you don't feel confident after the first test session, go back and practice on the tasks in the course or look for additional examples on the Internet. Hopefully, you feel confident after taking the second test in the CKAD simulator and are ready for the real exam!   During the exam, keep calm and work focused on one task at the time. Always ensure you are on the right cluster and in the right namespace before you start to work on a new task. If you get stuck on a task, you can flag it as unfinished and move on to the next task. If you have time left after going through all tasks, you can go back to unfinished tasks and work on them.  Using these preparations, I was able to be fast enough with  and  to work through all 19 exercises and got a score of 97%, see [my certification badge] and:  ![CKAD_Certificate_ML]  **Good luck with your preparations and the CKAD exam!**
 In part 3, we'll make our Accountservice do something useful.  - Declare an 'Account' struct - Embed a simple key-value store that we can store Account structs in. - Serialize a struct to JSON and serve over our /accounts/ HTTP service.  As in all upcoming parts of this blog series, you can get the complete source code of this part by cloning the source  and switching to the P3 branch, i.e:  git checkout P3  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  For a more elaborate introduction to Go structs, please check [this guide].  In our project, create a folder named _model_ under the /accountservice folder.  mkdir model  Now, create a file named _account.go_ in the _model_ folder with the following content:  package model  type Account struct { Id string  Name string  }  This declares our _Account_ abstraction that basically is an id and a name. The case of the first letter denotes scoping . We also use the built-in support for declaring how each field should be serialized by the json.Marshal function in Go.  For this, we'll use the [BoltDB] key-value store. It's simple, fast and easy to work with. We can actually preempt _go get_ to retrieve the dependency before we've declared use of it:  go get github.com/boltdb/bolt  Next, in the _/goblog/accountservice_ folder, create a new folder named "dbclient" and a file named _boltclient.go_. To make mocking easier later on, we'll start by declaring an interface that defines the contract we need implementors to fulfill:  package dbclient  import ( "github.com/callistaenterprise/goblog/accountservice/model" )  type IBoltClient interface { OpenBoltDb QueryAccount Seed }  In the same file, we'll provide an implementation of this interface. Start by declaring a struct that encapsulates a pointer to a bolt.DB instance.  // Real implementation type BoltClient struct { boltDB *bolt.DB }  Here is the implementation of _OpenBoltDb_. We'll add the two remaining functions a bit further down.  func  { var err error bc.boltDB, err = bolt.Open if err != nil { log.Fatal } }  This part of Go syntax can feel a bit weird at first, where we bind a function to a struct. Our struct now implicitly implements one of the three methods.  We'll need an instance of this "bolt client" somewhere. Let's put it where it's going to be used, in _/goblog/accountservice/service/handlers.go_. Create that file and add the instance of our struct:  package service  import ( "github.com/callistaenterprise/goblog/accountservice/dbclient" )  var DBClient dbclient.IBoltClient  Update _main.go_ so it'll open the DB on start:  func main { fmt.Printf initializeBoltClient // NEW service.StartWebServer }  // Creates instance and calls the OpenBoltDb and Seed funcs func initializeBoltClient { service.DBClient = &dbclient.BoltClient service.DBClient.OpenBoltDb service.DBClient.Seed }  Our microservice should now create a database on start. However, before running we'll add a piece of code that'll bootstrap some accounts for us on startup.  Open _boltclient.go_ again and add the following functions:  // Start seeding accounts func  { initializeBucket seedAccounts }  // Creates an "AccountBucket" in our BoltDB. It will overwrite any existing bucket of the same name. func  { bc.boltDB.Update error { _, err := tx.CreateBucket if err != nil { return fmt.Errorf } return nil }) }   // Seed  make-believe account objects into the AcountBucket bucket. func  {  total := 100 for i := 0; i < total; i++ {  // Generate a key 10000 or larger key := strconv.Itoa  // Create an instance of our Account struct acc := model.Account{ Id: key, Name: "Person_" + strconv.Itoa, }  // Serialize the struct to JSON jsonBytes, _ := json.Marshal  // Write the data to the AccountBucket bc.boltDB.Update error { b := tx.Bucket err := b.Put return err }) } fmt.Printf }  For more details on the Bolt API and how the Update method accepts a func that does the work for us, see the [BoltDB documentation].  We're done with the BoltDB part for now. Let's build and run again:  > go run *.go Starting accountservice Seeded 100 fake accounts... 2017/01/31 16:30:59 Starting HTTP service at 6767  Lovely! Stop it using Ctrl+C.  Now we finish our little DB API by adding a Query method to the _boltclient.go_:  func  { // Allocate an empty Account instance we'll let json.Unmarhal populate for us in a bit. account := model.Account  // Read an object from the bucket using boltDB.View err := bc.boltDB.View error { // Read the bucket from the DB b := tx.Bucket  // Read the value identified by our accountId supplied as []byte accountBytes := b.Get if accountBytes == nil { return fmt.Errorf } // Unmarshal the returned bytes into the account struct we created at // the top of the function json.Unmarshal  // Return nil to indicate nothing went wrong, e.g no error return nil }) // If there were an error, return the error if err != nil { return model.Account, err } // Return the Account struct and nil as error. return account, nil }  Follow the comments if the code doesn't make sense. The function will query the BoltDB using a supplied _accountId_ parameter and will return an Account struct _or_ an error.  Let's fix the _/accounts/_ route we declared in _/service/routes.go_ so it actually returns one of the seeded Account structs. Open routes.go and replace the inlined _func {_ with a reference to a function _GetAccount_ we'll create in a moment:  Route{ "GetAccount", // Name "GET", // HTTP method "/accounts/", // Route pattern GetAccount, },  Next, update _/service/handlers.go_ with a _GetAccount_ func that fulfills the HTTP handler func signature:  var DBClient dbclient.IBoltClient  func GetAccount {  // Read the 'accountId' path parameter from the mux map var accountId = mux.Vars["accountId"]  // Read the account struct BoltDB account, err := DBClient.QueryAccount  // If err, return a 404 if err != nil { w.WriteHeader return }  // If found, marshal into JSON, write headers and content data, _ := json.Marshal w.Header w.Header w.WriteHeader w.Write }  The GetAccount func fulfills the handler func signature so when Gorilla detects a call to /accounts/ it will route the request into the GetAccount function. Let's run it!  > go run *.go Starting accountservice Seeded 100 fake accounts... 2017/01/31 16:30:59 Starting HTTP service at 6767  Call the API using curl. Remember, we seeded 100 accounts starting with an Id of 10000.  > curl http://localhost:6767/accounts/10000   Nice! Our microservice is now actually serving JSON data from an underlying store over HTTP.  Let's check the same memory and CPU usage metrics as in [part 2]: Before, during and after our simple Gatling-based load test.  ![mem use]  2.1 mb, still not bad! Adding the embedded BoltDB and some more code to handle routing etc. added 300kb to our initial footprint. Let's start the Gatling test running 1K req/s. Now we're actually returning a real Account object fetched from the BoltDB which also is serialized to JSON:  ![mem use2] 31.2 mb of RAM. The extra overhead of serving 1K req/s using an embedded DB was really small compared to the naive service from Part 2.  ![cpu use] Serving 1K req/s uses about 10% of a single Core. The overhead of the BoltDB and JSON serialization is not very significant, good! By the way - the _java_ process at the top is our Gatling test which actually uses ~3x the CPU resources as the software it is testing.  ![performance] Mean response time is still less than one millisecond.  Perhaps we should test with a heavier load, shall we say 4K req/s? :  ![mem use] Approx 120 mb. Almost exactly an increase by 4x. This memory scaling with n/o concurrent requests is almost certainly due to the Golang runtime or possibly Gorilla increasing the number of internal goroutines used to serve requests concurrently as load goes up.  ![cpu use] CPU use stays just below 30% at 4K req/s. At this point, i.e. running on a 16 GB RAM / Core i7 equipped laptop, I'd say that IO or file handles would bottleneck sooner than available CPU cycles.  ![performance] Mean latency now finally rises above 1 ms with 95% of requests staying below 3ms. We do see latency starting to take a hit at 4K req/s, though I'd personally say that the little Accountservice with its embedded BoltDB performs really well.  One could probably write an interesting blog post about benchmarking this "accountservice" against an functionally equivalent microservice implemented on the JVM, NodeJS, CLR and others.  I did some _naive_ inconclusive benchmarking  on this myself late 2015 comparing a HTTP/JSON service + MongoDB access implemented in Go 1.5 vs Spring Boot@Java 8 and NodeJS. In that particular case the JVM and Go-based solutions scaled equally well with a slight edge to the JVM-based solution regarding latencies. The NodeJS server performed quite similarly to the others up to the point where the CPU utilization reached 100% on a single core and things started going south regarding latencies.  _Please don't take the benchmarking mentioned above as some kind of fact as it was just a quick and dirty thing I did for my own pleasure._  So while the numbers I've shown regarding performance at 4K req/s using Go 1.7 for the "accountservice" may seem very impressive, they can probably be matched by other platforms as well, though I doubt their memory use will be as pleasant. I guess your milage may vary.   In the [next part] of this blog series we'll take a look at unit testing our service using GoConvey and mocking the BoltDB client.
 How many databases/schemas does your project use ? One common setup is to have one for each staging environment  and one for each developer and tester in the project. Maybe you even need a set of schemas free to use for anyone at special occasions. If the application is deployed in multipel versions for different customers that will multiply the number as well. I have been in projects where 30 different database schemas have been used.  Obviously the schema definition and content has to change when the code changes. So the challenge is to keep each schema consistent with the version of the code that is run against that schema. At any time there may be any number of deployed version of the code at the different environments, and many developers may be busy working on different features requiring database changes.  So how do you manage this in an efficient way ?  One recent project used a database script containing "CREATE TABLE" statements. Over time the script evolved to contain all sorts of statements expressing both new tables and upgrades between later versions of the schema. Every time you ran the script all sorts of errors occurred but that was considered "normal" and always ignored. The result of this approach was very costly for the project.  Sometimes panic mail urging everyone to run a supplied upgrade in there own schema were sent out. No-one could tell with certainty which version of a schema that was used in different environments, we had several bugs due to the fact that inconsistent versions of databas and code was used. Maintaining the script was almost impossible, no-one understood how to do it and since the script always produced a lot of errors it was hard to tell if your change was correct or not. Reverting to a previos version meant you had to guess which lines in the script that was added after that version created, in practice an impossible task.  The project was in a maintenance phase meaning the database change rate was very low, I can only imagine the total chaos this approach would lead to in a busy development phase.  My current project uses the [liquibase], database-independent library for tracking, managing and applying database changes. We have found that it gives the project  - reproducible results - clarity - control - an easy way for all project members to understand and create changes  Most common use case for us is to  - create a schema from scratch in a specific version - upgrade a specific schema from one version to another keeping existing data  Not only do we do the normal stuff in database changes like adding tables, columns, renaming stuff, creating constraints. indexes etc we also maintain data in some tables that are considered to be part of the installation.  All changes related to upgrade to one version from the previous are kept in a separate file with the name of the version of the software making it easy to find. This is called a change set in liquibase lingo. All change sets are listed in a changelog file.  When you want to upgrade a schema you run a small scripts that promts for schema name and password and the applies the appropriate changes. It is very simple to use. After this is done 2 new tables are created in the schema by liquibase recording the changes that has been made.  A sample change set file containing only on change is found at the liquibase site:   This show the xml-intense way of defining changes.  The other way is replace everything within the <changesSet> tag by simple sql. If you do that you loose the possibility to apply the change to different databases with minor differences in sql syntax. Liquibase currently supports 14v different databases so if you use a mix of any of these or plan to change database within the system lifetime it might be a good idea to use the xml version.  There are a lot of features in liquibase that we have not tried out yet. You can produce a javadoc style report on what changes that has been made in a database [see sample here].  After we started to use liquibase the time wasted on chasing errors due to inconsistent database state has been reduced to a mimimum. We feel we have a reliable mechanism for upgrading our different database schemas.
In this part of the Go microservices [blog series].  Logs. You never know how much you miss them until you do. Having guidelines for your team about what to log, when to log and how to log, may be one of the key factors for producing a maintainable application. Then, microservices happen.  While dealing with one or a few different log files for a monolithic application is usually manageable , consider doing the same for a microservice-based application with potentially hundreds or even thousands of service containers each producing logs. Don't even consider going big if you don't have a solution for collecting and aggregating your logs in a well-structured manner.  Thankfully, a lot of smart people have already thought about this - the stack formerly know as ELK is perhaps one of the most well-known within the open source community. ElasticSearch, LogStash and Kibana forms the [Elastic Stack] solution for our centralized logging needs based on four parts:  1. Logrus - a logging framework for Go 2. Docker GELF driver - logging driver for the Greylog Extended Log Format 3. "Gelftail" - a lightweight log aggregator we're going to build in this blog post. Of course, we'll write it in Go. 4. Loggly - a [LaaS] provider. Provides similar capabilities for managing and acting on log data as similar services.  ![overview]   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P10  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._   Typically, our Go microservices has up until now logged using either the "fmt" or the "log" packages, either to stdout or stderr. We want something giving us more fine-granular control of log levels and formatting. In the Java world, many  is our logging API of choice for this blog series, it roughly provides the same type of functionality as the APIs I just mentioned regarding levels, formatting, hooks etc.  One of the neat things with logrus is that it implements the same interface we've used for logging up until now - _fmt_ and _log_. This means we can more or less use logrus as a drop-in replacement. Start by making sure your GOPATH is correct before fetching logrus source so it's installed into your GOPATH:  > go get github.com/sirupsen/logrus  We'll do this the old-school way. For _/common_, _/accountservice_ and _/vipservice_ respectively - use your IDE or text editor to do a global search&replace where _fmt.*_ and _log.*_ are replaced by _logrus.*_. Now you should have a lot of _logrus.Println_ and _logrus.Printf_ calls. Even though this works just fine, I suggest using logrus more fine-granular support for severities such as INFO, WARN, DEBUG etc. For example:  | fmt | log | logrus | |Println|Println|Infoln| |Printf|Printf|Infof| |Error||Errorln|  There is one exception which is _fmt.Error_ which is used to produce _error_ instances. Do not replace _fmt.Error_.  Given that we've replaced a _lot_ of _log.Println_ and _fmt.Println_ with _logrus.Println_ .  Again, make sure your GOPATH is correct. Then use _go get_ to download goimports:  go get golang.org/x/tools/cmd/goimports  This will install goimports into your $GOPATH/bin folder. Next, you can go to the root of the _accountservice_ or _vipservice_ service, e.g:  cd $GOPATH/src/github.com/callistaenterprise/goblog/accountservice  Then, run goimports, telling it to fix imports recursively with the "-w" flag which applies the changes directly to the source files.  $GOPATH/bin/goimports -w **/*.go  Repeat for all our microservice code, including the _/common_ folder.  Run _go build_ to make sure the service compiles.  go build   If we don't configure Logrus at all, it's going to output log statements in plain text. Given:  logrus.Infof  It will output:  INFO[0000] Starting our service...  Where _0000_ is the number of seconds since service startup. Not what I want, I want a datetime there. So we'll have to supply a formatter.  The _init_ function is a good place for that kind of setup:  func init { logrus.SetFormatter(&logrus.TextFormatter{ TimestampFormat: "2006-01-02T15:04:05.000", FullTimestamp: true, }) }  New output:  INFO[2017-07-17T13:22:49.164] Starting our service...  Much better. However, in our microservice use-case, we want the log statements to be easily parsable so we eventually can send them to our [LaaS] mode.  Let's change that _init_ code somewhat so it'll use a JSON formatter instead unless the "-profile=dev" flag is passed.  func init { profile := flag.String if *profile == "dev" { logrus.SetFormatter(&logrus.TextFormatter{ TimestampFormat: "2006-01-02T15:04:05.000", FullTimestamp: true, }) } else { logrus.SetFormatter } }  Output:     That's about it. Feel free to read the Logrus [docs] for more comprehensive examples.  It should be made clear that the standard logrus logger doesn't provide the kind of fine-granular control you're perhaps used to from other platforms - for example changing the output from a given _package_ to DEBUG through configuration. It is however possible to create scoped logger instances which makes more fine-grained configuration possible, e.g:   var LOGGER = logrus.Logger // <-- Create logger instance  func init { // Some other init code...  // Example 1 - using global logrus API logrus.Infof  // Example 2 - using logger instance LOGGER.Infof }  __  By using a _LOGGER_ instance it's possible to configure the application-level logging in a more fine-granular way. However, I've chosen to do "global" logging for now using _logrus.*_ for this part of the blog series.   What's GELF? It's an acronym for [Greylog Extended Log Format] which actually means that everything written within a container to stdout or stderr is "picked up" by Docker Engine and is processed by the configured logging driver. This processing includes adding a lot of metadata about the container, swarm node, service etc. that's specific to Docker. A sample message may look like this:  { "version":"1.1", "host":"swarm-manager-0", "short_message":"Starting HTTP service at 6868", "timestamp":1.487625824614e+09, "level":6, "_command":"./vipservice-linux-amd64 -profile=test", "_container_id":"894edfe2faed131d417eebf77306a0386b43027e0bdf75269e7f9dcca0ac5608", "_container_name":"vipservice.1.jgaludcy21iriskcu1fx9nx2p", "_created":"2017-02-20T21:23:38.877748337Z", "_image_id":"sha256:1df84e91e0931ec14c6fb4e559b5aca5afff7abd63f0dc8445a4e1dc9e31cfe1", "_image_name":"someprefix/vipservice:latest", "_tag":"894edfe2faed" }  Let's take a look at how to change our "docker service create" command in _copyall.sh_ to use the GELF driver:  docker service create \ --log-driver=gelf \ --log-opt gelf-address=udp://192.168.99.100:12202 \ --log-opt gelf-compression-type=none \ --name=accountservice --replicas=1 --network=my_network -p=6767:6767 someprefix/accountservice  - --log-driver=gelf tells Docker to use the [gelf driver] - --log-opt gelf-address tells Docker where to send all log statements. In the case of gelf, we'll use the UDP protocol and tell Docker to send log statements to a service on the defined IP:port. This service is typically something such as [logstash] but in our case, we'll build our own little log aggregation service in the next section. - --log-opt gelf-compression-type tells Docker whether to use compression before sending the log statements. To keep things simple, no compression in this blog part.  That's more or less it! Any microservice instance created of the _accountservice_ type will now send everything written to stdout/stderr to the configured endpoint. Do note that this means that we can't use _docker logs [containerid]_ command anymore to check the log of a given service since the  logging driver isn't being used anymore.  We should add these gelf log driver configuration statements to all _docker service create_ commands in our shell scripts, e.g. [copyall.sh].  There's one kludgy issue with this setup though - the use of a hard-coded IP-address to the Swarm Manager. Regrettably, even if we deploy our "gelftail" service as a Docker Swarm mode service, we can't address it using its logical name when declaring a service. We can probably work around this drawback somehow using DNS or similar, feel free to enlighten us in the comments if you know how ;)   If you really need to make your logging more container-orchestrator agnostic, an option is to use the [gelf plugin] for Logrus to do GELF logging using hooks. In that setup, Logrus will format log statements to the GELF format by itself and can also be configured to transmit them to a UDP address just like when using the Docker GELF driver. However - by default Logrus has no notion about running in a containerized context so we'd basically have to figure out how to populate all that juicy metadata ourselves - perhaps using calls to the Docker Remote API or operating system functions.  I _strongly_ recommend using the Docker GELF driver. Even though it ties your logging to Docker Swarm mode, other container orchestrators probably have similar support for collecting stdout/stderr logs from containers with forwarding to a central logging service.   That UDP server where all log statement are sent is often _Logstash_ or similar, that provides powerful control over transformation, aggregation, filtering etc. of log statements before storing them in a backend such as Elasticsearch or pushing them to a LaaS.  However, Logstash isn't exactly lightweight and in order to keep things simple  we're going to code our very own little "log aggregator". I'm calling it "gelftail". The name comes from the fact that once I had configured the Docker GELF driver for all my services, I had no way of seeing what was being logged anymore! I decided to write a simple UDP server that would pick up all data sent to it and dump to stdout, which then I could look at using _docker logs_. E.g. a stream of all log statements from all services. Not very practical but at least better than not seeing any logs at all.  The natural next step were then to attach this "gelftail" program to a LaaS backend, apply a bit of transformation, statement batching etc. which is exactly what we're going to develop right away!   In the root _/goblog_ folder, create a new directory called _gelftail_. Follow the instructions below to create the requisite files and folders.  mdkir $GOPATH/src/github.com/callistaenterprise/goblog/gelftail mdkir $GOPATH/src/github.com/callistaenterprise/goblog/gelftail/transformer mdkir $GOPATH/src/github.com/callistaenterprise/goblog/gelftail/aggregator cd $GOPATH/src/github.com/callistaenterprise/goblog/gelftail touch gelftail.go touch transformer/transform.go touch aggregator/aggregator.go  Gelftail works along these lines:  1. Starting an UDP server . 2. For each UDP packet, we'll assume it's JSON-formatted output from logrus. We'll do a bit of parsing to extract the actual _level_ and _short_message_ properties and _transform_ the original log message slightly so it contains those properties as root-level elements. 3. Next, we'll use a buffered go channel as a logical "send queue" that our _aggregator_ goroutine is reading from. For each received log message, it'll check if it's current _buffer_ is > 1 kb. 4. If the buffer is large enough, it will do an HTTP POST to the [Loggly] http upload endpoint with the aggregated statements, clear the buffer and start building a new batch.  Expressed using classic Enterprise Integration patterns  it looks like this:  ![gelftail overview]  The program will be split into three files. Start with [gelftail.go] with a _main_ package and some imports:  package main  import ( "bytes" "encoding/json" "flag" "fmt" "net" "net/http" "os" "io/ioutil" "github.com/Sirupsen/logrus" )  When registering with Loggly , we get an Authentication token that you must treat as a secret. Anyone having access to your token can at least send log statements into your account. So make sure you .gitignore _token.txt_ or whatever name you pick for the file. Of course, one could use the configuration server from Part 7 and store the auth token as an encrypted property. For now, I'm keeping this as simple as possible so text file it is.  So let's add a placeholder for our LaaS token and an _init_ function that tries to load this token from disk. If unsuccessful, we might as well log & panic.  var authToken = "" var port *string  func init { data, err := ioutil.ReadFile if err != nil { msg := "Cannot find token.txt that should contain our Loggly token" logrus.Errorln panic } authToken = string  port = flag.String flag.Parse }  We also use a flag to take an optional port number for the UDP server. Next, time to declare our main function to get things started.  func main { logrus.Println  ServerConn := startUDPServer // Remember to dereference the pointer for our "port" flag defer ServerConn.Close  var bulkQueue = make // Buffered channel to put log statements ready for LaaS upload into  go aggregator.Start // Start goroutine that'll collect and then upload batches of log statements go listenForLogStatements // Start listening for UDP traffic  logrus.Infoln  wg := sync.WaitGroup wg.Add wg.Wait // Block indefinitely }  Quite straightforward - start the UDP server, declare the channel we're using to pass processed messages and start the "aggregator". The _startUDPServer_:  func listenForLogStatements { buf := make // Buffer to store UDP payload into. 8kb should be enough for everyone, right Bill? :D var item map[string]interface // Map to put unmarshalled GELF json log message into for { n, _, err := ServerConn.ReadFromUDP // Blocks until data becomes available, which is put into the buffer. if err != nil { logrus.Errorf continue // Log and continue if there are problms }  err = json.Unmarshal // Try to unmarshal the GELF JSON log statement into the map if err != nil { // If unmarshalling fails, log and continue.  logrus.Errorln item = nil continue }  // Send the map into the transform function processedLogMessage, err := transformer.ProcessLogStatement if err != nil { logrus.Printf } else { bulkQueue <- processedLogMessage // If processing went well, send on channel to aggregator } item = nil } }  Follow the comments in the code. The [transformer.go] file isn't that exciting either, it just reads some stuff from one json property and transfers that onto the "root" GELF message. So let's skip that.  Finally, a quite peek at the "aggregator" code in _/goblog/gelftail/aggregator/[aggregator.go]_ that processes the final log messages from the _bulkQueue_ channel, aggregates and uploads to Loggly:  var client = &http.Client var logglyBaseUrl = "https://logs-01.loggly.com/inputs/%s/tag/http/" var url string  func Start { url = fmt.Sprintf // Assemble the final loggly bulk upload URL using the authToken buf := new for { msg := <-bulkQueue // Blocks here until a message arrives on the channel. buf.Write buf.WriteString // Loggly needs newline to separate log statements properly.  size := buf.Len if size > 1024 { // If buffer has more than 1024 bytes of data... sendBulk // Upload! buf.Reset } } }  I just love the simplicity of Go code! Using a _bytes.Buffer_, we just enter an eternal loop where we block at _msg := <-bulkQueue_ until a message is received over the  channel. We write the content + a newline to the buffer and then check whether the buffer is larger than our pre-determined 1kb threshold. If so, we invoke the _sendBulk_ func and clear the buffer. _sendBulk_ just does a standard HTTP POST to loggly.   Of course, we'll deploy "gelftail" as a Docker Swarm mode _service_ just as everything else. For that, we need a Dockerfile:  FROM iron/base  EXPOSE 12202/udp ADD gelftail-linux-amd64 / ADD token.txt /  ENTRYPOINT ["./gelftail-linux-amd64", "-port=12202"]  _token.txt_ is a simple text file with the Loggly authorization token, more on that in section 4. of this blog post.  Building and deploying should be straightforward. We'll add a new .sh script to the root _/goblog_ directory:  #!/bin/bash  export GOOS=linux export CGO_ENABLED=0  cd gelftail;go get;go build -o gelftail-linux-amd64;echo built ;cd ..  export GOOS=darwin  docker build -t someprefix/gelftail gelftail/ docker service rm gelftail docker service create --name=gelftail -p=12202:12202/udp --replicas=1 --network=my_network someprefix/gelftail  This should run in a few seconds. Verify that gelftail was successfully started by tailing its very own stdout log. Find it's container id using _docker ps_ and then check the log using _docker logs_:  > docker logs -f e69dff960cec time="2017-08-01T20:33:00Z" level=info msg="Starting Gelf-tail server..." time="2017-08-01T20:33:00Z" level=info msg="Started Gelf-tail server"  If you do something with another service that logs stuff, the log output from that service should now appear in the tail above. Let's scale the _accountservice_ to two instances:  > docker service scale accountservice=2  The tailed _docker logs_ above should now output some stuff such as:  time="2017-08-01T20:36:08Z" level=info msg="Starting accountservice" time="2017-08-01T20:36:08Z" level=info msg="Loading config from http://configserver:8888/accountservice/test/P10\n" time="2017-08-01T20:36:08Z" level=info msg="Getting config from http://configserver:8888/accountservice/test/P10\n"  That's all for "gelftail". Let's finish this blog post by taking a quick peek at "Loggly".    There are numerous "Logging as a Service" providers out there and I basically picked one  that seemed to have a free tier suitable for demoing purposes, a nice GUI and a rich set of options for getting your log statements uploaded.  ![loggly UI]  There is a [plethora] that allows us to send multiple log statements in small batches by newline separation.  I suggest following their [getting started] guide, which can be boiled down to:  1. Create an account.  2. Obtain an authorization token. Save this somewhere safe and copy-paste it into _/goblog/gelftail/token.txt_. 3. Decide how to "upload" your logs. As stated above, I chose to go with the HTTP/S POST API. 4. Configure your services/logging driver/logstash/gelftail etc. to use your upload mode of choice.  Leveraging all the bells & whistles of Loggly is out of scope for this blog post. I've only tinkered around with their dashboard and filtering functions which I guess is pretty standard as LaaS providers go.   In the first screenshot, I've zoomed in on a 35-minute time duration where I'm explicitly filtering on the "accountservice" and "info" messages:  ![loggly 2]  As seen, one can customize columns, filter values, time periods etc. very easily.  In the next sample, I'm looking at the same time period, but only at "error" log statements:  ![loggly 3]  While these sample use cases are very simple, the real usefulness materializes when you've got dozens of microservices each running 1-n number of instances. That's when the powerful indexing, filtering and other functions of your LaaS really becomes a fundamental part of your microservices operations model.  In part 10 of the [blog series] we've looked at centralized logging - why it's important, how to do structured logging in your Go services, how to use a logging driver from your container orchestrator and finally pre-processing log statments before uploading them to a Logging as a Service provider.  In the [next part], it's time to add circuit breakers and resilience to our microservices using Netflix Hystrix.
 This is somewhat old news by now but during this summer I ran into some problems while trying to upgrade a big Javascript code base from JQuery 1.4 to 1.6. It seems that the step to JQuery 1.6 has been cumbersome not only for me but for a lot of JQuery users out there so I would try to explain the  biggest pitfall with this migration in my way as well.  -[readmore]-  In JQuery 1.6 there is a new function named  function was changed to handle only attributes.  Previously you have been able to use the  to change both attributes and properties which the JQuery team intended to change in 1.6.  To simplify you could say that the difference between an attribute and a DOM property is that attributes are more or less the tag's/element's attributes that are stated in the HTML document while properties are the same values/properties but on the DOM objects, that are the browsers internal representation of the page elements, produced by parsing the HTML. This, more or less, means that attributes are not changed after the page is loaded but properties are updated as the user interacts with the page and for example checks a checkbox but there are also other small differences.  The JQuery team describes it in this way on their blog:  > Generally, DOM attributes represent the state of DOM information as retrieved from the document, such as the  attribute in the markup `<input type="text" value="abc">`. DOM properties represent the dynamic state of the document; for example if the user clicks in the input element above and types  the  remains .  The JQuery team went ahead and implemented the change in 1.6, which made it necessary for JQuery developers to learn about the difference of attributes and properties, but they received a lot of complaints from the users for this. The critique forced the team to rewrite  once again to be more backwards compatible in 1.6.1 which was released rather quickly after 1.6.0.  Unfortunately it is still not a hundred percent backwards compatible when it comes to boolean values. There is for example a difference in the value that makes a checkbox checked by default. The default state of an  tag with  is steered only by the existence of the  attribute and not by the value of the  attribute. The value of the DOM property checked, on the other hand, is denoting the curent state.  That is, the following tag ...   ... will make the checkbox checked by default and the attribute value is  but the DOM property  will get value  and not .  The other case is where we have the checkbox unchecked by default ...   ... then the  attribute wouldn't be set at all while the  property now is false.  The changes in managing boolean values are excellently described by the creator of JQuery, John Resig, here: [http://news.ycombinator.com/item?id=2513702]  All of this and the reasoning behind the changes is described in detail at John Resig's own blog [http://ejohn.org/blog/jquery-16-and-attr]
 I decided to see what it would take to deploy the weather feed of my previous post to Google App Engine - a cloud platform for Java servlets. I went the maven path, so that I could simply deploy to GAE via a maven build command. In order to keep the original project independent of GAE, I set up a second web-app project as a war overlay. A war overlay project is a maven war project that that declares a dependency to another war project. Maven then merges the web artifacts of both projects into the web war produced by the depending project. An additional advantage is that I could keep the maven default layout for the original web project. GAE needs a slightly different structure, which is then used in the overlay project only.  This was useful, since GAE-specific stuff goes into a GAE-specific deployment descriptor .   Of cause I went into a number of traps, before it all worked.  My app uses Groovy Templates for dynamic html output. Although web.xml had a servlet mapping that matched the extension used for the templates, they were served unprocessed by GAE servlet engine. The solution was to specifically exclude them from static resources in :     Although I found JAXWS on the list of Java EE specs not supported by GAE, I still had a hope that plain JAXB without JAXWS network access would work. It didn't. So I had to skip JAXB and use Groovy:s Markup builder. As a result, the DDD achieved by Groovy Categories is gone... ****  Point an RSS-reader to [http://rss2weather.appspot.com].
 Now that dynamic languages such as Python, Groovy and Ruby have started to become mainstream, it is time to glance at a different branch of languages for influences. I've just come home from the [QCon conference] functional.   [Simon Peyton-Jones], focusing mainly on the concept of purity. A pure function is one that has no side effects, meaning that each call to the function always has the same result. According to Simon, purity has a lot of advantages such as:  - Its easy to understand what each function does. - Improved testability. - Easier maintenance. - Easier to optimize the algorithms. - Parallelism - pure programs are "naturally parallel".  Unfortunately, a program which has no side effects at all does nothing useful, for example it is impossible to do I/O without side effects. Haskell, being a pure functional language, has solved the dilemma with something called [monads ]. I'm not even going to try to explain what it is other than to say that it's a very controlled way to do side effects. According to Simon one of the most challenging tasks for all programming languages over the coming years will be to combine the advantages of purity with doing something useful. In this area imperative languages such as Java can benefit from glancing at functional languages such as Haskell and vice versa.   Simon was followed by Joe Armstrong, who gave an equally inspiring talk about [Erlang]. He described Erlang as a "concurrent language which happens to be functional", i.e. its main focus is on concurrency.  Due to a paradigm shift in how hardware and CPUs are designed , a sequential program written today will actually run slower in a couple of years time. A concurrent program on the other hand will run faster as the number of cores increases. This means that languages like Java will need to improve its concurrency features, allowing for us developers to take advantage of the multiple cores. The concurrency implementation available in Java today is not at all satisfactory.  Even though both Erlang and Haskell have been around for some twenty years now, interest in them has grown a lot recently and they have also given inspiration to new functional languages.   The other two languages presented within the track was [F#], integrates both object-oriented and functional features and focuses on concurrency in very much the same way as Erlang.
 In yesterday’s blog post, I summarized my experience from the first day of the Spring XD tutorial. In this blog post I’ll try to wrap things up regarding the tutorial and the subject at hand.  -[readmore]- Spring XD jobs, [Hadoop] functionality.  First off, the [batch jobs] under the hood and one lab exercise was to develop a custom Spring Batch job and deploy it as a module into Spring XD. Regrettably, it wasn’t really development - it was plain copy/paste from examples and I guess most students finished within 5-10 of the allotted 30 minutes. An all-too common case during these two days.  The most time-consuming action of the day was definitely setting up Hadoop on our local machines. Suddenly we were installing ssh daemons, fiddling with keys and modifying configuration files in a number of places. However, the actual lab was just as trivial as before - a 4-line .csv file was read by a [filepollhdfs] job and copied into the hadoop filesystem. The actual “lab” part took about 30 seconds to implement and test, getting hadoop set up probably took another 30 minutes. Some students still hadn’t gotten hadoop running when the tutorial concluded a few hours later.  Another somewhat time-consuming action was getting Spring XD running in its fully [distributed] file and finally starting one XD admin process and an arbitrary number of XD container instances. However, this task proved worth one's while since it gave me the very first genuine “this is cool!” sensation of the tutorial.  ![streamandtap.png]  In the screenshot, we see how my three running XD containers have automatically load-balanced the "httpStream" and its four participating modules  onto the three different containers. Ignore that 'httpTap' for now, it's just a stream that taps the httpStream and redirects its output to a file sink itself.  To make a bit more sense of the stream above, let's go through the basic [DSL] for setting up that 'httpStream' present in the screenshot:  stream create --name httpStream --definition "http --port=9003 | filter --expression=payload.contains | transform --expression=payload.toUpperCase | file" --deploy  The important thing is the _--definition_ which enclosed inside the quotation marks defines a stream consisting of four modules, using the unix-style pipe character to denote 'piping' the result over to the next module:  1. A 'http' input source - listening on port 9003 2. A 'filter' processor - in this case all messages not containing a '1' character in the textual payload will be discared 3. A 'transform' processor - using an inline groovy statement 4. A 'file' sink, e.g. the output from the transform module will be written to a file, by convention named httpStream.out and placed in /tmp/xd on the file system.  So, if we do an http POST to localhost 9003 with the Stream above deployed containing a body such as 'Spring 1 XD' the file sink will result in the line SPRING 1 XD being written to the httpStream.out file. Banal, but comprehensible.  Anyway, besides from that really awesome automatic load balancing, the failover mechanism is quite sweet too:  ![onenodedown.png]  After killing one of the nodes by doing Ctrl+C old-school style in the console window, the second screenshot shows how the XD cluster automatically rebalanced itself on the two remaining XD containers. That happened in a split second thanks to the beautiful job done behind the scenes by ZooKeeper. Well - there were spots on this brightly shining example though - when bringing up the third XD container again, it sat empty and unused until I forcibly redeployed the httpStream. Spring XD doesn’t currently seem to be able to rebalance across XD containers as they become available, only doing so when containers go offline or when deploying streams or jobs.  This distributed scenario described above is definitely where Spring XD really shines. But there doesn’t seem to be any best practices in place on how to actually provision new hosts running XD containers and the required ZooKeeper, RDMBS and Redis/RabbitMQ/Kafka as each participating host must install all of these. That's likely rather easy to accomplish through virtualization images, chef, puppet, ansible or whatever but a topic not mentioned at all in the tutorial. Also, there were no mentioning at all about how one should manage one’s streams, jobs and taps expressed in the XD DSL or how to manage your custom jobs and processors which are packaged as jar files and uploaded as modules into the XD runtime. Another topic is logging - an enterprise may run scores of Streams and Jobs on an XD cluster, how would one differentiate streams from one another in logs? Are there mechanisms to track a given message in a stream across different XD containers?  From a DevOps point of view, I guess most of these things can be handled but this topic should have been brought up in the 300+ slide presentation.  I've previously complained about the trivial nature of the labs of the tutorial. An example lab scenario I would have liked is an incremental development of a complete data ingestion solution over the two days where the full ecosystem leveraging Spring XD eventually would come into play. I'm not talking about sinking some strings to a file, I'm referring to actually performing data ingestion from multiple sources where XD would have to transform, aggregate and sink data to various targets facilitating it for use in related products such as Apache PIG for analysis or perhaps Hive on top of Hadoop. Data simulation can be provided by a pre-made component  capable of simulating huge amounts of diverse data on the fly.  While the scope mentioned above sounds enormous for a 2-day tutorial, having the full stack of required software pre-packaged and configured into a virtualization image greatly reduces time spent on software installs. The actual lab tasks can still be limited in scope but made much more interesting by being more akin to real-life scenarios.  I think these two days spent in the Spring XD tutorial hosted by the Pivotal team was time well spent, though I think there is room for some improvement regarding compacting the presentation material and making the lab scenarios more geared towards how to use Spring XD to solve common real-life uses cases on the architectural level. Spring XD will certainly find traction among many enterprises dealing with Big Data in some way or another given its first-class support for hadoop and scaling abilities. For other applications not dealing with huge amounts of data at high throughput rates, Spring XD might just not be the best tool for the job.
In this part, we will outline an implementation strategy to encapsulate a Multi Tenant Data Access mechanism as a transparent, isolated [Cross Cutting Concern] with little or no impact on the application code. We will also introduce the notion of [Database Schema Migration] and explain why it is a critical part of a Multi Tenancy mechanism.  -[readmore]-  [comment]: #  [Multi Tenancy]: https://whatis.techtarget.com/definition/multi-tenancy [SAAS]: https://en.wikipedia.org/wiki/Software_as_a_service [Spring Framework]: https://spring.io/projects/spring-framework [Spring Boot]: https://spring.io/projects/spring-boot [Spring Data]: https://spring.io/projects/spring-data [Hibernate]: https://hibernate.org/orm/ [JPA]: https://en.wikipedia.org/wiki/Jakarta_Persistence [experimental support for the shared-database-using-discriminator-column pattern]: https://hibernate.atlassian.net/browse/HHH-6054 [Liquibase]: https://www.liquibase.org/ [Database Schema Migration]: https://en.wikipedia.org/wiki/Schema_migration [Flyway]: https://flywaydb.org/ [Cross Cutting Concern]: https://en.wikipedia.org/wiki/Cross-cutting_concern [Aspect]: https://en.wikipedia.org/wiki/Aspect-oriented_software_development [Reversibility]: https://martinfowler.com/articles/designDead.html#Reversibility [Github repository]: https://github.com/callistaenterprise/blog-multitenancy  [comment]: #  [neighbours]: /assets/blogg/multi-tenancy-with-spring-boot/undraw_neighbors_ciwb.png [Database Plan]: /assets/blogg/multi-tenancy-with-spring-boot/database-plan.jpg  - [Part 1: What is Multi Tenancy] - Part 2: Outlining an Implementation Strategy for Multi Tenant Data Access  - [Part 3: Implementing the Database per Tenant pattern] - [Part 4: Implementing the Schema per Tenant pattern] - [Part 5: Implementing the Shared database with Discriminator Column pattern using Hibernate Filters] - [Part 6: Implementing the Shared database with Discriminator Column pattern using Postgres Row Level Security] - Part 7: Summing up    Implementing database access to multi tenant data according to one of the patterns described in the previous part will require multi-tenant specific code to be developed. Depending on which pattern is chosen, the amount and characteristicts of the code will differ .  ![neighbours][neighbours]  In all cases, the code will most likely be used in all different parts of an application that access data. Hence the multi tenancy logic clearly constitutes a [Cross Cutting Concern], which can be tricky to cleanly decompose without *scattering* or *tangling* code as a result. Hence the multi tenancy pattern is best implemented using some sort of architectural capability or [Aspect], so that most parts of the application logic can be unaffected and totally unaware of the multi tenancy support. This is important to keep the technical complexity out of the application logic . It is also an important prerequisite for adopting an agile, evolutionary approach to multi tenancy, providing the necessary but tricky [Reversibility] of architechtural decisions that may allow us to start simple and evolve into more complex patterns in the future, if necessary.   An Object-relational mapper such as [Hibernate] or more generally [JPA] already provides an isolated, modular mechanism for general relational database access that hides many technical, low-level details. The [Spring Data] further raises the abstraction level, expanding it into non-relational databases such as MongoDB as well. At the heart of JPA, an aspect-oriented mechanism is used to inject a suitable implementation of the  interface into application code, with all the details about the underlying database connection kept fully separated in configuration. The [Spring Framework] provides excellent support for working with externalized configuration, wheres [Spring Boot] removes the need for explicit configuration by applying default configuration based on common conventions. The implementation strategy that we will define in this blog series will hence be to leverage the mechanisms of Spring Data and Spring Boot, but add the Multi Tenancy dimension. Our goal is to encapsulate the required code an configuration needed for the different Multi Tenancy patterns in such a way that they can be plugged in seamlessly.   Before diving into outlining the implementation strategy, let us introduce an important supporting mechanism: Database Migrations.  Database Schema Migrations refers to the management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that database's schema to some newer or older version. While migrations can be applied manually, in order to support agility in both development and operations, the migrations are typically performed programmatically by using a schema migration tool, such as [Liquibase] or [Flyway]. When invoked with a specified desired schema version, the tool automates the successive application or reversal of an appropriate sequence of schema changes until it is brought to the desired state.  !["database plan" by tec_estromberg is licensed with CC BY 2.0.][Database Plan]  Since both the Database-per-tenant and Schema-per-tenant patterns means all database tables are duplicated across tenants, a solid mechanism for automating Database Migrations will be critical. Hence we will have to include the setup and configuration of Liquibase in our implementation strategy from the start.   Time to start outlining the implementation strategy for encapsulating Multi Tenant Data Access. We'll start with the very basics, which is common to the different patterns: A mechanism for resolving the Current Tenant for each request, and make it available whenever needed.  A fully working, minimalistic example for this preliminary work as well as the forthcoming parts can be found in our [Github repository].   So let's start with resolving the tenant id to use for a request. The tenant id needs to be captured from some information associated with the current request  and be passed along to whoever needs it downstream. The idiomatic way to achieve this in Spring is to use a Web Interceptor to capture the information, and a ThreadLocal variable to invisibly pass it along to whoever needs it. Let's define a TenantContext class, to pass the tenant id along:   The exact mechanism for how to determine the Current Tenant will likely differ from case to case. Frequent options are to use an explicit http header, or to use a part of the domain's name to deternite the tenant id.  So let's continue and add an interceptor that capture the tenant id either from an http header  or from the sub-domain part of the request's server name:   Finally we add the configuration required for the interceptor:   We now have a transparent mechanism for capturing and communicating the Current Tenant to a downstream component.   The ThreadLocal mechanism only works out of the box for synchronous flows. If using asynchronous executions, we must also be able to pass along the Current Tenant to the asynchronous execution context. In Spring, asynchronous execution is encapsulated via the  abstraction. The  interface provides a mechanism to attach additional information to an asynchronous execution. Let's define a  class, to pass the tenant id along:   And the corresponding configuration to enable it:    We have taken the first preliminary steps in implementing an encapsuled mechanism for Dynamic Multi Tenant Data Access using Spring Boot. In the [next part], we'll implement the Database-per-tenant pattern using Hibernate, with Database Migrations using Liquibase and support for dynamically adding new tenants. 
 I guess you have noted that [functional programming] is gaining popularity? With Java 8 even Java SE has received support for functional programming. In this blog I have tried to use Lambda Expressions and the Streaming API to address a typical every day problem, building a query API for the well-known object structure of Orders, Order Lines and Products.  -[readmore]-  After defining the query API I first implemented the API using a traditional [imperative programming] style and then I tried to re-implement it using a more functional oriented programming style. Let’s see how that worked out…   Let’s start with a minimalistic  model over the well-known structure Order, Order Line and Product:  ![]  I decided to write a query API that could be used to answer the following questions:  1. What products from category X have been ordered in the date interval M to N? I want them ordered by their weight!  1. What products with weight from X to Y have been sold in orders with an order value from M to N? I want them ordered by their product ID!  First I defined a Java interface:   Yes, I know, I like long descriptive method names ☺   Next, I implemented the interface in the good old way using imperative programming. The first query-method  looks like:   As you can see I assumed that we can acquire all orders using the method  and I don’t really care how it is implemented for now. The implementation of the query-method is based on traditional for-loops and if-statements to filter out the products that meet the specific requirements. The implementation wraps up with removing duplicates and sorting the result as required.  The second query-method  looks very similar in its structure with the only differences being the specific implementation of its filtering and sorting rules:   When looking at the result I see source code in common with the two query-methods and in general excessive code that is required to navigate through the object structure. Not good. Can we rewrite the code to be more concise and unambiguous?   Let’s see if we can improve using the Java 8 Stream API and support for Lambda Expressions!  If the Java 8 Stream API and support for lambda expressions are new to you it might be worth spending a few minutes reading: • Lambda Expressions, [Part 1] • Java SE 8 Streams, [Part 1]  My first attempt looks like this:   Okay, the code is much more concise but what is actually going on?  First we convert the list of order-objects that `getOrders:    Next we apply a filter function to the order-objects, in order to keep the ones that are within the specified date interval:    > __Note:__ I use type inference ...`. > >Type inference should be used wisely, or else the code can be hard to read for others then the writer, but I hope you find it proper to use in this case. The type of the parameter is given by the result of the previous method call. In this case the call to  returns a stream of  objects and therefore the parameter  must be of type .  After that we need to attain the products in the selected orders. We find the products by traversing the order lines of each order. To transform the stream of orders to a stream of order lines we use the  function:    >__Note:__ Using the normal  function to transform a stream of orders to a stream of order lines as observed in the following example: > > > >... is not working since the  method on the list.  Given a stream of order lines we can easily transform it to a stream of products as:    ...and filter out the products from the selected category:    We wrap up the processing of the stream with removing any duplicates ` function:      ...and return the stream as a list of products using the  function:    Doesn’t that look promising?  Let’s also quickly look at the other query-method:   This code looks very similar to the implementation of the first query-method. Much more concise than before, but both methods still share source code of how to traverse the `order –> order lines -> product` structure. Can we avoid repeating this?   Yes, we can!  Let’s define a third method that takes three functions as parameters, two predicate functions to perform the filtering on orders and products respectively and one comparator function to perform the final sorting:   Now we have encapsulated the knowledge on how to traverse the object structure in one single method and the implementation of the query-methods is only about expressing the specific logic for their filters and ordering:    We now have source code that is concise and contains no code duplication, i.e. we are following the [DRY principle]!  Great, isn’t it?   The source code of this blog can be found at GitHub and given that you have Java 8 as your default Java environment you can try it with the following commands:   It should produce output similar to:   Given that you are a little accustomed to the Java 8 Streaming API and the support for Lambda Expressions, I’m sure you will enjoy functional programming in Java. You will start to find more and more areas where you can use it to make your code more concise and unambiguous, i.e. easier to write, read and maintain!   
 SwiftUI is an exciting new way to create UI on Apples platforms. But it is still a young technology and it can sometimes be hard to find information needed to support the implementation of more advanced use cases.  In porting an app to SwiftUI, I’ve been struggling a bit with the  view.  is a very powerful container view that makes it incredibly easy to create table style components.  But if you want to up your game by adding more features, such as dynamically loading more rows as the the user scrolls, or fetching data asynchronously, it gets a little trickier. That’s when you would like to know more about how  works its magic behind the scenes - but information is scarce about things like cell reuse, redrawing, handling animations, etc.  After some experimentation , I managed to come up with a list that met some more advanced requirements. I also created a few helper components that I would like to share - in case someone else is looking for similar solutions.  The requirements for my list are as follows.  1. The list should grow dynamically and batch-wise as the user scrolls 2. The data on each row is fetched from a  slow data source - must be possible to be performed asynchronously in the background 3. Some row data should be animated in two different situations: a) when data has been fetched and b) whenever a row becomes visible 4. Possibility to reset the list and reload data 5. Smooth scrolling throughout  Below is a simple video that shows an example of how the list should work.  [![Video]   I will now walk you through essential parts of the solution. The complete code is shown at the end of this article and can also be downloaded here: <https://github.com/callistaenterprise/SwiftUIListExample>  We start with some of the plumbing which consists of a few protocols and generic components which may be used as a foundation for any list of this type.   First off is the data model and we begin with the  protocol which should be adopted by the component representing row data. It must have an  index which is used to determine when more data needs to be loaded, more on this later.  It also contains a function -  - that retrieves data for the row in a second - possibly slower - step.  Next out is the  - a generic class that maintains the actual list of data items, stored in a published property that is used directly by the  view.  The method  is called by the view when a new row is presented. It will check if more items need to be fetched and will also initiate loading of additional data for each row.  The properties  and  can be used to fine tune the list behaviour.    Now we come to the second part of the plumbing - which concerns the views. The structure is similar to that of the data model components. There is one protocol -  - that the row view should adopt. And a generic struct -  - which is used as the actual list view.   may need some further explanation. The generic parameter  is a custom view that is used to visualize the row in any fashion you would like. The list gets it data from , a property of the type  that we defined above.  The  modifier on the row view is there to ensure that more items are fetched as needed by calling  every time a row becomes visible.  There is also an  modifier on the list view itself. What is the purpose of that, you may ask? The  property provides a unique id that remains the same until the list is reset, in which case a new unique id is generated. This ensures that list is completely redrawn when it is reset. Leaving it out could sometimes cause the list to not fetch enough data when reset, especially for small batch sizes.   That’s all for the plumbing, let’s move on to an example of how to use this to create our list.   Here is an example of how to implement a list using the components we’ve defined above.  To be able to demonstrate how it works with asynchronous data loading we start with a very simple datastore called . It offers a static function that will give us a random amount between 0 and 1 as a publisher which takes between 0.5 and 2.0 seconds to deliver the value - implemented as a  on a background queue.   Next, we create our custom data item in the class  which conforms to the  protocol. It is a quite straightforward implementation of the protocol requirements. Note that  needs to be a stored property in order to keep the publisher from  alive while waiting for the data to arrive.    The final part of the list is our custom row view which conforms to the  protocol. The row contains a horizontal stack with three elements: - A text view displaying the line number - A text view displaying "Loading..." if data is not yet available, otherwise displaying the amount. - A custom graph bar view that displays the amount graphically. The implementation of  is given with the complete solution at the end.  The animation is triggered by the two modifiers on the horizontal stack. The first one -  - is used when data first arrives from the . The second one -  - is used when the row appears and data is already available.  > **Note:** The animation triggered by  and  needs to be mutually exclusive, otherwise the animation will not work correctly. That’s why we test the property .   We now just need to wrap it all up in a container view as shown below.    I hope this article contains some useful information that will help you build your own lists based on similar requirements. Please let me know what you think, and feel free to post any questions, comments or suggestions.  The complete code is given below. You can also download sample XCode-project here: <https://github.com/callistaenterprise/SwiftUIListExample> 
 Not long ago I got caught up in a discussion on agile development from 2001 to remind me on the core points of agile development.   Have you been on a project where you have had a very good process or tools and that you can say that it was the process or tools that made you succeed, regardless of the people involved? I haven't, but I have been on numerous projects that have been saved by skillful, competent and sometimes heroic people. This does not mean that we should ignore and not value processes and tools and just throw them overboard. It just means that we should value people more. If the process or tools gets in the way of people making them less productive, something is wrong. When you follow the agile principles, you are also encouraged to have the guts to reveal your weaknesses and ask for help from your fellow team members. This helps spreading knowledge and it also helps spreading good practices. You are in it as a team and not as individuals, so if you do not get the team interacting in a good way you are not doing agile.   Well, at the end of the day it is the working software that our customers pay us for. The documentation is good to have, but it is the working system that is and must be our priority number one. Have you ever tried to do a requirements analysis on a web interface without some kind of mockup? If you have, I am pretty sure that you agree that no matter how good your use-case descriptions are or how fancy looking your photoshop pictures look, the customer will want to change the final result when you show it. Instead of producing pages of documents describing how you should build something, just build it and show it in order to get fast feedback. Find a way to get running software to show to your customer as often as possible to get feedback and correct according to that feedback. This is not easy but it should be your goal to deliver software that the customer really wants and finds valuable.   In the case with the web interface above, this statements mean that it is better to have a close collaboration with the customer and straiten out the requirements in an iterative process. Start to build some of the interface and show it to the customer, get feedback early, rebuild and show it again. Go round this loop until the result is satisfying for everyone. If you start with a signed off requirements document and deliver a working interface a few months later without any customer interaction or feedback, I am pretty sure that you are delivering something that the customer is not expecting and maybe not even wants. Now you say that "what if the customer always changes his mind? We have to finish all these use cases". That is the key point with this bullet. You have to get the customer "on board" and make them realize what major changes might result in. It is up to your customer to prioritize the requirements in order to maximize return of investment. In an agile project the resources like man-hours are more or less fixed. We realize that we have the team and resources that we have and the functionality is the rubber-band. We try to maximize the return of investment with the resources we have and keep shipping high quality software while doing it.   We all know that things are going to change during a software development project. Requirements are going to change, technology might change, people might change, the market might change and so on. When doing agile development you do not try to control change, you manage change. You have very short iterations and the only requirement you put on your customer is that they should know what they want for each iteration. With iteration lengths of two to three weeks that means that you require them to know what they want two to three weeks in advance. I do not think that that is to much to ask, even if I have been on projects where it apparently was. The customer gets the benefit of being able to change his or her mind and re-prioritize the requirements every two to three weeks. Isn't that great for the customer?  Take a look at the [Agile Manifest] again and try to find where it say that you should not do any documentation. You are right, it is not there. It is about how you value different aspects of a software project and the things in bold on the left side always have more value compared to the ones on the right side. Just remember that that doesn't mean that the things on the right side have no value at all!  I hope you think that this makes sense  and that the social aspects of software development sometimes are more complex than the technological ones.
Here at CQon there is a track dedicated to service-oriented architecture - mainly focused technical aspects. When things get complicated or structured to a level where the fun or the productivity or both are gone, application architects and developers tend to look for new approaches - typically more agile than current best-practice.  We have seen Corba go for SOAP, C++ go for Java, pure HTML apps go for Ajax etc. Currently there seem to be two major trends being debated - at least here at QCon:  - Java versus dynamic languages, like Ruby and Groovy. - Web Services versus HTTP Services   The most animated discussions are definitely on the SOA arena: Web Services versus REST.  When a technology matures, you typically forget all the problems you had before the technology was available. You get focused on the other side of the coin - the price-tag of getting the problems fixed by a new technology. Looking eight years back, Simple Object Access Protocol  arrived to rescue projects from Corba complexity:   SOAPs major merits was its heritage of the technologies that boosted the success of the World Wide Web: HTTP and XML. Then, all the dream went into reality. Application interop requires more than an agreed wire format at the syntactic level . WSDL arrived to extend SOAP with a standard for expressing service metadata. The term Web Service was coined. Then, requirements for security, integrity, reliability and more was added through more or less coordinated standardization efforts commonly referenced as "WS-*". The WebService interoperability Organisation was created to standardize the standardizations. Again, we went from over simplified to over-complicated:  And the reaction to over-complexity arrived with the increasing popularity of REST -"SOAP done right":  REST has a lot of promise. Although, it is a very accessible technology, it also represents a different perspective on architecture, than SOA in general. REST views services as resources uniquely identified by a URL and and the finite set of operations that can operate on these resources. the operations are defined by the HTTP protocol: GET, PUT, POST, DELETE. This style of architecture is squeezed into WS-* by a strangely named specification named WS-Transport. However, WS-Transport still lacks the accessibility of REST, due to its roots in SOAP.  Former Web Service evangelist, like Steve Vinosk  are spending the time bashing SOAP and WS-\*. HTTP Web Services and the architecture represented by REST is the new reaction to the over-complicated best-practice. REST has been used for many years and is core interfacing technology at global players like Google. Amazon is also increasing the use of REST. Looking at the history, is there anything specific with REST, that prevents it from starting its journey up the complexity scale, repeating the history of mainstream pre-decessors? Either that, or fail due to inability of accommodating new requirements? Are we heading REST-\* ? 
 I've been using CloudFormation in my latest project to automate the process of creating new environments. With CloudFormation one can use predefined templates to deploy new infrastructure on Amazon Web Services.  -[readmore]-  In the following example I will create a new EC2 instance with an Apache Tomcat server, Apache HTTP Server and two web applications. I'll also set up security settings and attach an elastic IP address to the instance. -[readmore]- The template can be found [here]. To deploy the stack described in the template choose "CloudFormation" in the AWS Management Console. Then click "Create stack" and upload the template file.  First of all we want the user to fill in some parameters when creating the stack in CloudFormation. In this case an existing EC2 KeyPair and the instance type:   Specify conditional values using Mappings.  specifies allowed EC2 instance types.  defines which AMI that should be used in each region. We will use EBS backed Amazon Linux AMI:s.   Then we define the AWS resources that should be created.   is the security group used by the AWS instance to define the firewall rules. In this case it should be possible to connect to the server with SSH and HTTP.   Specify an admin user with full administrator rights   Generate a secret access key and assign it to the admin user.   Create the web service instance and specify the packages to install. In our case the Java JDK, Apache Tomcat and Apache HTTP server.   Download the web applications to deploy. Artifacts found in a S3 bucket in this case.   Set AMI specified in  by region.  and  is set by the user when the stack is created . Set name of the instance to .   Define scripts to run on the server on first startup:   Create an elastic IP and bind it to the instance:   Wait for the scripts defined in the WebServer configuration to complete before completing the stack creation.   Return the IP address and public DNS name of the new instance when the stack is created.   For a more comprehensive guide, check out the [CloudFormation User Guide].
At SpringOne2GX I saw [Michael Ploed] and then some reflections related to microservices.  -[readmore]-  The [Event sourcing] pattern is, like Michael defined it, all about: "Event sourcing is an architectural pattern in which the state of the application is being determined by a sequence of events". There are some important implications of that:  * The full history of events are stored and can be inspected or used for playback to restore system state to a given point in time. * Events are typically stored in an object/document format, as opposed to being de-composed into a relational model. Note: In a traditional relational model, keeping historical records often takes quite a bit of work and being able to see what the state was at a particular point in time is often not possible, historical records are often kept on a per-table basis . * Events must be immutable and may never be deleted, or the system state can't be re-built. Delete is to be implemented as an own event, that is appended to the event store. * Running queries for current state against the stack of events would typically perform bad, since the application would have to rebuild/maintain the current state from all stored events. Mitigating this problem is up to our next pattern.  The [Command Query Responsibility Segregation  is basically a pattern for separating access to datastore in an application into different services for read and write operations. The major points of this being:  * possibility to scale read-parts separately from write-parts * possibility to choose an appropriate datastore technique for read vs write and optimize for each case, for example: for a read-heavy database you might want to have the equivalent of a hevily de-normalized database   If we let write-operations append to the event store and then let those events propagate asynchronously to a read optimized datastore  we can have a solution that is scalable and have some nice features as pointed out above. Notice that the async propagation of data from write-to-read access introduced eventual consistency into the solution, but we can't have it all, remember the [CAP-theorem].  Both event-sourcing and CQRS have been around for some time so that's not really new. What's interesting in the microservice context is that some event-handling will most likely be needed to synchronize state between different microservices in any reasonably complex landscape and these patterns can be a useful combination. In a more traditional monolithic solution the need for synchronization would often be less, typically due to access to a large data-model all at once, sometimes paired with a codebase with loose internal boundaries for data access. If we look at the success stories listed in the beginning of the [Microservices presentation], both Karma and SoundCloud use event-handling as vital parts in their architecture, although not exactly as above.   Note: the video recorded talk will show up on infoq.com later on, together with all the other talks from SpringOne2GX 2015.
 In my current project we are rewriting a feature-packed Struts application [^1] into a single page web app using Backbone and Bootstrap. The need for architectural patterns is becoming more and more evident as the application grows and we realize how many different ways you can use Backbone to do the same thing.  -[readmore]-  The good thing is that for me, coming from an enterprise Java and OO world, the Backbone objects feel really natural to work with. I get to write  and can refactor my handler code into separate methods to avoid too deep callback nesting.  Another realization is the need for a logical file organization, to store all views in one folder soon became unwieldy so we decided to group them by application functionality instead. With the amount of files increasing, the headache of load order and dependency management could have been a constant source of errors. But that is not a problem in this project and I’d like to share the solution with you.  We are using a module loader called [RequireJS] modules. An AMD module is an encapsulated and reusable piece of JavaScript code that can be loaded asynchronously because it contains a declaration of all other modules that must be loaded before it can run. To define a module I simply wrap my function in a define call:   Now I have a module that can be reused, e.g. to define a  that uses :   I prefer to think of the dependency array as "imports", but there is another benefit hidden in the define wrapper – namespaces. Inside my module I have a private namespace, and I can name my dependencies to whatever makes my code understandable without having to worry about name collisions. I don’t have to work with long namespaced variables, even though I have grouped my files in an elaborate hierarchy. This is a great way to encapsulate code and separate concerns.  The third benefit is loading, RequireJS helps me load all dependencies in the correct order. All I import in the index page is a config file, that specify where my modules are located and the module to load to startup my application. If I need to use any third party libraries that are not AMD modules, I can use a "shim" to help require interpret them as modules. If you have certain parts of an application that can be loaded later, RequireJS supports on the fly loading as well.  This sounds like there will be a lot of files and these days best practices dictates that we minify them and bundle them together. RequireJS also provides a tool for this, called [r.js]. It can be run in the browser, on Node or Rhino. I tried with node and was happy to see that all it took was a simple `npm install` [^2] call to install.   The  is a build file that describes where my js-files are located and where to put the minified files. All my files were minified and concatenated into one ready to deploy file. Naturally, the order of the concatenation is handled by RequireJS. Since we use a shim to define Backbone as a module and declare its dependencies, we need to use the  option, otherwise the shimmed modules are ignored and not included. This is my basic :   Another thing that RequireJS does is provide plugins, that can be loaded in the same way as other dependencies. The most important plugin is called "text" and it allows me to move my template markup into a separate file. I can then load the template into my module and access it via a variable just like any other module. There are many other plugins, e.g i18n. Take a look at [http://requirejs.org/docs/plugins.html] for a list of plugins and also information on how to create your own. That could also be a good way to encapsulate some reusable logic in your application.  I’m really happy that we chose RequireJS, it keeps saving my sanity when we switch plugins and other dependencies or refactor our code. The templates and text plugin helps in keeping the html in separate files, which is a big bonus. The code is modular and easier to maintain.  Since I can’t show you my project code, I have rewritten our [Backbone tutorial app from the 2013 Cadec].  I also used the text plugin, to illustrate how easy it is to separate the markup into files. The templates are inlined as text in the modules when running the optimizer, so the separate files is only for development.  * [http://requirejs.org] * [http://addyosmani.github.com/backbone-fundamentals/#modular-development] * [http://requirejs.org/docs/whyamd.html]  [^1]: Struts application: JavaEE, server-generated webapp, early 2000 [^2]: npm installs a module in Node, -g installs it globally
Are you one of those that think that Ant is not really the right way to do it and that the ideas behind Maven are really great, but you have never really felt comfortable using it. You might just be one of those that just think that XML is a big step for mankind in no particular direction at all or that XML is just not a very good tool for writing computer programs. Then you should check out a new build system for Java applications that lives in the Apache incubator. The build system is called Buildr and was born out of the frustration of using Maven.  Buildr is a nice DSL written in Ruby targeted at building Java applications based on Rake, the build system used for Ruby. It is a drop in replacement for Maven 2 and reuses both Mavens dependency management and file structure. If you have a Maven 2 project it is very easy to get started. Just install Ruby and Buildr  and go to your project directory and type 'buildr' and Buildr will create a nice buildfile to get you started with.  A Buildr project definition can be as easy as this one:   This definition will download the Axis dependencies, compile and test your source and package it nicely into a jar file.  If you get to the point where you do feel that the builtin tasks in Buildr are not enough for what you want to do you are free to extend it using Ruby and the Ruby libraries. Buildr also integrates with Ant, so if you think that you have just the right Ant task for the problem just call it from Buildr.  A nice Buildr screencast tutorial can be found here
 Getting back from one year off demands some effort to get started on a whole new computer.  I installed Eclipse and then installed the subclipse plugin to connect to Subversion through the “Install new software” in Eclipse. No problems. My code was already checkout on disk and created as Eclipse project with maven and imported as existing project. After installation of the subclipse plugin I tried to share the projects to the repository in subversion and got my first error: JavaHL library is missing. Reading and searching on the internet I tried two solutions:  1. Installed the JavaHL library 2. Unmark the use of JavaHL and use the SVNKit, Pure Java instead  The first solution resulted in mysterious things, the .svn files was suddenly gone on disk. I then had to copy my old project on disk to save the changes I had done, check out the project from SVN perspective in Eclipse. Copy the changes into the new project and then be able to commit the changes. Got tired of this procedure.  I tried the second solution, run on SVNKit instead. Could not join the project proper, the repository just complained that the project “was already existing” in the repository and the folders are not allowed to exist when committing.  I removed all installations regarding subclipse and then installed subversive plugin instead. Restarted eclipse and voila: All projects were now connected by default when entering eclipse and I could do my changes and commit perfectly. And even restart Eclipse and still, all projects are connected!  Conclusion: I will use subversive plugin from now on. Ohh and did I mention, I am doing this on a Mac – can’t imagine that it has something to do with it… There is probably some more technical differences to consider, but this will be my choice for now.
 In a recent project, I was assigned to setup monitoring of a set of web services. The idea was to call the web services every 5th minute and check whether they operated normally or not, i.e a valid SOAP response was returned. The web services used SSL Mutual Authentication to authenticate the calling client. Since I just wanted to make an easy setup; without invoking a "real" web service client  that would perform scheduled checks of the web services. This article can be of use when you simply want to call a web service from the command line or when you are in a headless environment.  -[readmore]-  When I started I had a pkcs12 file  for authentication, the endpoint address of the web service, and an xml file that should be used as input data to the web service.  [Curl’s man page] and executed following commands in my terminal:           For more information about the commands above check the [pkcs12 man page] I could start building my command to call the web services.   I had my three pem files for authentication, then endpoint of the web service, and I had my request xml file to post to the web service. To pass data to Curl I simply used the  parameter and since I had a file with my data I prepended @ to the file name. But using this option makes Curl to set the  http header to . I wanted to pass a soap xml file, and according to the standard it requires that when a soap message is placed in a http body the  header must be set to  . I formed my initial Curl command like:   I tried to execute the command just for fun but to no surprise it did not work. Curl complained that it tried to verify the server certificate using a default bundle of ca certificates. This was not what I wanted because I trusted the server certificate. To tell curl to not perform this verification I included the  or  parameter in my call. Of course, once again, Curl complained about bad certificate. Since I had my client certificate and the private key I combined the  and `-–key `parameters that formed the following command:   The call above gave me a SOAP response. However, it contained a SOAP fault:   After some research, I discovered that I had missed to add the  http header as required . I added the header and formed my final command:   Finally, success!  As you may have noticed I haven’t used the  file, which I created earlier. This is because the  parameter simply ignores the verification of the CA certificate. If I want to verify the server certificate using Curl, I simply removed the  flag and added the `–-cacert <cacert>` parameter. The final command now looked like:   Now, when we have a proper Curl call to a web service, we can easily use it together with Nagios for scheduled monitoring. How to configure Nagios is however out of scope of this article.  This article has shown that it can useful to use Curl to call web services under some circumstances. Especially in a closed, headless environment where there is no access to graphical tools. It is a quick way to just test if the authentication works and that it is possible to get result from a web service. The last thing I would like to mention about Curl, that is very useful, is the `–-trace-ascii <outfile>` parameter. Setting this, all incoming and outgoing data in your call is dumped to a file. I found it especially useful for debugging SSL/TLS handshakes.
 This blog post is mainly about updating the underlying software used in our [blog series] and upgrading the Docker development environment to Docker For Mac v1.12. Spring Boot has also been upgraded to v1.3.6.  We will also throw in a Spring Cloud Config server for externalised configuration and Spring Cloud Sleuth for handling correlation ids but the details regarding the configuration server and correlation ids will be saved for later blog posts.  -[readmore]-  The blog post basically repeats the tests performed in part 1 - 4 in the [blog series], but on the upgraded versions. First we verify that we have installed the tools we need, then we will get the source code, build it and finally perform tests to verify that everything works.  If your are interested, here are some links to information on how to migrate source code from Spring Cloud v1.0 :  * [Migrating Spring Cloud apps from Spring Boot 1.2 to 1.3] * [Migrating OAuth2 apps from Spring Boot 1.2 to 1.3] * [A @LoadBalanced RestTemplate is no longer created by default] * [Spring Cloud Bus is now powered by the recently released Spring Cloud Stream]   * You need to have Java SE 8 and Git installed to be able to checkout the source code and build it. * To deploy and run it you will need [Docker]. * To be able to run some of the test commands used below you also need to have the tools [cURL] installed.  It is strongly recommended that you give your Docker environment a lot of memory and CPUs/cores. I have configured Docker for Mac to use all eight CPU cores available on my Mac and allocating up to 8 GM of memory if required:  ![Docker]   Open a terminal and  into a folder of your choice:  $ cd a-folder-of-your-choice  Since we have externalised our configuration into a configuration repository we first need to get it from GitHub:  $ git clone https://github.com/callistaenterprise/blog-microservices-config.git  Next, we get the source code from GitHub:  $ git clone https://github.com/callistaenterprise/blog-microservices.git $ cd blog-microservices $ git checkout -b B8 M8  Time to build our microservices with:  $ ./build-all.sh  Finally, we verify that we have docker images created in our docker environment:  $ docker images | grep blogmicroservices blogmicroservices_rev latest 02a156c0e27d 3 days ago 126.1 MB blogmicroservices_composite latest 1c7b4d313595 5 days ago 128.2 MB blogmicroservices_pro latest 315c508ee5c9 5 days ago 126.1 MB blogmicroservices_auth latest e7ec2f569efc 5 days ago 106.4 MB blogmicroservices_edge latest 501ea469296d 5 days ago 125.2 MB blogmicroservices_rec latest d0331cbe7451 5 days ago 126.1 MB blogmicroservices_config latest 5452ef106f6a 5 days ago 126.9 MB blogmicroservices_discovery latest 82c0cc4f1f71 5 days ago 124.1 MB blogmicroservices_monitor latest a142cf8fe027 5 days ago 109.7 MB   Now, we can start up the microservices as Docker containers:  $ docker-compose up -d  We can follow the startup procedure with the command:  $ docker-compose logs -f  ...once the output from the logs stops rolling   In another terminal window, try the following curl command:  $ curl -s -H "Accept: application/json" http://localhost:8761/eureka/apps | jq .applications.application[].name  It should respond with something like:  "REVIEW-SERVICE" "CONFIG-SERVER" "RECOMMENDATION-SERVICE" "COMPOSITE-SERVICE" "EDGE-SERVER" "PRODUCT-SERVICE"  We can also open the following URL in a web browser to get a graphical representation of the same information:  [http://localhost:8761]  It should look something like:  ![Eureka]  See [Part 1] regarding details of the microservice landscape in the blog series.   First we will get an OAuth Access Token and execute a successful request using the Access Token.  Next we will use Docker to scale up and down the number of microservice instances.  Thirdly we will introduce problems into the microservice landscape and see how our circuit breaker, Netflix Hystrix, can be used to monitor and mitigate the problem.  Finally, we will use a shell script to fully automate an end-to-end test, including starting up a test environment, run tests and then bring the test environment down again.   Since our microservices are protected using OAuth we first need to get an OAuth Access Token. To avoid involving a Web Browser we will use OAuth [resource owner password credentials grant] flow:  $ TOKEN=$ $ echo $TOKEN f2340e30-2122-473c-9d6d-e9a27e912050  See [Part 3] for details regarding use of OAuth.   Now, we can use the access token to execute a request to the microservice landscape:  $ curl -H "Authorization: Bearer $TOKEN" \ -ks 'https://localhost/api/product/1046' | jq . { "productId": 1046, "name": "name", "weight": 123, "recommendations": [ ... ], "reviews": [ ... ] }  After a couple of requests the log should look something like:  composite_1 | 2016-08-14 06:24:19.634 INFO [composite-service,ba5eab537ac321b4,ebb02a82821b3a0,false] 1 --- [ XNIO-2 task-2] s.c.m.c.p.s.ProductCompositeService : Synch start... pro_1 | 2016-08-14 06:24:19.641 INFO [product-service,ba5eab537ac321b4,8b39d9547a53e9ba,false] 1 --- [ XNIO-2 task-2] s.c.m.c.product.service.ProductService : /product called, processing time: 7 rec_1 | 2016-08-14 06:24:19.663 INFO [recommendation-service,ba5eab537ac321b4,b406512ef768f18c,false] 1 --- [ XNIO-2 task-2] s.c.m.c.r.service.RecommendationService : /recommendation called, processing time: 15 rev_1 | 2016-08-14 06:24:19.694 INFO [review-service,ba5eab537ac321b4,b5a63711698de4be,false] 1 --- [ XNIO-2 task-2] s.c.m.core.review.service.ReviewService : /reviews called, processing time: 5  **NOTE:** In the log output you can see that the output from the four microservices contains one and the same correlation id, . This is Spring Cloud Sleuth in action, more about that in a future blog post!  See [Part 1] regarding details of the microservice landscape.   Scale up the Recommendation Service to two instances:  $ docker-compose scale rec=2 Creating and starting blogmicroservices_rec_2 ... done  Note output in the log!  Verify that we now have two instances:  $ docker-compose ps rec Name Command State Ports --------------------------------------------------------------------------- blogmicroservices_rec_1 java -Dspring.profiles.act ... Up 8080/tcp blogmicroservices_rec_2 java -Dspring.profiles.act ... Up 8080/tcp  Execute some new requests  in the log:  rec_1 | 2016-08-14 06:32:11.900 INFO [recommendation-service,c7e72729ea4f882a,809aff42409141c4,false] 1 --- [ XNIO-2 task-4] s.c.m.c.r.service.RecommendationService : /recommendation called, processing time: 12 rec_2 | 2016-08-14 06:32:12.839 INFO [recommendation-service,75f6a7a27a8c0bb4,a558db8647189c44,false] 1 --- [ XNIO-2 task-2] s.c.m.c.r.service.RecommendationService : /recommendation called, processing time: 7  Scale down the Recommendation Service to one instance:  $ docker-compose scale rec=1 Stopping and removing blogmicroservices_rec_2 ... done  Verify that we only have one instance left:  $ docker-compose ps rec Name Command State Ports --------------------------------------------------------------------------- blogmicroservices_rec_1 java -Dspring.profiles.act ... Up 8080/tcp  Execute some more requests and note responses only from **rec_1** in the log:  rec_1 | 2016-08-14 06:34:11.198 INFO [recommendation-service,268a97317916e011,4aec1db3daf67fa9,false] 1 --- [ XNIO-2 task-6] s.c.m.c.r.service.RecommendationService : /recommendation called, processing time: 8 rec_1 | 2016-08-14 06:34:13.094 INFO [recommendation-service,6fa3111ae02b02a0,7f6a62fa890afd70,false] 1 --- [ XNIO-2 task-7] s.c.m.c.r.service.RecommendationService : /recommendation called, processing time: 10  See [Part 4] regarding more information on how to use Docker.   We will now configure one of our microservices to respond slowly. This will make it possible to see our circuit breaker, Netflix Hystrix, in action. See [Part 2] for background and details on the circuit breaker.  Our microservices use a Spring Bean, , to calculate a simulated processing time.  calculates a random processing time given two configuration parameters:  and .  These parameters can be changed during runtime using the configuration server without require a restart of the affected microservice. See  for details.  Let's try it out!  Change the processing time limits and the log level  in the Review Service by editing the file  in the configuration repository:  $ cd ../blog-microservices-config $ vi review-service.yml  Remove the comment marks  in the file. After you are done,  should look like:  service: defaultMinMs: 6000 defaultMaxMs: 6000  logging.level.se.callista: DEBUG  Commit the change to the Git based configuration repository:  $ git commit -a -m "make review service slow and increase log-level to DEBUG" $ cd ../blog-microservices  Ask the Review Service to reload its configuration:   $ docker-compose exec rev wget -qO- localhost:8080/refresh --post-data="" ["logging.level.se.callista","service.defaultMaxMs","service.defaultMinMs"]  Execute a new request to verify that the microservice now responds slower:  $ curl -H "Authorization: Bearer $TOKEN" -ks 'https://localhost/api/product/12345' | jq .  From the log output we can now see  level logging and that the processing time for the Review Service has increased to 6 sec:  rev_1 | 2016-08-14 06:45:23.036 DEBUG [review-service,1962e7e12f2d3f6b,4992a65717f568b6,false] 1 --- [ XNIO-2 task-11] s.c.microservices.util.SetProcTimeBean : Return calculated processing time: 6000 ms   This will cause the Composite Service to timeout its request to the Review Service and, using Netflix Hystrix, applying a fallback method to compensate for the missing response:  composite_1 | 2016-08-14 06:45:27.033 WARN [composite-service,1962e7e12f2d3f6b,e2ddef075bb6640,false] 1 --- [ HystrixTimer-8] s.c.m.c.p.s.ProductCompositeIntegration : Using fallback method for review-service  The result can be seen in the response from the Composite Service where the review part now comes from the fallback method in the Composite Service :  "reviews": [ { "reviewId": 1, "author": "Fallback Author 1", "subject": "Fallback Subject 1" } ]  We can also monitor the state of our microservices using the Netflix Hystrix dashboard. Immediately after a failing request the [dashboard] should display a timeout error for the  circuit like:  ![Hystrix-warning]  Since only one error is reported on , Hystrix keeps the circuit <strong style="color: green;">Closed</strong>, i.e. still allow requests to the Review Service to come through!   Now, we will make things worse. Force Hystrix to open the circuit by executing three requests immediately after each other. Verify that  circuit now is <strong style="color: red;">Open</strong> in the [dashboard] like:  ![Hystrix-warning]  The dashboard reports two timeout errors on the  circuit and the circuit is open, i.e. it does not allow calls to the Review Service!  If we execute a new requests to the API:  $ curl -H "Authorization: Bearer $TOKEN" -ks 'https://localhost/api/product/12345' | jq .  We will still get the fallback response in the review part, but the response is **returned immediately**. This means that the circuit breaker **failed fast**, i.e. never tried calling the Review Service!   After a while  the circuit breaker will allow a request to be passed to the Review Service, to see if the services is working again. Once the failing service is operational again the circuit breaker will detect this and close the circuit again. This means that the system landscape is self healing!  Let's make the Review Service fast again!  First remove the changed response time limits and remove the debug log level in the configuration of the Review Service:  $ cd ../blog-microservices-config $ vi review-service.yml  Add back the comment marks . After you are done,  should look like:  #service: # defaultMinMs: 6000 # defaultMaxMs: 6000  #logging.level.se.callista: DEBUG  Commit the change to the git based configuration repository:  $ git commit -a -m "make review service fast again and decrease log-level to INFO" $ cd ../blog-microservices  Ask the Review Service to reload its configuration:  $ docker-compose exec rev wget -qO- localhost:8080/refresh --post-data="" ["logging.level.se.callista","service.defaultMaxMs","service.defaultMinMs"]  Make a new request:  $ curl -H "Authorization: Bearer $TOKEN" -ks 'https://localhost/api/product/12345' | jq .  Probably 30 sec has passed while we were reconfiguring the Review Service so the circuit breaker allows a request to see if the service is up again:  rev_1 | 2016-08-14 07:21:38.634 INFO [review-service,2a7ce13ae36ce0d,258cec0ffc5cf69d,false] 1 --- [ XNIO-2 task-24] s.c.m.core.review.service.ReviewService : /reviews called, processing time: 6  We will get a normal response and the circuit is closed, i.e. the system landscape is self healing:  "reviews": [ { "reviewId": 1, "author": "Author 1", "subject": "Subject 1" }, ... ]  Also verify that the  circuit is <strong style="color: green;">Closed</strong> in the [dashboard] like:  ![Hystrix-warning]   Even if doing these tests are rather fun to perform the first time   Therefore I have written a small bash-script that automates parts of the tests above   The script will fist verify that all services are registered in Netflix Eureka, then get an Access Token from the OAuth server and finally call the composite service through the edge server.  Optionally, the script can also startup the system landscape using  before the tests are executed and shut it down again afterwards .  Try it out with no containers up and running:  $ ./test-all.sh start stop  Fractions of the output will look like:  Start: Sun Aug 28 10:19:07 CEST 2016 Restarting the test environment... $ docker-compose down $ docker-compose up -d  Wait for: localhost:8761... Wait for: localhost:8761/eureka/apps/config-server...  Get an OAuth Access Token: ACCESS TOKEN: fabb3934-3d5e-42aa-b17e-f7ba1688f32e  Call API with Access Token... Wait for API: ... Is the API awake? $ curl -ks https://localhost:443/api/product/123 -H "Authorization: Bearer $TOKEN" | jq . Ok  We are done, stopping the test environment... $ docker-compose down End: Sun Aug 28 10:20:43 CEST 2016  Afterwards all containers are removed, i.e. the test environment only existed during the execution of the automated end-to-end tests. Very useful, for example, in a CI/CD solution!   Ok, so we are back on track with updated versions of Docker, Spring Boot and Spring Cloud!  All the previously described functionality in earlier [blog posts] still works and we have added a fully automated end-to-end test script. We have also seen some new components in action like the Spring Cloud Configuration Server and Spring Cloud Sleuth. Actually Spring Cloud Stream is also new, but much harder to observe.    In the next blog posts in the [Blog Series - Building Microservices] we will cover the newly added components, Spring Cloud Configuration Server and Spring Cloud Sleuth. Eventually we will also cover the ELK stack that I promised a looong time ago, stay tuned...
 In [part 1] toolchain can help us author components and ease integration in some of today's popular frameworks.  -[readmore]-   * This line will be replaced with the      As we could see in our simple accordion example in [part 1], writing components using vanilla js and native DOM APIs is not very efficient. For example, we had to manually register and de-register event handlers and make sure changes to attributes and state are reflected in the components DOM representation. We have none of the things we nowadays take for granted using a framework. I've chosen to take a look at stencil.js that originates from the team behind ionic. As they themselves describes it:  > Stencil is a toolchain for building reusable, scalable Design Systems. Generate small, blazing fast, and 100% standards based Web Components that run in every browser.  By using popular concepts from leading frameworks and libraries they promise a much nicer developer experience with features like Virtual DOM, Reactive data-binding, Typescript, JSX , but you can of course use it to build a smaller scale component library. An important feature is that the output of the stencil compiler is plain javascript with no runtime dependencies and therefore a very small footprint.  Like other popular libraries stencil has a CLI for scaffolding your projects. Simply run:   and choose "component" for a simple starter project type. By default, some files for a sample component "my-component" is generated that shows the basic features.    You will get a number of files generated:  - my-component.css  Here you define you styles. It's very easy to add support for scss if you prefer that. - my-component.tsx  The actual component with its JSX and code.  By default you also get unit and e2e test skeletons that you can update as you progress:  - my-component.e2e.ts - my-component.spec.ts  For comparison I've chosen to implement the acme-accordion component that we developed earlier using vanilla js. Here's how a corresponding implementation could look like using stencil.js:   That's a lot less code compared to the native version and in my opinion a lot easier to read!  Like some other frameworks, Stencil uses decorators to abstract boilerplate code and JSX to define the template. If you have some familiarity with modern js frameworks you probably already recognize and understand most of what's going on here:  The  is used to define some metadata about the component, its tag name, which style file to use and whether it should render using shadow DOM or not.  The  class also has some helping decorators.  that tells us this is an attribute that the component depends on for its logic. Any changes to the attribute ` array in the native version.   which simply denotes an internal state  that we want to use in the component - in this example we want to track if the accordion is open or not. As for props, changes to a state variable will trigger re-rendering.  The  function itself should be very familiar to any react developer as it's using JSX syntax.  During the build, Stencil's compiler will transform this into plain js that has no external dependencies. The resulting component library module can be published to a repository like NPM and easily added as any other dependency in your app.   To evaluate stencil.js I wanted to create a component at bit more fun and complex than the accordion component.  The imagined scenario is that ACME Inc has an shopping site network where different parts of the sites are developed using different tech-stacks . To keep things consistent and to avoid having to create and maintain different implementations of a frequently used "product view" component, we will create a custom component for this and then use it across the sites. The data model for an ACME product is something like this:  - pid  - name - desc - imageSrc  - price - badge   Beside presentation, the component should also have a "Add to cart" button. Clicking it should emit a "addToCart" event with the product's pid as payload. The react/vue/angular components using the product view component can then listen this event to update a shopping cart or similar.  This is what the design team at ACME Inc came up with:  ![Screenshot of acme-product-card]  Using stencil's  command, we add the new "acme-product-card" component and implement it.   It has properties matching the product data model and also uses another stencil decorator  which enables us to conveniently emit events from our component.  As you can see in the example, we define a separate  for *each attribute* we want to use as input. If we had even more input data it might be cleaner to have *one* custom attribute matching the actual product model, but as HTML attributes only support basic primitive types like strings and numbers, we can't do that.   Instead, you could use getter/setters or the  decorator to allow imperative manipulation of a element's properties and behaviour, but that somewhat complicates the usage as you would need to invoke methods on the component instance itself:   Google has listed some [best practices] on how to deal with these considerations.  Frameworks like Vue and Angular  *can* set values by their corresponding property instead of its attribute, but for the sake of this example I'm keeping it simple and use standard primitive HTML attributes in the component.  Ok, so now we have a  small component library which we now publish to NPM so that we can import it into our sample projects. The following examples have all been created using each framework's "hello-world" starter instructions and a dependency to the  lib has been added to package.json.   Some setup is required for our custom component library to work. A suitable place to do this in a Vue app is in  First we import the bootstrapping init functions that Stencil has created for us:   Then we need to tell Vue to ignore our custom tags .   and finally, register our components.  The generated  function will take care of registration of *all* included components, so there's no more setup to to even if the library contained many components. The `applyPolyfills  After this, the app can use the custom component like other native elements. To keep things simple, we'll use some static mocked product data for a couple of products:     In any Vue component in the app we can now render the products with our component using standard Vue templating and binding syntax:   To verify that the event handling works as intended - all sample applications stores and renders a message for each *addToCart* event received.  ![Screenshot of vue-acme-product-card]  *Rendered output after clicking each of the "Add to Cart" buttons. [View] full source.*  The setup in Angular is very similar, the first requirement is that we need to inform angular that we're using custom tag names  in our angular templates in    Then in the apps  we register the components:   Then we're free to start using it in our angular component.    The template binding syntax in angular is not that intuitive in my opinion, and I also had to use the attr-prefix so that protractor locators could find the non-standard attributes when writing tests.  Again, very similar setup as for the other frameworks - we register the custom components in the apps :  Because React implements its own synthetic event system, it  cannot listen for DOM events coming from custom elements without the use of a workaround. One such workaround is to manually add/remove an event listener for the custom event:    If you made it this far you should have a better understanding of what Web Components are, and what you can do with them. To finish off this post, let's revisit our initial questions.  Given today's fragmented frontend landscape, who would not like to have a standard compliant, future-proof, well tested and dependency free library of custom elements that you can simply reuse when switching between frameworks and projects?  With major browser adoption finally a reality - I would say they are more relevant than ever!  Hopefully this post has given you some insights into how you can create and use them, either by using native DOM APIs or by leveraging the benefits of a tool like Stencil. There are of course several other alternatives, like Polymer's [LitElement].  Even Vue, Angular and React themselves have plugins that enables you to wrap *their* own component implementations and expose them as standard web components - kind of the inverse of what we did here. A big drawback with that approach is that you then have a dependency to that framework's runtime when you use the components in another context.  Web Components won't *replace* other frameworks, but rather complement them. As I've demonstrated here, the integration into various frameworks still have some quirks but is pretty straight forward. Developing a high-quality component library or perhaps even a full design system  certainly is a complex and expensive task and it's not for everyone.  Investing in Web Components would likely pay off if you have a heterogeneous tech stack and need an efficient way to achieve more consistency and reuse in your frontend apps. It might be a good idea to start small, implementing some simple, but highly reusable "dumb" components to encapsulate branding, style or layouts to evaluate it before going all in. It's also highly recommended that you use a tool like Stencil to develop components even though it's always good to have a grasp of the underlying platform first.  All source code in this post can be found in the [stencil-blog-demo] github repository.  Some recommended resources I found useful when writing this post:  - https://stenciljs.com/ - https://www.webcomponents.org/ - https://developer.mozilla.org/en-US/docs/Web/Web_Components - https://custom-elements-everywhere.com/ - https://developers.google.com/web/fundamentals/web-components - https://caniuse.com/custom-elementsv1 
 In this blog series we'll build Microservices using the [Go] landscape.  If you're unsure what a microservice is, I suggest reading Martin Fowler's [article] explains the key concepts really well.  This blog series won't be a beginner's guide to coding in Go, though we will nevertheless write some code as we progress through the series and I'll explain some key Go concepts along the way. We'll be looking at a lot of code especially during the first parts where we'll cover basic functionality, unit testing and other core topics.  Part one will be an introduction to key concepts and the rationale for exploring the possibilities with Go-based microservices.  _Note: When referring to "Docker Swarm" in this blog series, I am referring to running Docker 1.12 or later in [swarm mode]" as a standalone concept was discontinued with the release of Docker 1.12._  - Part 1: Introduction and rationale for Go microservices  - [Part 2: Building our first Go microservice] - [Part 3: Embedding a data store and serving JSON] - [Part 4: Unit testing HTTP services with GoConvey] - [Part 5: Deploying on Docker Swarm] - [Part 6: Adding Health checks] - [Part 7: Service Discovery & Load-balancing] - [Part 8: Centralized configuration using Spring Cloud config and Viper] - [Part 9: Messaging with AMQP] - [Part 10: Logging to a LaaS with Logrus and Docker's log drivers] - [Part 11: Circuit Breakers and resilience with Netflix Hystrix] - [Part 12: Distributed tracing with Zipkin] - [Part 13: Distributed persistence with CockroachDB and GORM] - [Part 14: GraphQL with Go] - [Part 15: Monitoring with Prometheus and Grafana] - [Part 16: It's 2019, time for a rewrite!] - _More to come..._  The image below provides an overall view of the system landscape we'll be building throughout this blog series. However, we'll start by writing our first Go microservice from scratch and then as we progress along the parts of the blog series, we'll get closer and closer to what the image below represents.  ![landscape overview] The legend is basically:  - The dashed white box: A logical Docker Swarm cluster, running on one or more nodes. - Blue boxes: Supporting service from the Spring Cloud / Netflix OSS stack or some other service such as Zipkin. - Sand-colored / white box: An actual microservice.  It's more or less the same landscape used in [Magnus Larssons microservices blog series], with the main difference being that the actual microservices are implemented in Go instead of Java. The _quotes-service_ is the exception as it provides us with a JVM-based microservice we can use for comparison as well as a testbed for seamless integration with our Go-based services.  Why would we want to write microservices in Go, one might ask? Besides being a quite fun and productive language to work with, the main rationale for building microservices in Go is the tiny memory footprint Go programs comes with. Let's take a look at the screenshot below where we are running several Go microservices as well as a Microservice based on [Spring Boot] and Spring Cloud infrastructure on Docker Swarm:  ![Memory footprint]  The _quotes-service_ is the Sprint Boot one while the _compservice_ and _accountservice_ ones are Go-based. Both are basically HTTP servers with a lot of libraries deployed to handle integration with the Spring Cloud infrastructure.  Does this really matter in 2017? Arn't we deploying on servers these days with many gigabytes of RAM that easily fits an impressive number of let's say Java-based applications in memory? That's true - but a large enterprise isn't running tens of services - they could very well be running hundreds or even thousands of containerized services on a cloud provider. When running a huge amount of containers, being resource-efficient can save your company a lot of money over time.  Let's take a look at Amazon EC2 pricing for general purpose on-demand instances :  ![pricing]  Comparing the various t2 instances, we see that for a given CPU core count, doubling the amount of RAM  also _doubles_ the hourly rate. If you're not CPU-constrained, being able to fit twice the amount of microservices into a given instance could theoretically halve your cloud provider bill. As we'll see in later blog posts, even when under load our Go services use a _lot_ less RAM than an idling Spring Boot-based one.  This blog series is not just about how to build a microservice using Go - it's just as much about having it behave nicely within a Spring Cloud environment and conform to the qualities a production-ready microservice landscape will require of it.  Consider :  - Centralized configuration - Service Discovery - Logging - Distributed Tracing - Circuit Breaking - Load balancing - Edge - Monitoring - Security  All of these are things I think you must take into account when deciding to go for a microservice architecture regardless if you're going to code it in Go, Java, js, python, C# or whatever's your liking. In this blog series I'll try to cover all these topics from the Go perspective.  Another perspective are things _within_ your actual microservice implementation. Regardless of where you're coming from, you probably have worked with libraries that provides things such as:  - HTTP / RPC / REST / SOAP / Whatever APIs - Persistence APIs  - Messaging APIs  - Testability  - Build tools / CI / CD - More...  I won't touch on all of these topics. If I would, I could just as well write a book instead of a blog series. I'll cover at least a few of them.   A basic premise of the system landscape of this blog series is that Docker Swarm will be our runtime environment which means all services - be it the supporting ones . When we're at the end of the blog series, the following Docker command:  docker service ls  Will show us a list of all services deployed in the sample landscape, each having one replica.  ![Service list]  Again - please note that the services listed above includes a lot more services than we'll have when we'll setup up our Swarm cluster in [Part 5] of the blog series.   Ok - so Go microservices has a small memory footprint - but will they perform? Benchmarking programming languages against each other in a meaningful way can be quite difficult. That said, if one looks at a site such as [Benchmarkgame] where people can submit implementations of explicit algorithms for a variety of languages and have them benchmarked against each other, Go is typically slightly faster than Java 8 with a few notable exceptions. Go in it's turn, is typically almost on par with C++ or - in the case of a few benchmarks - a lot slower. That said - Go typically performs just fine for typical "microservice" workloads - serving HTTP/RPC, serializing/deserializing data structures, handling network IO etc.  Another rather important attribute of Go is that it is a garbage collected language. After the major rewrite of the Garbage Collector for Go 1.5 GC pauses should typically be a few milliseconds at most. If you're coming from the world of JVMs  you can tweak regarding GC behaviour in Go which controls the total size of the heap relative to the size of reachable objects.  However - keeping track of performance impact as we'll build our first microservice and then add things like circuit breakers, tracing, logging etc. to it can be very interesting so we'll use a [Gatling] test in upcoming blog posts to see how performance develops as we add more and more functionality to the microservices.  Another nice characteristic of your typical Go application is that it starts _really_ fast. A simple HTTP server with a bit of routing, JSON serialization etc. typically starts in a few hundred milliseconds at the most. When we start running our Go microservices within Docker containers, we'll see them healthy and ready to serve in a few seconds at most, while our reference Spring Boot-based microservice typically needs at least 10 seconds until ready. Perhaps not the singularly most important characteristic, although it can certainly be beneficial when your environment needs to handle unexpected surges in traffic volumes by quickly scaling up.   Another big upside with Go-based microservices in Docker containers is that we get a [statically linked] that weighs in at just ~6 mb.  FROM iron/base  EXPOSE 6868 ADD eventservice-linux-amd64 / ENTRYPOINT ["./eventservice-linux-amd64", "-profile=test"]  In other words - no JVM or other runtime component is required except for the standard C library  which is included in the base image.  We'll go into more detail about how to build our binaries and that _-profile=test_ thing in later blog posts.   In this blog post, we introduced some of the key reasons for building microservices using Go such as small memory footprint, good performance and the convenience of statically linked binaries.  In the [next part], we'll build our first Go-based microservice.
 How should one approach testing of microservices? Are there any unique challenges one needs to take into account when establishing a testing strategy for this particular domain? In part 4 of this blog series, we will take a look at this topic.  - The subject of testing microservices in the unit context - Write unit tests in the BDD-style of [GoConvey] - Introduce a mocking technique  Since this part won't change the core service in any way, no benchmarks this time.   First of all, one should keep the principles of the [testing pyramid] in mind.  ![pyramid]  Unit tests should form the bulk of your tests as integration-, e2e-, system- and acceptance tests are increasingly expensive to develop and maintain.  Secondly - microservices definitely offers some unique testing challenges and part of those is just as much about using sound principles when establishing a software architecture for your service implementations as the actual tests. That said - I think many of the microservice-specifics are beyond the realm of traditional unit tests which is what we're be going to deal with in this part of the blog series.  Anyway, a few bullets I'd like to stress:  - Unit test as usual - there's nothing magic with your business logic, converters, validators etc. just because they're running in the context of a microservice. - Integration components such as clients for communicating with other services, sending messages, accessing databases etc. should be designed with dependency injection and mockability taken into account. - A lot of the microservice specifics - accessing configuration, talking to other services, resilience testing etc. can be quite difficult to unit-test without spending ridiculous amounts of time writing mocks for a rather small value. Save those kind of tests to integration-like tests where you actually boot dependent services as Docker containers in your test code. It'll provide greater value and will probably be easier to get up and running as well.  As before, you may checkout the appropriate branch from the cloned repository to get the completed source of this part up front:  git checkout P4  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  Unit testing in Go follows some idiomatic patterns established by the Go authors. Test source files are identified by naming conventions. If we, for example, want to test things in our _handlers.go_ file, we create the file _handlers_test.go_ in the same directory. So let's do that.  We'll start with a sad path test that asserts that we get a HTTP 404 if we request an unknown path:  package service  import ( . "github.com/smartystreets/goconvey/convey" "testing" "net/http/httptest" )  func TestGetAccountWrongPath {  Convey { req := httptest.NewRequest resp := httptest.NewRecorder  Convey { NewRouter  Convey { So }) }) }) }  This test shows the "Given-When-Then" Behaviour-driven structure of GoConvey and also the "So A ShouldEqual B" assertion style. It also introduces usage of the httptest package where we use it to declare a request object as well as a response object we can perform asserts on in a convenient manner.  Run it by moving to the root "accountservice" folder and type:  > go test ./... ? github.com/callistaenterprise/goblog/accountservice [no test files] ? github.com/callistaenterprise/goblog/accountservice/dbclient [no test files] ? github.com/callistaenterprise/goblog/accountservice/model [no test files] ok github.com/callistaenterprise/goblog/accountservice/service 0.012s  Wonder about _./..._? It's us telling go test to run all tests in the current folder _and_ all subfolders. We could also go into the _/service_ folder and type _go test_ which then would only execute tests within that folder.  Since the "service" package is the only one with test files in it the other packages report that there are no tests there. That's fine, at least for now!  The test we created above doesn't need to mock anything since the actual call won't reach our _GetAccount_ func that relies on the DBClient we created in [part 3] package.  In the _/dbclient_ folder, create a new file called _mockclient.go_ that will be an implementation of our [IBoltClient] interface.  package dbclient  import ( "github.com/stretchr/testify/mock" "github.com/callistaenterprise/goblog/accountservice/model" )  // MockBoltClient is a mock implementation of a datastore client for testing purposes. // Instead of the bolt.DB pointer, we're just putting a generic mock object from // strechr/testify type MockBoltClient struct { mock.Mock }  // From here, we'll declare three functions that makes our MockBoltClient fulfill the interface IBoltClient that we declared in part 3. func  { args := m.Mock.Called return args.Get }  func  { // Does nothing }  func  { // Does nothing }  MockBoltClient can now function as our explicitly tailored programmable mock. As stated above, this code implicitly implements the IBoltClient interface since the _MockBoltClient_ struct has functions attached that matches the signature of all functions declared in the IBoltClient interface.  If you dislike writing boilerplate code for your mocks, I recommend taking a look at [Mockery] which can generate mocks for any Go interface.  The body of the QueryAccount function may seem a bit weird, but it is simply how strechr/testify provides us with a programmable mock where we have full control of its internal mechanics.   Let's create another test function in _handlers_test.go_:  func TestGetAccount { // Create a mock instance that implements the IBoltClient interface mockRepo := &dbclient.MockBoltClient  // Declare two mock behaviours. For "123" as input, return a proper Account struct and nil as error. // For "456" as input, return an empty Account object and a real error. mockRepo.On mockRepo.On  // Finally, assign mockRepo to the DBClient field  DBClient = mockRepo ... }  Next, replace the ... above with another GoConvey test:  Convey { req := httptest.NewRequest resp := httptest.NewRecorder  Convey { NewRouter  Convey { So  account := model.Account json.Unmarshal So So }) }) })  This test performs a request for the known path _/accounts/123_ which our mock knows about. In the "When" block, we assert HTTP status, unmarshal the returned Account struct and asserts that the fields match what we asked the mock to return.  What I like about GoConvey and the Given-When-Then way of writing tests is that they are really easy to read and have great structure.  We might as well add another sad path where we request _/accounts/456_ and assert that we get a HTTP 404 back:  Convey { req := httptest.NewRequest resp := httptest.NewRecorder  Convey { NewRouter  Convey { So }) }) })  Finish by running our tests again:  > go test ./... ? github.com/callistaenterprise/goblog/accountservice [no test files] ? github.com/callistaenterprise/goblog/accountservice/dbclient [no test files] ? github.com/callistaenterprise/goblog/accountservice/model [no test files] ok github.com/callistaenterprise/goblog/accountservice/service 0.026s  All green! GoConvey actually has an [interactive GUI] that can execute all tests everytime we save a file. I won't go into detail about it but looks like this and also provides stuff like automatic code coverage reports:  ![goconvey-goblog.png]  These GoConvey tests are unit tests though the BDD-style of writing them isn't everyone's cup of tea. There are many other testing frameworks for Golang, a quick search using your favourite search engine will probably yield many interesting options.  If we move up the [testing pyramid] and finally acceptance-style tests perhaps using something such as cucumber. That's out of scope for now but we can hopefully return to the topic of writing integration tests later on where we'll actually bootstrap a real BoltDB in our test code, perhaps by using the Go Docker Remote API and a pre-baked BoltDB image.  Another approach to integration testing is automating deployment of the dockerized microservice landscape. See for example the [blog post] I wrote last year where I use a little Go program to boot all microservices given a .yaml specification, including the support services and then performing a few HTTP calls to the services to make sure the deployment is sound.   In this part we wrote our first unit tests, using the 3rd party _GoConvey_ and _stretchr/testify/mock_ libraries to help us. We'll do more tests in later parts of the [blog series].  In the [next part], it's time to finally get Docker Swarm up and running and deploy the microservice we've been working on into the swarm.
 A common use case in a UI is to enter some data and submit it to the server. One way to do this in flex is to define an mxml like this with a Form  _Bild saknas_  To get some validation you add validators  _Bild saknas_  By doing this the user get visual assistance when entering data in the input fields.  Errors are reported by default in a tooltip and a red frame is drawn around the input field that it is errror.  Taking the phone input field there are some thing that I would like to change  1. The validator implementation requires 10 characters to be entered and this is not possible to change 2. The validator kicks in when the user leaves a field, I want the content to be validated each time it changes 3. The error messages should be in swedish instead of english 4. I want to be able to check the state of the field, is it valid or not? to enable/disable the submit button of the form 5. Lets go a little wild and crazy and bundle the input field and validator into one reusable component.  We define a new class in ActionScript that extends the .  _Bild saknas_  The constructor changes the error messages to swedish .  Looking at the source code of PhoneNumberValidator it seems like the length check is the last thing that is made. Assuming that the first error is the one that is displayed we can write a solution that will work with the current implemetation of  but needs to be revised when that changes. If the only reported error is the wrongLength error and the size is greater than 7 charachters, we reset the results varable.  Lets make the composite component , a new class that extends TextInput and contains a   Lets call it . I did that and the plan was to make a Validated....Input class for each type of content . After doing that I realized that where very few differences between the three. So I decided to refactor and create an abstract base class called ValidatedTextInput containing all the common behaviour.  _Bild saknas_  The variable validator has the validator that should be used to validate the input field. It must be supplied by subclasses by implementing the getValidator function.  The  variable is public . It is set everytime the contant is changed.   The constructor wires the validator together with the TextInput and adds a listener to the Change Event..  The listener function calls the validator and saves the result of the validation.  The implementation of  is now very simple  _Bild saknas_  Now it is possible to simplify the mxml file to  _Bild saknas_  No validators specified any more.  This version also contains the enabling/disabling of the submit button depending on if the input fields are valid or not.  _Bild saknas_  My conclusion is that creating reusable components in Flex makes a lot of sense and is pretty simple to do.
 We have a new fish in the pool! PacketBeat has joined the [elastic].  -[readmore]-  ![PacketBeat]   In [CADEC 2015] we had a talk on addressing the problem with IT organizations that don’t have enough insight in whats happening in their system landscape. Typical reasons for these problems are technical challenges when it comes to distributed applications, restricted access to environments and correlating events.  We introduced [monitoring logs in real time using the ELK stack] as an open source alternative for collecting, indexing, searching and analyzing log data.   Just recently the [elastic].  [PacketBeat].  Some of the capabilities is to - Capture network traffic between application servers - Decode application layer protocols  - Correlate requests and responses in network transactions  [PacketBeat] comes with - an agent to be installed on the servers - predefined [Kibana 4] views to help monitoring the network events  ![elk_packetbeat.jpg]   Other Beats implementations are discussed in the active community, for example FileBeat  and other upcoming Beats.  PacketBeat enables monitoring of the MySQL protocol, used between MySQL clients and MySQL servers, to get information like: - Number of errors - Methods used  - Reponse times - Throughput  This enables us to monitor and identify: - Queries taking long time - The amount of queries executed - The relation between reads and writes in the database  In [Kibana 4] the throughput can be visulaized like this:  ![mysql-throughput.png]   PacketBeat enables monitoring of the HTTP protocol to get information like: - HTTP codes - Methods used  - HTTP headers used - Client IP address  This enables us to monitor and identify: - Total number of web transactions - Trends on errors - Latency  In [Kibana 4] the number of web transactions can be visulized like this:  ![webtransactions-sum.png]  Or, why not visualize the number of transactions based on clients ip address and geographical location:  ![webtransations-geo.png]  We have been introduced to [PacketBeat] we can visualize the network metrics and events to get a good overview on what's happening in the system landscape. When writing this blog post PacketBeat was still in 1.0.0-Beta1.
 Over the years I have been looking for an open source based load test tool that I feel comfortable with.  I've been using either low level Linux commands  or I have been struggling with tools of which the graphical user interfaces are cumbersome to use and prevents me to see the big picture of my tests. I constantly have to look into a number of different dialogs to ensure that all pieces are setup correctly.  Recently, a colleague of mine demonstrated [Gatling] on a conference and I realized that this is what I have been looking for!  -[readmore]-  Gatling, built using [Scala].  In this blog I'll use Gatling to load test the REST service we developed in the [blog about Spring Boot]. But first we have to understand how to install it...  As mentioned above we will be reusing the source code from this [blog]. Ensure that you have Java SE 7 and Git installed and then check out the code with the following commands:   Gatling can be installed and configured by the following steps:  - Install Gatling by simply download [Gatling 1.5.3] and unzip it to a folder that we call  for now. - To configure Gatling you edit the configuration file  * For example set the parameter  to 10000.   If you are on a Linux or Unix system you will most likely hit the limit of the max number of open files that are allowed at the same time . This can be prevented by:  * In each command window where you start Gatling or the web app under test first run the command:   * If you are on OS X, you also need set the following parameters , e.g.:   To load test the REST service we need to:  * Build and start the web app * Setup the load test * Run the load test * Increase the load a bit...  The web app use Gradle as its build tool. Therefore the web app is built and started with the following command:    In the end it should result in something like:   You can verify that the web app works with the following command:   Gatling use a Scala based DSL where the core element is a scenario and it can be defined like:   The scenario  defines the following;  - A duration of the test - A HTTP GET request - HTTP headers to use in the request - A check to verify that the response is as expected - A pause interval between the requests  The scenario is launched using the  method like:   Where we also define:  - The number of concurrent users in the test - The ramp up time used to start up the users - General configuration of the HTTP protocol   For more detailed information of how you can set up a load test read the [Gatling Wiki].  For our test create a file called  in the folder  with the following content:   This setup will use a 20 second ramp up phase, then during one minute send requests from 1000 concurrent users. Each user will wait randomly one to three seconds between each request. The REST Service is asked to simulate a response time between 0.5 and 1 sec.  If you want to you can start a JMX tool, such as JConsole, to monitor the resource usage of the web app.  Then give the following command in the folder $GATLING_HOME/bin to start the load test:   The test will print out a log like:  Users : [#################################################################]100% waiting:0 / running:0 / done:1000  A test report will be available in the folder .  In the test report you can, for example, find graphs over the number of requests per second during the test:  ![]  ...and the response times during the test:  ![]  As you can see in the test reports the response times from the REST service are as expected, we asked it to respond in between 500 and 1000 ms. With 1000 users and some 350 reqs/sec it was not a problem to achieve.  That looks good but there is a problem, we are close to run into major problems. Since our REST service is implemented in a blocking style we will lock one thread per request. If we look at the JConsole output for this test we can see a potential problem ahead of us:  ![]  We are steadily consuming over 300 threads in this test. Tomcat by default only allow us to use 200 threads   Let's raise the number of concurrent users and simulate a slightly slower REST service . We will also run the test for a few more minutes. Change the load test script to the following values:   Rerun the test and you will very soon get an awful amount of timeout errors. The resulting test reports will report major problems  like:  ![]  ...and:  ![]  JConsole will reveal the issue:  ![]  The thread pool gets exhausted by the load  and a wait queue builds up until the 10 sec timeout is reached in the Gatling HTTP clients and errors are starting to be reported...  Theoretically we could continue to increase the thread pool but that is known for being both very costly and fragile so that is not the way to go!  Our Gating tests have clearly indicated that this REST service needs to be redesigned to be able to handle a large number of concurrent users.  Good for Gatling, not so good for our REST service :-)  We have seen Gatling in action with its compact and elegant DSL where you can understand a load test script by just reading the script, without looking at the documentation first!  It also provides us with very useful test reports  where we can see important characteristics of the test results such as requests/sec and response times.  We also used Gatling to point out a sever scalability issue with the REST service that we used in the test.  In an upcoming blog we will address this scalability issue by replacing the blocking design used in the REST service with a non-blocking design.  ...and of cause use Gatling to prove that the non-blocking design provides an improved scalability!  Stay tuned...
 Y3 intro...  -[readmore]-   This part is mainly about mainly about building cooperating microservices using Spring Boot and running them as containers using Docker Compose. It covers:  1. Create skeleton code for a microservice using Spring Initializr and its CLI tool `spring init`. A sample command to create skeleton code for a microservice named  looks like:   spring init \ --boot-version=2.1.0 \ --build=gradle \ --java-version=1.8 \ --packaging=jar \ --name=product-service \ --package-name=se.magnus.microservices.core.product \ --groupId=se.magnus.microservices.core.product \ --dependencies=actuator,webflux \ --version=1.0.0-SNAPSHOT \ product-service   2. Making the microservices reactive by developing: 1. *non blocking* synchronous REST APIs with WebFlux and *non blocking* API clients using the reactive WebClient. and 2. *event driven* asynchronous services using Spring Cloud Stream, together with both RabbitMQ and Kafka.  This is illustrated by the following figure:  ![reactive-1]   3. Documenting the APIs based on Swagger using SpringFox:  ![reactive-1]  4. Making the microservices storing its data persistent using Spring Data together with both a no SQL database, MongoDB, and a traditional relational database, MySQL.  <img src="/assets/blogg/build-microservices-part-9/ch6-layers.png" width="600">  5. Running the microservices as Docker containers using Docker COpose:  <img src="/assets/blogg/build-microservices-part-9/ch4.4-output-12.png" width="600">  Each area also covers how to write proper tests that cover unit testing, integration testing and end-to-end tests. The end-to-end tests verifies that the cooperating microservices not only works on their own but also together.   This part of the book focus on how to use Spring Cloud to handle the challenges with a system landscape of cooperating microservices.  In the middle of writing this book Spring Cloud 2.1 was released, placing many of my favorite Netflix OSS components that Spring Cloud initially was based on in maintenance mode, i.e. Hystrix, Zuul and Ribbon. The book is based on the suggested replacements:  In January 2019 Spring Cloud 2.1 was released, placing many of my favorite Netflix OSS components that Spring Cloud initially was based on in maintenance mode, i.e. Hystrix, Zuul and Ribbon.  Suggested replacements:  | Netflix OSS component | Replacement | | --------------------------- | --------------------------------- | | Zuul | Spring Cloud Gateway | | Ribbon | Spring Cloud LoadBalancer | | Hystrix | Resilience4J | | Hystrix Dashboard / Turbine | Micrometer + Prometheus + Grafana |  For details, see <https://spring.io/blog/2019/01/23/spring-cloud-greenwich-release-is-now-available>.  The following areas of Spring Cloud are covered in this part of the book:  1. Using Netflix Eureka for service discovery  **TODO**...  ![eureka-1]  1 Using Spring CLoud Gateway as an edge server  ![gateway-1]  1. Securing the APIs with OAuth and OIDC with Spring Security  The chapter learn both how to: 1. use a local authorization server for development and tests based on Spring Security:  ![oidc-1]  2. and also how to configure the microservices to use an OIDC provider, Auth0.  <img src="/assets/blogg/build-microservices-part-9/ch11-7-auth0-implicit-login.png" width="400">  **TODO**: The chapter also demonstrates how to use the various grant flows that Auth0 supports, both the common code grant and implicit grant flows but also the client credentials grant flow that is very useful when running automated tests for acquiring an access token without involving an end user  1. Using Spring Cloud Config server for centralized configuration   spring.rabbitmq.password: '17fcf0ae5b8c5cf87de6875b699be4a1746dd493a99d926c7a26a68c42 2117ef'   1. Making the microservices resilient using  cb, timeout, retry, (overload bulkhead?  <img src="/assets/blogg/build-microservices-part-9/ch8-7-circuit-breaker.png" width="300">   1. Using Spring Cloud SLeuth and Zipkin for distributed tracing  An example based on both synchronous API calls and asynchronous sending of events:  ![zipkin-1]  In the end of this chapter we have added many supporting services to the four microservices:  <img src="/assets/blogg/build-microservices-part-9/ch14.landscape.png" width="400">  We actually have more supporting services . In the next part of the book this is handled...  Let's see how CO & SM can provide a platform for executing microservices where we don't need to package all the supporting services ourself!   Ths focus on this part of the book is how to run a set of cooperating ms in production where you typically need a cluster of servers that run Docker for both scalability and high availability reasons. To manage and monitor microservices running in containers on these server you need a container orchestrator.  <img src="/assets/blogg/build-microservices-part-9/p1.2.7-pattern-controller-manager.png" width="400">  The most popular CO is K8S.  ![k8s-1]  Recently the concept of a SM has been introduced providing improved capabilities for observability, security, routing et al. One of the most popular SM product is IStio.   Istio comes bundled with:  * Kiali that brings observability to a new level * Jaeger used for distributed tracing * Prometheus for collecting and storing time series based metrics * Grafana used for visualizing metrics and also for setting up alarms on these metrics  ![istio-1]  My favorites when all work together Observability using Istio/Kiali     kustomize, deploy to dev,test,qa and prod...   **TODO**: Describe picture!!  ![x]  Simplifies the deployment, less component to deploy, configure and run as containers...  Reduces the need for Gateway, Eureka, Config and Zipkin. Replaced by... + Cert Manager  | Spring Cloud component | Replacement when using Kubernetes & Istio | | --------------- | ----------- | | Spring Cloud Gateway | Kubernetes Ingress or Istio Ingress Gateway | | Netflix Eureka | Kubernetes Service and kube-proxy | | Spring Cloud Config Server | Kubernetes Config Maps and Secrets | | Zipkin | Jaeger, bundled in Istio |  Cert Manager is also described...  Distributed Tracing of sync/asynch processing using Jaeger similar to Zipkin...  ![jaeger]  Picture#3Picture#4  ![x] ![x]   e.g. on circuit breaker using Prometheus/Grafana  Istio Mesh Dashboard:  ![istio mesh]  JVM Dashboard  ![JVM Dashboard]  Dashboard for retries and CB:  ![retry+cb]  Alert Dashboard:  ![alert]  Alerts email:  <img src="/assets/blogg/build-microservices-part-9/ch20.9.16.alert-mail.png" width="400">  If you are interested, the book is available at [Packt web site].  Happy Reading!     For more blog posts on new ..., see the blog series - [building microservices]. 
 I'm an enterprise architect. My work is to define architectures that span systems, organizations and sometimes countries. Not because architectures become better if they do. No, because business-, enterprise- and pan-european integration projects depend on an agreed abstraction , but a reference architecture that gives structure to a very small set of significant, structural problems of the domain. It is a bit like picking the most appropriate class hierarchy for a domain model. What ever abstraction you chose, you win something and sacrifice something else. But you can't just start solving the first little problem that crosses your way and then the next and then the next when working at that scale.  Well, if you don't have access to people who have been designing solutions in the domain for a long time you may have to find ways of building that experience in a high velocity using agile principles. It will likely be extremely costly for the customer though. "Refactoring" is usually not possible at the enterprise level. What you crank out will be an obstacle for evolution of the business for half a man-age or so. Since there are so many factors that influence the outcome of an enterprise architecture, I think one shouldn't bother about it unless the possibility to harvest relevant abstractions from many, many years of domain experience is at hand.  For me, my first coding dojo turned out to mirror the same process, but at the speed of light. Three days rather than thirty years. For an enterprise architect, three days equals the speed of light. A typical feed-back cycle is 2 years. Because the problem  and started the ever ongoing test-a-little, code-a-little, refactor-a-little-cycle. During the two-hours with the fizz-buzz kata we collectively solved the problem without any up-front design at all. The result was a solution hard-wired to the explicit requirements. Without access to people who have developed and maintained solutions for the domain over decades, we were on our own and the best we could do was to iterate our way to a shallow understanding of the domain. The solution didn't carry any promise for evolution of the business, but everyone could understand it.  After a coding dojo, you should be able to repeat the solution at home, on your own. But hey - I'm an architect. After this short exposure to a new domain, I didn't need to repeat anything. I felt ready for THE abstraction! So I chose among a set of possible dimensions. My choice fell on a rule-based structure with two properties: a list for additive rules and a then a fallback-rule. With this abstraction of a fizz-buzz kata, I had a path to a reference architecture that could be used to govern a family of fizz-buzz-like game projects.  Now that I have found the abstraction that will allow business people  to reason about all significant variation of their domain, I need to show that there is a viable, realization of this fizz-buzz pattern/abstraction that can be re-used across all projects that target a fizz-buzz-like game.  I decided to use the dynamic Groovy language to implement a higher-order language tailored for creating fizz-buzz-like solutions. The result? It had all the qualities of an abstraction made without access to experienced solution architects of the domain:  - The abstraction turned out to be irrelevant. There was no such thing as a fizz-buzz-"like" game - Solutions implemented using the framework perform badly - Solutions are so hard to maintain , that no one dared to maintain it. As a result, the fizz-buzz game hasn't evolved over the past 20 years.  Oh - the code? You find it [here] accompanied by a portable enterprise build script of a size that exceeds the code it builds.
 JAX-WS is the Java-standard for Web-Service XML to Java POJO binding. It entered the scene in Java EE 5 and Java SE 6. I wrote a [blog entry]. Thanks to the standard, JAX-WS-compliant service consumers and producers can be deployed into any Java EE 5 or Java SE 6 execution environment.   While JAX-WS and the associated JAX-B specifications standardize how WSDL and XSD-defined services are modeled by Java classes with annotations, they do not standardize the tooling for generating Java-classes. Thus, different JAX-WS implementations have different features in terms of how and to what detail the generation process can be controlled. As an example, the [CXF].   Recently, I set up a Maven project with CXF, to build a jar of JAXWS/JAXB POJOs from a WSDL with accompanying XML Schemas. The WSDL and the schemas was from different domains , so I decided to go for Metro in my Grails application.   Without giving it much thought, I started to configure Metros wsimport Maven plug-in  does not only allow me to deploy to different JAX-WS run-time libraries / Java EE-containers - it also gives me the option of choing one vendors wsdl-to-java tooling over anothers! Yes, yes - obvious, but it didn't strike me as a value until recently, I must admit. So from now on, CXF is my preferred WSDL-to-java tooling, regardless of JAX-WS implementation to be used at run-ime.  This is how CXF allows me to control name-space-to-package mappings :  **CXF**   In Metro, I can only control the package mapping for the target namespace of the WSDL, not for the schema name-spaces. The others will be mapped to package names by default rules :  **Metro** 
 The initial W3C Web Components specification draft was introduced way back in 2011. Every now and then over the years I've read articles and blog posts about the progress, but it's only recently that v1 of the spec has been adopted by the major browser vendors. In the meantime, popular frontend libraries and frameworks like React, Vue, and Angular have created their own separate ways of creating components. For me, this raises a few questions:  - Are Web Components still relevant? - How do you create them? - What's their place in today's ecosystem?  In this two part blog series I will try to answer these questions by creating sample components using different techniques and subsequently integrating them in some popular frameworks. First we will go through a quick rundown of some basic concepts before moving on to explore stencil.js in the second part of the series.  -[readmore]-   * This line will be replaced with the      Web Components are a set of new native platform features that lets you create your own HTML elements like  or . Like standard HTML elements they can have attributes, properties and methods, fire and respond to events, and even have encapsulated styles and DOM trees to bring along their own look and feel. Let's have a quick overview of the key parts before we continue.  The new element  enables you to write markup templates that are not displayed in the rendered page. Although it can be used for other purposes, its primary use would be to define the basis of a custom element's content and associated styles.   Within templates we also have the new  element that enables you to control where any children of the element are rendered within the template. You can even have multiple slots in a template, each targeted by a name. A simple example could look like this:   A custom component, e.g.  using that template would "pass through" arbitrary content to each of the slots when used in a page:  This can be a powerful feature when you want to compose or wrap other components!  This part consists of a set of javascript APIs for attaching an encapsulated *shadow* DOM tree to an element. A shadow DOM tree is rendered *separately* from the regular document DOM . This allows you to keep its features private, so they can be scripted and styled without the fear of colliding with other parts of the document.  This isolation means that you can safely use simple and descriptive id attributes, class names and css styles *within* your component - without worrying about clashes or leakage from the document DOM . If you inspect the DOM structure a custom component in dev tools you will see the shadow DOM boundary:  ![shadow-dom]  Cloning and appending template snippets could be useful on its own to compose parts of a layout, but for interactivity we need javascript. The Custom Component spec includes native APIs that allows you to implement and register a new class that extends an  using standard ES6 syntax.  After registration it can be used in page exactly like any standard element. The API is relatively simple and gives a component class some lifecycle callbacks.  | Name | Is Invoked when | |---|---| | constructor | An instance of the element is created or upgraded. Useful for initializing state, settings up event listeners, or creating Shadow DOM. See the spec for restrictions on what you can do in the constructor. | | connectedCallback | The element is inserted into the DOM. Useful for running setup code, such as fetching resources or rendering UI. Generally, you should try to delay work until this time | | attributeChangedCallback | Each time one of the custom element's attributes is added, removed, or changed. The actual effect of an attribute change is entirely up to you. | | disconnectedCallback | The element is removed from the DOM. Useful for running clean-up code. |    Components are divided into two types, with slightly different purposes:   These are new standalone elements that extends the base  and could be something like:   A new element must have a "-" in the name and can not be self-closing. The name requirement is there to guarantee that it will not clash with any existing or future W3C standard tags. The base  is indeed very basic, so you're really starting with an empty shell in an autonomous component.   This variant is used when you want to extend *existing* HTML elements, such as a button, p or input. By using the "is" attribute on the original tag element you instruct the browser to replace the standard element with your custom implementation, like this:   An important feature of extending a standard element like a  is that you keep all existing attributes, styles and behaviour of the original, native  as its implementation will extend .  Which type of element you choose of course depends on your use case, but if you are developing a component that semantically differs from any existing element, an autonomous component would probably be a better choice.  Let's say we have a site or app where we need to display accordion-type info  with a header and some content that is displayed when you click the header. It would be nice to be able to use some simple markup for this, maybe something like:   Let's create this component using just plain js, html and css. We start by defining a template in an html page:   NOTE: You don't *have* to use a template to define the contents of a component. You could create the content entirely within your class using native APIs like ,  or  etc.  Just a template is not enough since we want some expand/collapse behaviour. Let's define our component implementation :   Almost there, now we just need to register the component with a tag-name  so that the browser knows what to do when it encounters it:   Now our new element is ready be used in the body of the page and we have our first component!   ![Screenshot of accordion]  That's a **lot** of boilerplate code for something as simple as a toggle accordion widget! But remember that we're working with pure, native low-level DOM APIs here. No frameworks, libraries or external dependencies - just native javascript, html and css and it works out of the box in all major browsers.   It's somewhat refreshing to be able to build something that just runs in the browser - without having to install a few GBs worth of node dependencies and depending on a very complicated buildchain...  But at the same time it's hardly the best developer experience and arguably not a viable approach in a larger real-world use case. If we're building components for production use, we would surely want support for: - More efficient abstractions to write less and more concise code - Test environments - Packaging and distribution - Polyfills for legacy browser support   Fortunately for us there are several frameworks and tools to help us with that. One such tool is stencil.js that we'll explore in [part 2].
Vendors continuously try to retrofit the term Enterprise Service Bus  to the architecture of their offering. I like the definition contributed by Paul Fremantle at WSO2 in his article Reclaiming the ESB. It makes sense to define it as SOA infrastructure, based on a rather enterprisey requirement of canonical service interfaces. I would like to add - though - that most of the "EAI"-products rebranded into ESBs, are good tools for employing this architecture. At the same time, they are packaged for the swiss-army-knife definition EAI of an ESB.  I think it is utterly important to make a distinction between product and architecture. As an example, JBI is a standard for a category of products in the integration space, with a focus on service semantics .  You need all these kind of tools to create the end-to-end solution. Same goes for proprietary platforms with EAI heritage, like IBM WebSphere Message Broker. The architecture described in the article is considered "best practice" at many enterprises to day. Whether the same tool family is used for multiple parts of the architecture does not make the solution more or less SOA, or more or less implemented. Whether "the product" should include the ESB, rich adapter technology, service composition technology and service orchestration technology or just the ESB  is a completely different question.  Once you've defined your reference architecture, you'll find your self with a gap between the vision and the current system landscape. You need tooling to fill that gap. Typically, you need two completely different set of tools - one for a "pure SOA" vision, driven by BPM and another for "accept that the process is embedded into my ERP, so that I'm left with message transformation services with no value to my strategic BPM vision". In technical terms, the BPM vision heavily relies upon request/response web services, while the second one depends on traditional asynchronous messaging.  The architecture described in the article, applies to both, but it resolves into two chunks of platform requirements. JBI is unique, in that it intends to support both of them, based on a single interface model - that of Web Services. IBM WebSphere Message Broker does not support the BPM-targeting reference architecture. IBM targets the BPM vision with an alternative product stack centered around WebSphere Process Server. SAP NetWeaver has an integrated stack with explicit layers for both challenges .  In order to decide for SOA tooling, the architecture as described in the referenced article is a good starting-point for defining a reference architecture for SOA. But there are many more aspects that needs to be incorporated before a tooling strategi can safely be outlined. Most high-end products bundle technology covering ESB, adapter-solutions, orchestration and service composition. This is  however often without making the clear distinction between adapter and ESB.  This makes up a challenge for governance, but most of all for funding and skill allocation. The author of the article means that a product could help resolving these challenges. I don't. A solid reference architecture needs to be in place. The reference architecture must have impact on how projects are organized, funded and managed. This can only happen when architecture is rooted in the programme office. And that is most likely to happen when the reference architecture can be motivated in terms of positive impact on business challenges.
 I recently got the opportunity to do a spike on a RESTful web services interface and decided to use JAX-RS and the reference implementation Jersey to do the prototyping. The term REST was coined by [Roy Fielding]" will also give you a good insight in a RESTful architecture.  There are currently four implementations of JSR 311 that I am aware of; [Jersey].  JSR-311 defines an API that is a POJO-based and annotation driven like almost all new frameworks and APIs nowadays. The API assumes that the underlaying protocol is HTTP and it provides mappings between HTTP and URI elements and the API classes.  Using JAX-RS, web resources are implemented by resource classes and resource requests are handled by resource methods on the resource classes. A resource class is an ordinary Java class that is annotated with a  annotation or has one or more methods annotated with a  annotation. The resource methods are annotated with one of the request method designators; , , ,  or . A JAX-RS implementation instantiates a resource class instance for each web request to the resource. The resource method that is invoked depends on the HTTP request method . By default the last parameter in a parameter list of a method is bound to the content of the request body, the entity body. Conversion between a Java objects and an entity body is done by an entity provider.  The rest of this blog will present a very simple example. Like always things like error handling has been left out to make the example clean.  To get started download the Jersey distribution. Initially you will need the ,  and  files. Create a web project in your favorite IDE and put the jar files in the lib catalog of the project. Now you just have to enable Jersey in  as follows to get going:   Lets assume that we have a Customer class with the attributes id, name and address. Our customer objects are handled by a CustomerRepository object that is accessible from our resource class and we want to expose the Customer objects as web resources. If the resource class is deployed in a Java EE 5 compliant container the repository could be a injected Session Bean. We define a customer resource for our customer collection and sub-resources for the customer objects. The collection url is used to create new customer entities . On the sub-resources we allow GET to get a representation of the customer entity with a specific id, PUT to update the customer entity and DELETE to delete it. The following methods are allowed on the resources:  [http://host/resources/customer]  [http://host/resources/customer/]  Suppose that we have entity providers written that converts a customer object to and from a XML representation. I will not show how that is done, but you can find examples in the attached code. An entity provider does just take an Java object and a bunch of properties as input and spits out a representation on some format like XML, json, Atom, GIF or what ever.  Lets start by defining the resource class, anchor it at the relative URI  using the `@Path annotation and define the methods needed.   The first two methods handles the collections resource and the last three handles the sub-resources. The methods that produce an entity body are annotated with the mime type of the entity body produced. The last three methods have a  template that lets us bind the last part of the URL to the id parameter of the method using the  annotation. All methods but one returns an instance of . This is a special class that lets you build a response in a very nice way using the builder pattern. The first method returns a String and that will be properly converted to the entity body.  The first two methods will return links  that points to the sub-resources and we will use the  class to build those URI:s. An instance of the UriInfo class is bound by the JAX-RS implementation to the uriInfo parameter of the list and post methods. The UriInfo class also uses the builder pattern and is very easy to work with.   Here is the list method just illustrating a really easy case where we build the XML "by hand" using a StringBuilder. The links to sub-resources are created using the UriInfo instance by just adding the customer id to the current absolute path.   The most complicated part of the post method is done by the entity provider that converts the entity body of the POST request to a customer object. Since the customer object appears last in the parameter list the JAX-RS implementation will try to find a registered entity provider that is able to convert from the entity body mime type to a Customer object. The customer object is stored in our repository and we return a HTTP 201 Created status code and the URL to the newly created resource in the Location header.  The rest of the methods are more or less trivial:   The common part of the get, put and delete methods is that they bind the last part of the url to the parameter id. The JAX-RS handles the conversion from String to long.  I hope that this simple example has given you some ideas of what the JAX-RS specification is all about. The full example is provided for download at the end of this page. If you want to explore it further you can have a look at the Jersey distribution which have some nice examples.  I think that the JAX-RS makes it really easy to implement RESTful services compared to using the servlet API and URL/URI classes of the JDK. I have not tried to use [JAX-WS for RESTful webservices]. It enables you to execute all the HTTP methods.
 In the [previous blog post]. From the operations model we will cover the core parts: *service discovery*, *dynamic routing*, *load balancing* and to some extend an *edge server*, leaving the other parts to upcoming blog posts.  -[readmore]-  We will use some core components from Spring Cloud and Netflix OSS to allow separately deployed microservices to communicate with each other with no manual administration required, e.g. keeping track of what ports each microservice use or manual configuration of routing rules. To avoid problems with port conflicts, our microservices will dynamically allocate free ports from a port range at startup. To allow simple access to the microservices we will use an edge server that provides a well known entry point to the microservice landscape.  After a quick introduction of the Spring Cloud and Netflix OSS components we will present a system landscape that we will use throughout the blog series. We will go through how to access the source code and build it. We will also make a brief walkthrough of the source code pointing out the most important parts. Finally we wrap up by running through some test cases on how to access the services and also demonstrate how simple it is to bring up a new instance of a microservice and get the load balancer to start to use it, again without any manual configuration required.   Spring Cloud is a [new project] works.  The table below maps the generic components in the [operations model] to the actual components that we will use throughout this blog series:  <img src="https://callistaenterprise.se/assets/blogg/build-microservices-part-1/mapping-table.png" width="500" />  In this blog post we will cover *Eureka*, *Ribbon* and to some extent *Zuul*:  * **Netflix Eureka** - Service Discovery Server Netflix Eureka allows microservices to register themselves at runtime as they appear in the system landscape.  * **Netflix Ribbon** - Dynamic Routing and Load Balancer Netflix Ribbon can be used by service consumers to lookup services at runtime. Ribbon uses the information available in Eureka to locate appropriate service instances. If more than one instance is found, Ribbon will apply load balancing to spread the requests over the available instances. Ribbon does not run as a separate service but instead as an embedded component in each service consumer.  * **Netflix Zuul** - Edge Server Zuul is  to the outside world, not allowing any unauthorized external requests pass through. Zulu also provides a well known entry point to the microservices in the system landscape. Using dynamically allocated ports is convenient to avoid port conflicts and to minimize administration but it makes it of course harder for any given service consumer. Zuul uses Ribbon to lookup available services and routes the external request to an appropriate service instance. In this blog post we will only use Zuul to provide a well known entry point, leaving the security aspects for coming blog posts.  **Note:** The microservices that are allowed to be accessed externally through the edge server can be seen as an [API] to the system landscape.   To be able to test the components we need a system landscape to play with. For the scope of this blog post we have developed a landscape that looks like:  ![system-landscape]  It contains four business services :  * Three core services responsible for handling information regarding **products**, **recommendations** and **reviews**. * One composite service, **product-composite**, that can aggregate information from the three core services and compose a view of product information together with reviews and recommendations of a product.  To support the business services we use the following infrastructure services and components :  * **Service Discovery Server**   * **Dynamic Routing and Load Balancer**   * **Edge Server**   > To emphasize the differences between microservices and monolithic applications we will run each service in a separate microservice, i.e. in separate processes. In a large scale system landscape it will most probably be inconvenient with this type of fine grained microservices. Instead, a number of related services will probably be grouped in one and the same microservice to keep the number of microservices at a manageable level. But still without falling back into huge monolithic applications...   If you want to check out the source code and test it on your own you need to have Java SE 8 and Git installed. Then perform:   This will result in the following structure of components:  ![source-code]  Each component is built separately  as the build system, if you don't have Gradle installed the build file will download it for you. To simplify the process we have a small shell script that you can use to build the components:   > If you are on **Windows** you can execute the corresponding bat-file !  It should result in six log messages that all says:    Let's take a quick look at some key source code construct. Each microservice is developed as standalone [Spring Boot].  Instead let us focus on how to use the functionality in Spring Cloud and Netflix OSS!  **Note:** We have intentionally made the implementation as simple as possible to make the source code easy to grasp and understand.   In the spirit of Spring Boot, Spring Cloud has defined a set of starter dependencies making it very easy to bring in the dependencies you need for a specific feature. To use Eureka and Ribbon in a microservice to register and/or call other services simply add the following to the build file:   For a complete example see [product-service/build.gradle].  To be able to setup an Eureka server add the following dependency:   For a complete example see [discovery-server/build.gradle].   Setting up an infrastructure server based on Spring Cloud and Netflix OSS is really easy. E.g. for a Eureka server add the annotation  to a standard Spring Boot application:   For a complete example see [EurekaApplication.java].  To bring up a Zuul server you add a  - annotation instead. For a complete example see [ZuulApplication.java].  With these simple annotations you get a default server configurations that gets yo started. When needed, the default configurations can be overridden with specific settings. One example of overriding the default configuration is where we have limited what services that the edge server is allowed to route calls to. By default Zuul set up a route to every service it can find in Eureka. With the following configuration in the  - file we have limited the routes to only allow calls to the composite product service:   For a complete example see [edge-server/application.yml].   To auto register microservices with Eureka, add a  - annotation to the Spring Boot application.   For a complete example see [ProductServiceApplication.java].  To lookup and call an instance of a microservice, use Ribbon and for example a Spring RestTemplate like:   The service consumer only need to know about the name of the service  will find a service instance and return its URI to the service consumer.  For a complete example see [Util.java].   > In this blog post we will start the microservices as java processes in our local development environment. In followup blog posts we will describe how to deploy microservices to both cloud infrastructures and Docker containers!  To be able to run some of the commands used below you need to have the tools [cURL] installed.  Each microservice is started using the command `./gradlew bootRun`.  First start the infrastructure microservices, e.g.:   Once they are started up, launch the business microservices:   > If you are on **Windows** you can execute the corresponding bat-file !  Once the microservices are started up and registered with the service discovery server they should write the following in the log:   In the service discovery web app we should now be able to see our four business services and the edge server :  ![Eureka]  To find out more details about our services, e.g. what ip addresses and ports they use, we can use the Eureka REST API, e.g.:   We are now ready to try some test cases, first some happy days tests to verify that we can reach our services and then we will se how we can bring up a new instance of a microservice and get Ribbon to load balance requests over multiple instances of that service.  **Note:** In coming blog posts we will also try failure scenarios to demonstrate how a circuit breaker works.   Start to call the composite service through the edge server, The edge server is found on the port 8765  like:   ...if we are on the inside of the microservice landscape we can actually call the services directly without going through the edge server. The problem is of course that we don't know on what ports the services runs on since they are allocated dynamically. But if we look at the output from the call to the eureka rest api we can find out what ports they listen to. To call the composite service and then the three core services we should be able to use the following commands    Try it out in your own environment!   To avoid service outage due to a failing service or temporary network problems it is very common to have more than one service instance of the same type running and using a load balancer to spread the incoming calls over the instances. Since we are using dynamic allocated ports and a service discovery server it is very easy to add a new instance. For example simply start a new review service and it will allocate a new port dynamically and register itself to the service discovery server.   After a short while you can see the second instance in the service discovery web app :  ![eureka]  If you run the previous curl command  a couple of times and look into the logs of the two review instances you will see how the load balancer automatically spread in calls over the two instances without any manual configuration required:  ![Hystrix]   We have seen how Spring Cloud and Netflix OSS components can be used to simplify making separately deployed microservices to work together with no manual administration in terms of keeping track of what ports each microservice use or manual configuration of routing rules. When new instances are started they are automatically detected by the load balancer through the service discovery server and can start to receive requests. Using the edge server we can control what microservices that are exposed externally, establishing av API for the system landscape.   Ok, so happy days scenarios looks great!  But a lot of questions remains unanswered, e.g.:  * What about something goes wrong, like a failing microservice? * How do we prevent unauthorized access to our API ? * How do we get a good consolidated picture of what is going on in the microservice landscape, e.g. why isn't order #123456 delivered yet?  In upcoming blog posts in the [Blog Series - Building Microservices] we will look at how to increase resilience using a circuit breaker, use OAuth 2 to restrict external access. We will also look into how we can user the ELK stack to collect and present log entries from all the microservices in a centralized and consolidated way and more.  Stay tuned!
Do you want interoperability in the middleware arena without vendor lock-in?  Then AMQP can be something for you!  AMQP stands for Advanced Message Queuing Protocol and is a emerging protocol specification that intends to enable implementations from many sources and address the most common business requirements.  To enable complete interoperability for the messaging middleware, both the networking protocol and the semantics of the broker services are specified in AMQP. The AMQP protocol is a binary protocol . The services are of three main types, which are connected into processing chains in the server  - The _exchange_ receives messages from publisher applications and routes these to message queues. - The _message queue_ stores messages until they can be processed by a consuming client application. - The _binding_ defines the relationship between a message queue and an exchange.  Among the supporters you will find Cisco, IONA, Red Hat and JPMorganChase.  Qpid is the Apache project  on AMQP and delivers implementations in Java, C++, Python, C# and Ruby.  Try it out when you want to get out of the shadow from the big middleware market holders!
 In [Part 1], enabling separately deployed microservices to communicate with each other. In this blog post we will focus on fault handling in a microservice landscape, improving resilience using *Hystrix*, Netflix circuit breaker.  -[readmore]-  Now bad things will start to happen in the system landscape that we established in [Part 1]. Some of the core services that the composite service depends on will suddenly not respond, jeopardizing the composite service if faults are not handled correctly.  In general we call this type of problem a *chain of failures*, where an error in one component can cause errors to occur in other components that depend on the failing component. This needs special attention in a microservice based system landscape where, potentially a large number of, separately deployed microservices communicate with each other. One common solution to this problem is to apply a *circuit breaker* pattern, for details see the book [Release It!]. A *circuit breaker* typically applies state transitions like:  <img src="https://callistaenterprise.se/assets/blogg/build-microservices-part-2/circuit-breaker.png" width="600" />   Enter *Netflix Hystrix* and some powerful annotations provided by *Spring Cloud*!   From the table presented in [Part 1] we will cover: *Hystrix*, *Hystrix dashboard* and *Turbine*.  <img src="https://callistaenterprise.se/assets/blogg/build-microservices-part-1/mapping-table.png" width="500" />  * **Netflix Hystrix** - Circuit breaker Netflix Hystrix provides circuit breaker capabilities to a service consumer. If a service doesn't respond  on every subsequent call until the service is available again. To determine wether the service is available again Hystrix allow some requests to try out the service even if the circuit is open. Hystrix executes embedded within its service consumer.  * **Netflix Hystrix dashboard and Netflix Turbine** - Monitor Dashboard Hystrix dashboard can be used to provide a graphical overview of circuit breakers and Turbine can, based on information in Eureka, provide the dashboard with information from all circuit breakers in a system landscape. A sample screenshot from Hystrix dashboard and Turbine in action:  ![Hystrix]   The system landscape from [Part 1] is complemented with supporting infrastructure servers for *Hystrix dashboard* and *Turbine*. The service *product-composite* is also enhanced with a *Hystrix* based circuit breaker. The two new components are marked with a red line in the updated picture below:  ![system-landscape]  > As in [Part 1], we emphasize the differences between microservices and monolithic applications by running each service in a separate microservice, i.e. in separate processes.   As in [Part 1] we use Java SE 8, Git and Gradle. So, to access the source code and build it perform:   > If you are on **Windows** you can execute the corresponding bat-file !  Two new source code components have been added since [Part 1]: *monitor-dashboard* and *turbine*:  ![source-code]  The build should result in eight log messages that all says:    New from [Part 1] is the use of Hystrix as a circuit breaker in the microservice product-composite, so we will focus on the additional source code required to put the circuit breaker in place.   We now have a couple of Hystrix-based starter dependencies to drag into our build files. Since Hysteric use [RabbitMQ] to communicate between circuit breakers and dashboards we also need to setup dependencies for that as well.  For a service consumer, that want to use Hystrix as a circuit breaker, we need to add:   For a complete example see [product-composite-service/build.gradle].  To be able to setup an Turbine server add the following dependency:   For a complete example see [turbine/build.gradle].   Set up a Turbine server by adding the annotation  to a standard Spring Boot application:   For a complete example see [TurbineApplication.java].  To setup a Hystrix Dashboard add the annotation  instead. For a complete example see [HystrixDashboardApplication.java].  With these simple annotation you get a default server configuration that makes you get started. When needed, the default configurations can be overridden with specific settings.   To enable Hystrix, add a -annotation to your Spring Boot application. To actually put Hystrix in action, annotate the method that Hystrix shall monitor with  where we also can specify a fallback-method, e.g.:   The fallback method is used by Hystrix in case of an error .   > As in [Part 1] tools installed to be able to run some of the commands below.  As already mentioned, Hystrix use RabbitMQ for internal communication so we need to have it installed and running before we can start up our system landscape. Follow the instructions at [Downloading and Installing]. Then start RabbitMQ use the  program in the -folder of your installation.   > If you are on **Windows** use Windows Services to ensure that the RabbitMQ service is started!  We are now ready to start up the system landscape. Each microservice is started using the command `./gradlew bootRun`.  First start the infrastructure microservices, e.g.:   Once they are started up, launch the business microservices:   > If you are on **Windows** you can execute the corresponding bat-file !  Once the microservices are started up and registered with the service discovery server they should write the following in the log:   As in [Part 1]:  ![Eureka]  Finally ensure that the circuit breakers are operational, e.g. *closed*. Try a call to the composite service through the edge-server as in [Part 1] (shortened for brevity:   Go to the url [http://localhost:7979]:  ![Hystrix]  We can see that the composite service have three circuit breakers operational, one for each core service it depends on. They are all fine, i.e. *closed*. We are now ready to try out a negative test to see the circuit breaker in action!   Stop the  microservice and retry the command above:   The **review** - part of the response is now empty, but the rest of the reply remains intact! Look at the log of the  services and you will find warnings:   I.e. the circuit breaker has detected a problem with the  service and routed the caller to the fallback method in the service consumer. In this case we simply return null but we could, for example, return data from a local cache to provide a best effort result when the  service is unavailable.  The circuit is still closed since the error is not that frequent:  ![Hystrix]  Let's increase the error frequency over the limits where Hystrix will open the circuit and start to fast fail  for this:   Now the circuit will be opened:  ![Hystrix]  ...and subsequent calls will fast fail, i.e. the circuit breaker will redirect the caller directly to its fallback method without trying to get the reviews from the  service. The log will no longer contain a message that says :   However, from time to time it will let some calls pass through to see if they succeeds, i.e. to see if the  service is available again. We can see that by repeating the curl call a number of times and look in the log of the  service:   As we can see from the log output, every fifth call is allowed to try to call the  service .  Let's start the  service again and try calling the composite service!  **Note:** You might need to be a bit patient here  must be made aware of that a  service instance is available again before the call succeeds.  Now we can see that the response is ok, i.e. the review part is back in the response, and the circuit is closed again:  ![Hystrix]   We have seen how *Netflix Hystrix* can be used as a *circuit breaker* to efficiently handle the problem with *chain of failures*, i.e. where a failing microservice potentially can cause a system outage of a large part of a microservice landscape due to propagating errors. Thanks to the annotations and starter dependencies available in Spring Cloud it is very easy to get started with Hystrix in a Spring environment. Finally the dashboard capabilities provided by Hystrix dashboard and Turbine makes it possible to monitor a large number of circuit breakers in a system landscape.   In the next blog post in the [Blog Series - Building Microservices] we will look at how to use OAuth 2.0 to restrict access to microservices that expose an external API.  Stay tuned!
At the QCon conference, Dave Syer today gave an update on the Spring Batch project that is approaching the 1.0 release  are mundane and unsexy. Never the less we have to deal with them, over and over again.  This is an area that is ideal for framework support: Let the framework take care of the batch-related responsibilities, and allow an application developer to plug in reusable business logic into a batch execution scenario. At Callista, we have written several such project- and customer-specific frameworks over the years, since no suitable commercial or open-source alternatives have existed. This is clearly an area where the JavaEE platform is missing an important piece.  Spring Batch changes the scene. It is a light-weight framework for Batch Processing that blends very well with the rest of the Spring framework family: Business logic expressed as Spring beans can easily be reused in Batch scenarios, where the batch semantics is applied as decorators to the business logic, and where the Spring Batch framework provides well architected solutions to most of the recurring Batch challenges.  So, good news for everyone that has already committed to using the Spring DI container. If your business logic lives in EJB 3.0 beans, Spring Batch won't be of any help. Let's hope that the Spring Batch initiative influences the JavaEE 6 JCP work. It is badly needed!
 [RSocket] is a new communication protocol that promises to solve the issues we have with HTTP, and together with that, it might also simplify the way we design and build distributed systems and microservices. I will come back to that last statement in a later blog post.  -[readmore]-  [comment]: # [Reactive Streams]: http://www.reactive-streams.org/ [Reactive Principles]: https://www.reactivemanifesto.org/ [RSocket]: https://rsocket.io/ [Netifi]: https://www.netifi.com/ [Pivotal]: https://tanzu.vmware.com/pivotal [Lightbend]: https://www.lightbend.com/ [frame header]: http://rsocket.io/docs/Protocol.html#frame-header-format [RSocket specification]: https://rsocket.io/docs/Protocol.html [SETUP frame]: https://rsocket.io/docs/Protocol.html#setup-frame-0x01 [REQUEST_RESPONSE frame]: https://rsocket.io/docs/Protocol.html#request_response-frame-0x04 [PAYLOAD frame]: https://rsocket.io/docs/Protocol.html#payload-frame-0x0a [REQUEST_FNF frame]: https://rsocket.io/docs/Protocol.html#request_fnf-fire-n-forget-frame-0x05 [REQUEST_STREAM frame]: https://rsocket.io/docs/Protocol.html#request_stream-frame-0x06 [REQUEST_CHANNEL frame]: https://rsocket.io/docs/Protocol.html#request_channel-frame-0x07 [REQUEST_N frame]: https://rsocket.io/docs/Protocol.html#request_n-frame-0x08 [ERROR frame]: https://rsocket.io/docs/Protocol.html#error-frame-0x0b [comment]: #  <img src="/assets/blogg/rsocket-part-1/rsocket-logo.svg" height="60px">  This blog is the first in a series that covers RSocket, a new reactive communication protocol. I first read about RSocket in late 2019, and my first thought was that this protocol could revolutionize the way we build distributed systems and microservices. Since the Spring team at Pivotal has embraced it, I am sure that it is here to stay. The specification has not yet reached 1.0, but the Spring Framework includes the Java implementation since version 5.2. I recommend the [blogs] my colleague Anna has written on the subject if you are new to reactive programming.  When building modern distributed applications , we are faced with several challenges. One of them is how our services communicate and exchange information over the network. HTTP is probably the most widely used protocol both between services inside our data centers and to the outside. It has become a de-facto standard due to its superior interoperability.  The use of HTTP presents some problems, though. * It only supports the request/response interaction model. * It's inefficient. * It's not reactive  A modern application architecture often needs to support other communication patterns, like streaming and fire-and-forget. When that need arises, we often bring in a message broker to support those use-cases, even if we don't need the durability of messages.  HTTP is a text-based protocol whose primary usage is fetching documents over the Internet. Using this protocol in a data center is inefficient, especially the earlier versions 1.0 and 1.1. Why is performance relevant? Because inefficiency in memory, CPU, and network utilization are in today's systems often directly translated into Cloud costs.  And last but not least, HTTP is not reactive! There is no problem with using HTTP in a reactive context, but the protocol itself has no concepts of reactiveness.  [RSocket] is an open-source, binary encoded protocol designed by people that used to be at Netflix, helping develop [Reactive Streams] together with [Pivotal], [Lightbend] and others. Companies that are currently actively supporting the protocol includes [Netifi], [Pivotal], Facebook, and Alibaba, among others. Implementations exist in many different programming languages. RSocket is message-based and requires some lower-level transport protocol to carry the messages. The requirements put on the transport protocol are that it should be reliable, connection-oriented, and byte stream-based, so protocols like TCP, Websockets, and Aeron can be used. If the transport protocol does not have framing , then RSocket provides it.  Contrary to HTTP, RSocket is symmetric and operates on a single stateful and persistent _connection_ between two communicating peers. The peers can assume either the _client_ or the _server_ role, but that distinction is only relevant during connection establishment. The _client_ connects to the _server_, but both can act as _requester_ or _responder_ in further interactions. The requester is the party initiating a communication interaction, called _stream_ in the [RSocket specification]. For example, this means that the server can act requester and send requests to the client, where the client might be a Javascript application on a web page, and the server might be a back-end Java server.  RSocket defines four types of interaction models or streams.  1. Request / Response 2. Fire And Forget 3. Request / Stream 4. Channel  The specification defines the connection as "an instance of a transport session", and the protocol supports _session_ resumption. Session resumption allows for the recovery of long-lived streams across different connections and transport protocols. It is typically useful for mobile communication where network connections can be dropped and reconnected on another transport. Each stream exists for a finite period, and a Stream ID identifies it. The Stream ID is bound either by the lifetime of the connection or, if session resumption is in use, by the lifetime of the Session . Messages relating to the connection uses a Stream ID of 0.  Let's have a closer look at how RSocket maps the reactive streams concepts on the wire. To follow along in detail in every bit and byte, I recommend that you follow along with the [RSocket specification] open.   All frames start with a [frame header], which includes Stream ID, Frame Type, and flags. Two flags etadata are always present, but the others depend on the frame type.    The client has to connect to the server to set up a connection, and as soon as the connection is established, it sends a SETUP frame. Let's assume that the transport protocol is TCP, then the [SETUP frame] looks like this if resumption is not in use: TCP has no framing, so the first 24 bits . Finally, we have metadata and data encoding mime-types, which are text/plain for both in this example.  ![request response]<br> Request/response is probably still the most common interaction model, but in RSocket as well as in [Reactive Streams] semantics, this interaction model is just a special case of request/stream where the response stream only has one element or frame. The requester sends one request frame, and the responder replies with a stream of one frame.  The request frame carries the Stream ID and frame type . If the client initiates the stream ID, it is odd and starts with 1 for the first stream. The server uses even stream IDs beginning with 2. Here below is an example of a request/response interaction with an echo service using TCP. The client is the requester and sends a "Hello World!" message to the server that is the responder and echoes the message back. Similar to the SETUP frame above, framing is used, and the frame is 18 bytes , the frame type is 0x04 for [REQUEST_RESPONSE frame], and all flags are 0. You can see the response from the echo service below. The response frame is of the same length, but the type is now [PAYLOAD frame] with an identifier of 0x0A. Two flags, the _ on the subscriber on the requester side.  Notice that the connection does not terminate after the response message. The two peers are still connected and able to initiate new interactions or streams, possibly switching requester and responder roles.  ![fire and forget]<br> Fire and Forget is an optimized stream where the requester is not expecting any response. This type of interaction cannot be achieved by HTTP since HTTP, by default, has a response, and even if the requester ignores the response, it is sent and processed by both peers. One useful scenario for Fire and Forget streams could be logging. The frame type is now 0x05 [REQUEST_FNF frame]. The Stream ID is 0x00000003 since this is the second stream initiated by the client. Again all flags are set to 0.  ![request stream]<br> In a Request/Stream interaction, the requester sends one request, and the responder responds with a Stream of items. The stream can potentially be infinitely long. In the example here, the requester sends a "Hello World!" message and the responder echoes back the same message twice in a stream of two items. As can be seen, the stream ID is once again incremented by two to 0x00000005. The frame type is now 0x06 [REQUEST_STREAM frame], and all flags are 0. The frame also has a field for _demand_ or "Initial Request N" that signals how many items the requester can handle. A peer uses demand signaling to enable backpressure. In this particular case, the responder signals that it can receive  items, which is the maximal amount, so in effect, no backpressure is applied.  The requester receives the response in three frames. The response frames are of sizes 0x12, 0x12, and 0x06, and they all have a stream ID of 0x00000005. All three framed are of [PAYLOAD frame]s .  ![channel]<br> The final interaction pattern is the Channel stream. This interaction opens a bi-directional channel with two potentially infinite streams between the requester and the responder. The request frame from the responder is more or less identical to the initial frame sent in the Request/Stream case except for the frame type which is now 0x07 [REQUEST_CHANNEL frame]  Stream ID is 0x00000007, all the flags are set to 0, so frame type and flags become 0x1c00, and demand has the max value of 0x7fffffff, so in effect, no backpressure is applied. The responder sends back three frames with sizes 0x0a, 0x12, and 0x12. The last two are [PAYLOAD frame]s  is signaled from the responder as well. So in the Channel stream, both sides can signal demand and thereby enable backpressure. The requester continues the stream with a second "Hello World!" message and then it terminates the stream. The requester sends another payload frame with the .  The responder echoes this message twice. It sends two payload frames with "Hello World!" and , which terminates the stream from the responder.  In this post, I have shown how the reactive concept maps into the RSocket protocol. The callbacks _onNext_ and _onComplete_ of the Subscriber interface translates from the flags in the payload frame. I didn't show it, but there is also an [ERROR frame] , that translates to an _onError_ call in the subscriber. I have also shown how demand signaling between the peers results in backpressure. In the next blog post, I will show how you can use RSocket, with code examples in Java.
Over the past 18 months a crack-team of software developers and IT architects from Callista Enterpise, lead by the formidable trio Magnus Larsson, Peter Larsson and Fredrik Larsson has developed a new programming language for the nano-service, cross-cloud, no-platform based programming model.  It has now become time to share the good news and release this paradigm changing novel programming language to the wider community. We sincerely hope you will enjoy it!  -[readmore]-   The Callang programming model introduces some rather novel concepts, such as Domain Model Missinterpretation Detection, or DMMD for sort, as well as extensive cross language support at compile time. This creates a completely new programming model where the programmer can focus on writing cool stuff, fast, and deploy fast without need for the whole build-test-deploy life-cycle of traditional development.  Some key requirements have of course been real-time start-up capability, ubiqious support for any communication protocol, as well as the mentioned cross languge support, to be able compile and run any code witten for any of the other top languages such as Java, Golang, C/C++, Python or even Rust.  The new Domain Modelling Missinterpretation Detection  feature is based on the use of advanced deep learning algorithms to detect and correct domain modelling missinterpretations at compile time. A new feature, but something that we believe will become more of a standard as languages evolve and become smarter over time.   The development team for the Callang language are proud to present the results from some early trials and comparisions to other leading programming languages. I should mention that the team consist of, apart from the lead trio, also the senior members Björn Gylling, Björn Beskow, and Björn Genfors. But first let's look at some code! As mentioned Callang a new feature for detecting and correcting the domain model of any appllication. This enables some pretty amazing new code styles, such as this api for crud operations:   And now some results, comparing with Go and Java, measuring computation time for factoral operations:  ![Measurements, factoral calculations between languages]   So, with these initially very promising results we want to invite our friends and collegues to try out our new programming language for the future, Callang, the Callista language! Go to [www.callistaenterprise.se/april_first.html] to find out more!
 Having used Subversion for a long time I wanted to learn how to use a distributed versioning system. The selling points for these products are very interesting.  For instance the possibility to separate commit from delivery seems like a nice option. If you work on something that takes a couple of days to do, you want to commit often even when only parts of the work is ready without disturbing your colleagues. You want to commit even if all tests are not green. You want to be able to experiment and throw some stuff away by going back to previous committed versions. With a DVCS  you do all of this in your local repository and only when you are finished you deliver to the project repository where other people then can use it.  -[readmore]-  Which product should you use ? Git and Mercurial seems to be the popular choices. Based on the post [Git is McGywer and Mercurial is James Bond] I choose Git, although I'm not a command-line wizard.  Installation on OS-X is very easy as expected. I used <http://git-scm.com/download>. In windows it is a bit more complicated because you have to choose between 2 different setups. I choose the option without Cygwin and running bash in command windows. I wonder who has the balls to choose the third option in this question?  [![]  Anyway creating a repository and checking files in is very easy   The  command will create the repository. The only bit of magic is the `git add` command. It will add one or many files to something called the index. Only changes added to the index will we affected by the next `git commit`.  Now you can start playing around with creating branches, make changes and merging branches. Working with branches is much less expensive than you thing and a recommended way of using git.  But having only a single repository probably is not want you want to do. Collaboration between different team members is of course the next thing to try out. In order to understand how that work I did like this.   Now you have three complete repositories. You can imagine that  and  are local repositories at different users and that repo the a project common repository. I used three different command windows, one for each repo. Then it was easy to change something in , push it to repo an pull the changes to . Also simulating conflicts and how to resolve them is very easy since you have full control of all three repos.   In conclusion getting started with Git is easy. Mastering all  of the Git functionality and different distributed repository topologies will probably take much longer.  Martin Fowler makes an interesting comparison between different products  ![]  From this you can see that Git and Mercurial are useful but more complex to learn.  When it comes to topologies I think that [Scott Chacon] has some nice pictures, how about this one :  ![]  * [http://help.github.com/git-cheat-sheets/] * [http://book.git-scm.com/index.html] * [http://ndpsoftware.com/git-cheatsheet.html#loc=remote_repo;] * [http://whygitisbetterthanx.com/#] * [http://martinfowler.com/bliki/VersionControlTools.html]  And .. this video is a must  <div class="ce-video"> <iframe src="http://www.youtube.com/embed/CDeG4S-mJts?feature=oembed" frameborder="0" allowfullscreen></iframe> </div>
In the [last part], we implemented the Database-per-tenant pattern, and observed that it has limited scalability. In this part, we will tweak the solution and implement the Schema-per-tenant pattern in much the same way.  -[readmore]-  [comment]: #  [Multi Tenancy]: https://whatis.techtarget.com/definition/multi-tenancy [SAAS]: https://en.wikipedia.org/wiki/Software_as_a_service [Spring Framework]: https://spring.io/projects/spring-framework [Spring Boot]: https://spring.io/projects/spring-boot [Spring Data]: https://spring.io/projects/spring-data [Hibernate]: https://hibernate.org/orm/ [JPA]: https://en.wikipedia.org/wiki/Jakarta_Persistence [experimental support for the shared-database-using-discriminator-column pattern]: https://hibernate.atlassian.net/browse/HHH-6054 [Liquibase]: https://www.liquibase.org/ [Database Schema Migration]: https://en.wikipedia.org/wiki/Schema_migration [Flyway]: https://flywaydb.org/ [Cross Cutting Concern]: https://en.wikipedia.org/wiki/Cross-cutting_concern [Aspect]: https://en.wikipedia.org/wiki/Aspect-oriented_software_development [Reversibility]: https://martinfowler.com/articles/designDead.html#Reversibility [Github repository]: https://github.com/callistaenterprise/blog-multitenancy [schema branch]: https://github.com/callistaenterprise/blog-multitenancy/tree/schema  [comment]: #  [neighbours]: /assets/blogg/multi-tenancy-with-spring-boot/undraw_neighbors_ciwb.png [Schema]: /assets/blogg/multi-tenancy-with-spring-boot/SeparateSchemaMultiTenancy.png  - [Part 1: What is Multi Tenancy] - [Part 2: Outlining an Implementation Strategy for Multi Tenant Data Access] - [Part 3: Implementing the Database per Tenant pattern] - Part 4: Implementing the Schema per Tenant pattern  - [Part 5: Implementing the Shared database with Discriminator Column pattern using Hibernate Filters] - [Part 6: Implementing the Shared database with Discriminator Column pattern using Postgres Row Level Security] - Part 7: Summing up   A fully working, minimalistic example for this part can be found in the [Github repository] in the [schema branch].   The major scalability problem with the Database-per-tenant implementation from last week is the fact that it forces us to use a separare DataSource per tenant. A Schema-per-tenant implementation can overcome this limitation by using one single DataSource and instead decorate each connection borrowed from the pool with the correct Schema for the specific tenant.  ![Schema][Schema]  So let's tweak the implementation from last episode into Schema-per-tenant!   The  implementation remains unchanged:     We will now only need a single dataSource, we no longer have to override the Spring Boot default DataSource. We will use a 'master' schema for the *master repository* with information about each tenant and its corresponding schema.  The JPA entity to represent meta data about a Tenant will just map a tenantId to a database schema:   The Spring Data Repository remains unchanged:   We can now simplify the implementation of the  interface . We use the single datasource to provide the connections, but decorate them with the correct schema to use before handling the connection to Hibernate. Likewise, we remove the schema information when the connection is returned.    We still need to configure two entityManagers: One master entityManager to host the tenant repository, and a separate entityManager to serve the tenant-specific databases. The entityManagers need their own transaction managers as well.  The configuration for the master entityManager remains almost the same as in the previous part:   Again, this configuration is very similar to the Spring Boot auto-configuration, but since we need dual entityManagers, we still have to configure them explicitly.  We do the same for the tenant entityManager, but this time we set the  to . We also explicitly remove any configuration, since it will always be set explictly.   As last time, since we mark the tenantEntityManagerFactory and tenantTransactionManager as , they will be used by default in any component that autowires a PersistentContext or EntityManager.  The externalized properties in application.yml are similar to the previous part. Most notably, we can now use the default configuration for DataSources:    The  which we use to onboard new Tenants becomes sligthly simplified. It still uses raw SQL to create the schema. Since the SQL is potentially vendor specific , you may need to tweek it to work with another database:   The simple, administrative REST endpoint to create new tenants is almost similar:    The Liquibase config also remains almost similar. We still need to run Liqubase all liquibase migrations on the Master repository as well as for all tenants. We'll start with the master liquibase configuration:   This is again more or less identical to to the Spring Boot auto-configuration, but since we need one config for the master database and a separate config for the tenant databases, we need to configure it explicitly.  Let's continue with the tenant database migrations. Just as before, ee'll need to query the TenantRepository for all tenants, and run a migration on each of them, using the correct schema.   And the config:   The liquibase configuration is externalized into application.yml as before:    We now have a dynamic implementation of the Schema-per-tenant Multi Tenancy pattern! Since we now use one single DataSource, we can expect the scalability to be much better in that respect.  A fully working, minimalistic example can be found in the [Github repository] in the [schema branch].   The Schema-per-tenant pattern provides a reasonably strong data separation between tenants. Most databases supports a large number of achemas, so we should likely have no problem in scaling this solution to thousands of tenants.  A scalability concern may however arise with the database migrations: Since we duplicate all tables for each tenant using Liquibase, running migrations for all tenants may take a substantial time. In our current implementation, we run any required migrations on application start . This may lead to a very long startup time. Even when there are no new migrations to apply, Liquibase will still do a negotiation with the database to find that out.  Hence for a large number of tenants, we would most likely need to rethink when database migrations are carried out . But that's a story of its own.  In the [next part] , we'll instead implement the Shared database with Discriminator Column pattern, using Hibernate Filters and some AspectJ magic. Stay tuned!
Today is Valentine's day and if you haven't yet thought about what you are doing for your loved one tonight, you'd better start now. It's winter now  and you don't want to be out in the cold, do you...? Well, anyway, I think you can agree with me that one of the foundation stones of love is the ability to communicate with each other.  I don't really see myself as an expert of communications between loved ones and this blog is not generally about celebration of love but more like a celebration of our own love to knowledge, information and aid to our clients, but I'll make an exception here.. So.. if you're having trouble communicating with your beloved, please continue to read on.  My job as a consultant - developer and architect, is very much about helping clients to communicate with business partners in the best possible way. In a sense you might think of me as some kind of a therapist, sitting down with my clients, sorting out their problems asking questions like "...and what do You think about that", making it easier for everyone to see what the problem is. And then perhaps reaching an understanding of what we need to do to get the communication flowing and hopefully, at the end of the session, we might have a conversation going that helps their relation to continue and grow more and more mature over time.  So you might say that helping partners to communicate is an important part of my job and to discover patterns of behaviour that reoccur over time, or perhaps create new behavioural patterns that might be more creative for the relationship in question. The same patterns occur over and over again when communicating  and they are often easy to discover but not that easy to change. The same goes for businesses and communication between business partners.  Luckily there is aid to find on the web. On Gregor Hohpe's excellent book and site on enterprise integration patterns, specifically the messaging pattern, you can read: "Use Messaging to transfer packets of data frequently, immediately, reliably, and asynchronously, using customizable formats.". Well isn't that the receipt for keeping your love going and growing? Let's disect that sentence and put it into a different context.  In order for a relationship to really be a lasting relationship we need to exchange information on a regular and _frequent_ basis, i.e. the more communication the better. There is really no time to throw away either, life's to short for that so start communicating _immediately_.  A part of messaging is to make sure your partner really got the message, if you know what I mean. So be clear in your message so there is no doubt what you are talking about and to be sure that the message really reaches your partner. Some sort of message _reliability_ that is.  An example of asynchronous messaging could be leaving a sweet message on the breakfast table or perhaps a rose beside the pillow before leaving for work in the morning. An _asynchronous_ action that later might lead to an email or a phone call where your partner confirms their love for you. It's that easy really...  Lastly, mr. Hohpe talks about customizable formats and that is, I must say, what Valentine's day is all about. Using well known patterns and formats, customizing them to your needs to maintain your relationship. As easy as that, yeah? Uhm, not really, but perhaps some words to help you on your way anyway.  Have a nice Valentine's day!
 [Soi-toolkit]. You also get support for logging, property handling, WSDL and XML Schema creation and much more.  -[readmore]-  So how do you get started? The Soi-toolkit website contains extensive help here and I have used the [installation guide] provided.  Since I'm on Mac OS I already have the recommended version of Java and Maven installed. Eclipse was already setup as well.  I'm used to running maven in a terminal window so I skipped the recommended Maven Eclipse plugin installation. Since I was only going to do some local work at this time, I skipped the Subversive Eclipse plugin. So the only thing I had left to do was to install the Soi-toolkit Eclipse plugin.  At the **Install new Software** menu option I entered the Soi-toolkit update site <http://soi-toolkit.googlecode.com/svn/eclipse-update-site> and got the confirmation that I was on the right track.  ![]  I also skipped the optional installations for now.  The configuration of the toolkit was no big deal after finding where maven was installed. I used the **eclipse:eclipse** options.  ![]  Next step is to create a component for my integrations. Using the wizard **Create a new component** and specify something like this:  ![]  Remember to select Mule 2.2.1 which is the public available version.  The first time eclipse:eclipse command is executed a lot of dependencies area downloaded, you must be patient. The  variable has to be set to where your maven repository is,  in my case.  Three eclipse projects are created, the service project is where the integration is going to be.  Now it is time to create an integration using the wizard **Create new service**. I picked a simple file transfer scenario to start with.  ![]  The main configuration file that was generated look like this   Also some test were generated and they show green when you execute them.  ![]  So we have a complete project setup including test, property handling, logging and build support. From then you can add transformers and other features to your integration. Obviously that need more knowledge on how Mule works, but using Soi-toolkit you know that you will be starting with best practise Mule usage.  Working with services and integrations in an open source environment has never been easier and Soi-toolkit takes Mule usage to the next level.
In this blog post we'll take a look at gracefully shutting down a Go program having worker goroutines performing tasks that must be completed before allowing the program to shut down.  In a recent project we had a use-case where a Go-based microservice was consuming events emitted from a 3rd party library. These events would undergo a bit of processing before resulting in a call to an external service. That external service handles each request quite slowly, but can on the other hand handle many concurrent requests. Therefore, we implemented a simple internal worker-pool to fanout incoming events onto several concurrently executing goroutines.  On a general level, it looks like this:  ![figure 1]  However, we needed to make sure that if the microservice is shutting down, any currently running requests to the external service must be allowed to finish with the outcome persisted to our internal backend.  The [worker pool pattern], but we realized that a number of our requirements made our use-case somewhat more complicated.  When the program receives a SIGTERM / SIGINT signal - for example, from our container orchestrator scaling down the number of replicas - any currently executing worker goroutines must be allowed to finish their long-running work before terminating the program.  To make things slightly more complicated, we had no control over the producer-side library. We get to register a callback function which is invoked each time the library has a new event for us. The library blocks until the callback function has finished executing and then invokes it again if there's more events.  The worker-pool goroutines are fed with events to process using the standard "range over channel" construct, e.g:  func workerFunc { for event := range jobsChan { // blocks until an event is received or channel is closed. // handle the event... } }  Which means that the cleanest way to let a worker "finish" is to close that "jobsChan" channel.  One of the first things you first learn about closing channels in Go is that the program will panic if a send occurs on a closed channel. This boils down to a very simple rule:  "Always close a channel on the producer side"  What's the producer side anyway? Well, typically the *goroutine* that is putting events on that channel:  func callbackFunc { jobsChan<-event }  This is our callbackFunc that we've registered with the external library that passes events to us. __  How do you *safely* protect the piece of code above from sending on a closed channel? It's not trivial to go down the route of Mutexes and boolean flags and if-statements to determine if some *other* goroutine has closed the channel and to control whether a send should be allowed or not. Very open to potential race conditions and non-deterministic behaviour.  Our solution was to introduce an intermediate channel and an internal "consumer" that acts as a proxy between the callback and the jobs channel:  ![figure 2]  The consumer function looks like this:  func startConsumer { // Loop until a ctx.Done is received. Note that select blocks until either case happens for { select { case event := <-intermediateChan: jobsChan <- event case _ <- ctx.Done: close return // exit this function so we don't consume anything more from the intermediate chan } } }  Ok, wait a minute. What's this "select" and "ctx.Done"?  The [select].  The _return_ statement after closing the jobsChan will step us out from the for-loop and function which makes sure *no new events can be passed* to the jobsChan and *no events* will be consumed from the intermediateChan.  So either an event is passed to the jobs  or the jobsChan is closed *in the same goroutine* as the producer.  Closing the jobsChan means all workers will stop ranging on the jobsChan on the consumer side:  for event := range jobsChan { // <- on the close, all goroutines waiting for jobs here will exit the for-loop // handle the event... }  Waiting for a Go program to terminate is a well-known pattern:  func main { ... rest of program ...  termChan := make signal.Notify <-termChan // Blocks here until either SIGINT or SIGTERM is received. // what now? }  At "what now?" the main goroutine resumes execution after capturing SIGINT or SIGTERM. We need to tell the consumer goroutine that passes events from the intermediateChan to the jobsChan to close the jobsChan across goroutine boundaries.  Again, it is technically possible - but rather awkward and error prone - to solve this using Mutexes and conditional statements. Instead, we'll utilize the cancellation support of context.Context we touched on earlier.  Somewhere in our _func main_ we set up a root background context with cancellation support:  func main { ctx, cancelFunc := context.WithCancel // ... some omitted code ...  go startConsumer // pass the cancellable context to the consumer function  // ... some more omitted code ... <-termChan  cancelFunc // call the cancelfunc to notify the consumer it's time to shut stuff down. }  This is how the _<-ctx.Done_ select case becomes invoked, starting the graceful teardown of channels and workers.  There's only one problem with the solution above - the program will exit immediately after the _cancelFunc_ invocation, which means our worker goroutines having in-flight invocations won't have time to finish, potentially leaving transactions in our system in an indeterminate state.  We need to halt shutdown until all workers report that they're done with whatever stuff they were doing. Enter [sync.WaitGroup] that lets us wait for an arbitrary number of goroutines to finish!  When starting our workers, we pass along a pointer to a WaitGroup created in _func main_:  const numberOfWorkers = 4  func main { // ... omitted ... wg := &sync.WaitGroup wg.Add  // Start [workerPoolSize] workers for i := 0; i < workerPoolSize; i++ { go workerFunc }  // ... more omitted stuff ...  <-termChan // wait for SIGINT / SIGTERM cancelFunc // send the shutdown signal through the context.Context wg.Wait // program will wait here until all worker goroutines have reported that they're done fmt.Println }  This changes our worker startup function a little:  func workerFunc { defer wg.Done // Mark this goroutine as done! once the function exits for event := range jobsChan { // handle the event... } }  The _wg.Done_. Graceful shutdown complete!   The source for the final program comes in the next section. In it, I've added some logging statements so we can follow what happens.  Here's the output for an execution of the program with 4 worker goroutines, where I use Ctrl+C to stop the program:  $ go run main.go Worker 3 starting Worker 2 starting Worker 1 starting Worker 0 starting Worker 3 finished processing job 0 Worker 0 finished processing job 3 ^C********************************* <-- HERE I PRESS CTRL+C Shutdown signal received ********************************* Worker 3 finished processing job 4 Worker 2 finished processing job 1 Worker 1 finished processing job 2 Consumer received cancellation signal, closing jobsChan! <-- Here, the consumer receives the <-ctx.Done Worker 3 finished processing job 6 Worker 0 finished processing job 5 Worker 1 finished processing job 8 Worker 2 finished processing job 7 Worker 0 finished processing job 10 Worker 0 interrupted <-- Worker 0 has finished job #10, 3 left Worker 2 finished processing job 12 Worker 2 interrupted <-- Worker 2 has finished job #12, 2 left Worker 3 finished processing job 9 Worker 3 interrupted <-- Worker 3 has finished job #9, 1 left Worker 1 finished processing job 11 Worker 1 interrupted <-- Worker 1 has finished job #11, all done All workers done, shutting down!  As one may observe that the point in time where the consumer receives _<-ctx.Done_ is actually non-deterministic due to how the Go runtime schedules communications on channels into the select statement. The Go specification says:  "If one or more of the communications can proceed, a single one that can proceed is chosen via a uniform pseudo-random selection."  This is why jobs may be passed to the workers even after CTRL+C is pressed.  Another peculiar thing is that it seems as jobs  are passed to the workers even _after_ the jobsChan has been closed. Well - they were actually passed _before_ the channel was closed. The reason for this is our use of a buffered channel with 4 "slots". This means that if all four workers have consumed a job from the channel and are processing them, there are potentially four new jobs waiting to be consumed by workers on the channel given that our 3rd party producer is constantly passing new events to us at a higher rate than our workers can manage. Closing a channel doesn't affect data already buffered into the channel - Go allows consumers to consume those.  If we change the jobsChan to be unbuffered:  jobsChan := make  And run again:  $ go run main.go .... omitted for brevity .... ^C********************************* Shutdown signal received ********************************* Worker 3 finished processing job 3 Worker 3 started job 5 Worker 0 finished processing job 4 Worker 0 started job 6 Consumer received cancellation signal, closing jobsChan! <-- again, it may take some time until the consumer is handed <-ctx.Done Consumer closed jobsChan Worker 1 finished processing job 1 <-- From here on, we see that each worker finishes exactly one job before being interrupted. Worker 1 interrupted Worker 2 finished processing job 2 Worker 2 interrupted Worker 0 finished processing job 6 Worker 0 interrupted Worker 3 finished processing job 5 Worker 3 interrupted All workers done, shutting down!   This time we don't see any "unexpected" jobs being consumed by the workers after the channel was closed. Having a channel buffer the same size as the number of workers is a however a common optimization to keep workers fed with data without unnecessary stalling on the producer side.  The snippets above are somewhat simplified to keep them as concise as possible. The full program with some structs for encapsulation and simulation of the 3rd-party producer follows here:  package main  import ( "context" "fmt" "math/rand" "os" "os/signal" "sync" "syscall" "time" )  const workerPoolSize = 4  func main { // create the consumer consumer := Consumer{ ingestChan: make, jobsChan: make, }  // Simulate external lib sending us 10 events per second producer := Producer go producer.start  // Set up cancellation context and waitgroup ctx, cancelFunc := context.WithCancel wg := &sync.WaitGroup  // Start consumer with cancellation context passed go consumer.startConsumer  // Start workers and Add [workerPoolSize] to WaitGroup wg.Add for i := 0; i < workerPoolSize; i++ { go consumer.workerFunc }  // Handle sigterm and await termChan signal termChan := make signal.Notify  <-termChan // Blocks here until interrupted  // Handle shutdown fmt.Println cancelFunc // Signal cancellation to context.Context wg.Wait // Block here until are workers are done  fmt.Println }  The consumer struct:  // -- Consumer below here! type Consumer struct { ingestChan chan int jobsChan chan int }  // callbackFunc is invoked each time the external lib passes an event to us. func  { c.ingestChan <- event }  // workerFunc starts a single worker function that will range on the jobsChan until that channel closes. func  { defer wg.Done  fmt.Printf for eventIndex := range c.jobsChan { // simulate work taking between 1-3 seconds fmt.Printf time.Sleep fmt.Printf } fmt.Printf }  // startConsumer acts as the proxy between the ingestChan and jobsChan, with a select to support graceful shutdown. func  { for { select { case job := <-c.ingestChan: c.jobsChan <- job case <-ctx.Done: fmt.Println close fmt.Println return } } }  Finally, the Producer struct that simulates our external library:  // -- Producer simulates an external library that invokes the // registered callback when it has new data for us once per 100ms. type Producer struct { callbackFunc func } func  { eventIndex := 0 for { p.callbackFunc eventIndex++ time.Sleep } }  I hope this little blog post have given a simple example of goroutine-based worker pools and how to gracefully shut them down using context-based cancellation, WaitGroups and producer-side closing of channels.
Too often when I hear someone talk about artificial intelligence , the Terminator/Matrix scenario is repeated, a warning that we shouldn't meddle with powers we don't understand and that the consequences of not adhering to this warning can be dire indeed.  This bothers me a little. I don't mean to say that AI/ML won't ever pose a risk in the future , but what bothers me is that I think this fear has the telltale signs of _a fear of the unknown_. AI and machine learning are seen as something mysterious and therefore threatening, concepts shrouded in mystique and understood only by a few Gnostics of the AI/ML cult. I don't think it has to, or should be, that way.  So, what is machine learning? How does it work? What are the real risks? By diving into these questions, and maybe some new ones on the way, I think we can shred the clouds of mystique and start to look at machine learning in a more rational way.   [Tom Mitchell] coined the following definition of machine learning in 1998: _"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E"_. Now, I can't argue with that definition, but allow me to elaborate a bit. Machine learning, including deep artificial neural networks, are algorithms that can be adapted to process widely diverse input data and still produce the desired expected output. An ML algorithm can even be expected to produce the desired output for inputs it has never before encountered. This is possible because ML algorithms, during their adaptation - or training - phase, are made to identify the underlying patterns of the input data and its correlation with the desired output.  What machine learning is not, is something sentient - or mysterious for that matter. Something that has its own will and desires. I think that there is a great risk of inferring more meaning into the words machine learning and artificial intelligence than what is really there, and thereby create a growing-ground for misconceptions about this group of algorithms. One big problem is of course that we still don't really know or understand from where all our own wants and desires originate, and that means that we are hard pressed to answer if machines through some advanced algorithms will ever develop wants and desires for themselves. But that is food for an endless philosophical debate and another blog post.   First of all, an exhaustive detailed analysis of the machine learning field is obviously too much to cover in a single blog post, but I will attempt to cover the common aspects of the most common ML algorithms . Note that I will not delve into any of the functions that are used by these algorithms, such as different cost or optimization functions. In this blog post I will try to keep things simple and general, in later ones I will dive into more specifics regarding certain algorithms and tools.  One thing is very important though, if you, like me, come from a software developer background your focus has probably been on the implementation of algorithms , in machine learning though, the data really comes into play. Bigly.  Machine learning starts with the assumption that there is some correlation between data that you have, or are in a position to acquire, and whatever it is your ML algorithm is expected to produce, but that this correlation is difficult to capture solely in program code. However, it might be possible to capture using a combination of arithmetic calculations and the right, and right amount of input data.  Before we go into the workings of the ML algorithms it is important to understand that machine learning involves a lot more than just passing a bunch of data through an intelligent algorithm and then you somehow - magically - get the output you desired. Now, you are of course free to run your ML project as you see fit, but in one way or another we all need to at least consider the following activities .  What data do you have available and what data is relevant to your task? You might have some databases you can tap for data, but also log files, some external but available data are good potential sources. What data you can use for your algorithm will have a great impact on the performance on your machine learning task, so spend time on this and try different datasets, can you simulate some data that you might not have?  It is also important to practice, and eventually get comfortable with, data visualization. It is by visualization many key features of your data will become apparent.  When you have a relatively good notion about the data you want to use in your machine learning algorithm it is time to look at how the data should be fed to the algorithm. This can involve for example some cleaning, formatting, sampling and grouping of your data.  Cleaning of data is the task of dealing with incomplete data and data that you suspect might be faulty. When it comes to missing data the question is if you should attempt to estimate the missing values or discard that data altogether and when it concerns suspiciously looking data you have to ask yourself if you trust the data or not? ... Well, do you?  Formatting is somewhat simpler, basically making sure the data is in a format that is understood by the algorithm.  Sampling and grouping the data concerns making sure you have representative data covering the as much as possible of the problem domain, and that you have data that can be used for training, cross-validation, and testing.  After having preprocessed your data you might actually be ready to start training your machine learning algorithm, but in order to get even better results some data transformation activities might come in handy. Data transformation involves e.g. scaling, aggregation, decomposition, and synthesizing of data.  Scaling is about making sure the values across different features are on a similar value scale. This ensures that no one single feature becomes overly dominant in the machine learning algorithm. Aggregation and decomposition of data involves turning multiple features into one or vice versa, also known as feature engineering. This is about finding the features that are really relevant to your task, e.g. if you have the times a user logged in and logged out from your system you might want to aggregate those into a feature that tells you for how long the user was logged in.  Decomposition of features is used for the same reason but require that you can deconstruct the data into its constituent parts. An example of data decomposition could be to break down email addresses into the part before the @-sign and the internet domain part, or datetime values into distinct date and time values.  Synthesizing data might not be the most common of techniques used in machine learning but useful in for example image recognition projects. Synthesizing data means to artificially create completely new data, e.g. by distorting images through the use of some image manipulation software, for example create gray-scale versions of full color images etc.  Independent of which machine learning algorithm you implement they all have a few common traits that can be useful to know about. We have talked about the input data and features of the data, but how does this data look? We can think of the input data as a matrix of values, where each row represents one instance of the input data and each column a data feature. This is one such common feature between all machine learning algorithms, and one reason why all machine learning algorithms are heavily dependent on matrix operations.  Another common trait between machine learning algorithms is the use of weights to manipulate the input values. An algorithm has one weight per feature of the input data . During the training of the algorithm the weights gets updated in ways particular for each algorithm, but it is the update of these weights that eventually improves the performance of the machine learning algorithm as it is trained.  The purpose of this blog post is not to delve too deep into the particulars of different machine learning algorithms, but it can be useful to point out that there are a lot of machine learning algorithms available out there, and that they are useful for different things. Typical problems suited for machine learning algorithms are forecasting/continuous value prediction and classification problems, and different algorithms have been developed to handle these problems.  Further, the algorithms are often divided into supervised and un-supervised algorithms where supervised means that the training of the algorithm requires training data with known expected output, whereas un-supervised algorithms don't and are typically used for grouping data into clusters. Below is a small table with typical machine learning problems and some common corresponding algorithms.   | | Supervised | Un-supervised | | ---------- | ---------- | ---------- | | Forecast/continuous value | Linear regression | | | Classification | Logistic regression, Support vector machines, Artificial neural networks | K-means |  How do machine learning algorithms "learn"? I think this is key in order to de-mystify the machine learning algorithms. As mentioned above the weights of the algorithm gets updated during training and this causes the algorithm to perform better. How to they get updated? When you train your machine learning algorithm you let all your training data pass through your algorithm and record its output for each instance of training data. In case you are using a supervised learning algorithm you can then compare the output from the training run to the expected output and calculate a "cost" of using your current set of weights. In order to calculate the cost there are different cost functions you can utilize, depending on which machine learning algorithm you are using. Important to understand here is that you can use a cost function to calculate the relative cost of a certain set of weight for your algorithm.  Next, we will try to minimize the cost, or output value of the cost function by manipulating the weight parameters. Again, different functions can be utilized for this, but one so common it must be mentioned is gradient decent, which uses a typically quite small value called "the learning rate" to manipulate the derivative, or gradient, of the cost value and then subtracts this from the weights of the algorithm to make bit by bit lessen the cost and optimize the algorithm. 1. Make test run with test data. Record output. 2. Calculate cost of current set of weights by comparing real output and expected output in a cost function. 3. Manipulate algorithm weights in order to produce output closer to expected output, e.g. by using derivative of cost in a gradient decent function. 4. Repeat until the algorithm is no longer improving.  There are many real and current risks with machine learning, and I will just mention a few to show there are more acute ones than machines turning us into bio-electric slaves or turn our skulls into cobble stones.  An ML algorithm will learn from its training data, but it is often very hard to see just how the algorithm reaches its final output. This makes it very difficult to detect maliciously placed backdoors or unintended flaws that causes the algorithm to respond in unexpected ways. If you are interested, this article in [Wired] bring up just such a case of a maliciously placed backdoor.  Well, we can safely assume that even the best ML algorithm will eventually get something wrong , but as long as it is better at it than a human doing the same task everything is fine, right? But what do we do when things eventually do go wrong and the consequences are serious? It is too easy to become complacent and say _"as long as the AI does the job better than a human, what's the problem?"_. But algorithms can't go to court, simply put.  Machine learning is a very powerful and diverse tool and being such a powerful and diverse tool, it will inevitably be put to work on many problems. This will make ML algorithms more and more pervasive in our society and in our organizations. But these algorithms are complex and hard to supervise, and things get even more complex when we combine many ML systems together to solve ever more complex problems. The problem with this is that we end up with a situation where it is extremely difficult to see how some very important decisions were made, when the data that was the basis for the decision has passed through a number of black-box ML systems on the way.  So, these are just three  very real risks or challenges with ML algorithms that we need to watch out for, if we remain cool-headed and rational and all that.  As I said in the beginning of this blog post I think there exist a level of anxiety about machine learning that has to do with not fully understanding how it works or even what it is. If you encounter such fears but are unsure how to counter them I hope that this blog post gives you a bit of ammunition, hopefully without too many false claims.  Finally, when I myself got interested in machine learning I tried to read as much as I could on different blogs and guides online, but I often got stuck on many  new concepts and algorithms and I found it hard to get a bird's-eye view of the field. This blog is in no way an exhaustive cover of the field at any level, but I have also tried to write this blog such as I would have liked to read it when I first started looking at this field. Please feel free to leave any comments, good or bad, and thank you for reading this far.
 In [part 1].  In part 2 I will discuss what happens when the test suite grows, look in depth at the [Kafka Consumer], and offer one solution for how to reduce long execution times.  The [source code for the example application] can be found by following the link.  -[readmore]-   * This line will be replaced with the    [Part 1] left us with a single working test that verified that a call to a RESTful endpoint resulted in 5 events being published to a Kafka topic. Running this test on my machine takes between 15 and 20 seconds.  Monitoring the JVM of the Gradle worker that executes the integration test with help from [VisualVM] shows minimal CPU usage and a single thread that spends most of its time sleeping. Using `docker stats` to monitor the container running the Spring Boot application shows a similar picture of inactivity. What's going on?  The test uses a Kafka [Consumer] to consume events. The producer is working asynchronously and the test needs to poll for events for a reasonable amount of time to be certain that it has fetched all relevant events. In this example the  will poll the Kafka broker for 10 seconds:   Reasonable is a judgement call - choose too short a time period and the tests will fail or be flakey, choose too long a time and the test suite will run for what feels like an eternity. For this case 10 seconds of polling is judged to be reasonable. Note though that the test cannot execute for less than 10 seconds.  A single "happy path" test is unlikely to be sufficient for a comprehensive test. Adding error scenarios, new endpoints or complex behavioural tests will quickly grow the test suite. Even a relatively small application may have hundreds of tests and execution time will increase linearly.  To simulate this let's run this test 100 times using the  annotation on the test method. What happens? On my machine the entire suite always completes successfully but takes about 15 minutes to do so. This is probably reaching a pain point for developer - they will need to start the tests, get a cup of coffee, and hope things are done when they get back .  How can we make things run a little faster?  As the [JUnit 5 user guide] describes...  > "By default, JUnit Jupiter tests are run sequentially in a single thread. Running tests in parallel — for example, to speed up execution — is available as an opt-in feature since version 5.3."  This sounds like a way forward for faster testing! Let's extend our Gradle task to enable parallel testing...   This configuration will execute all test classes and all test methods in parallel. The  strategy has been chosen with a  of 4 implying that up to 4 tests will be executed in parallel.  The next step is to enable our test class to be run in parallel:   Starting the test suite will cause 4 tests to start in parallel but each test fails. Why?   A quick check of the test output shows the problem:   Up until now all tests have been running sequentially and in the same thread. The test uses a Kafka  to consume events. Configuring the  is quite verbose and to avoid soiling the test code the  is encapsulated in an  class. Setting up a  is relatively expensive so the encapsulating class is implemented as a singleton to ensure that the  is only instantiated once per test suite execution:   How can we then deal with the restriction that the  can only be accessed by a single thread?   It is tempting to solve this by ensuring that each test instantiates its own . This will remove the concurrency problem but the tests will still fail. Why?  Above we have configured our Kafka  to join a single [consumer group] is defined as such:  >A consumer group is a set of consumers which cooperate to consume data from some topics. The partitions of all the topics are divided among the consumers in the group. As new group members arrive and old members leave, the partitions are re-assigned so that each member receives a proportional share of the partitions.  The intention here is to split up the workload but also to ensure no two s are working on the same partition at the same time.  In our single threaded scenario there is a single  consuming from all partitions. Adding more s to the group will result in each  working on a subset of the available partitions, or, if no unassigned partitions are available, no partition at all. As shown in part 1 our events are now distributed to a random partition on the topic. This means that a test with it's own consumer may or may not be consuming from the partition where the event has been published - it is effectively up to chance whether our test will consume related events or not.  A consumer group maintains a register of the offset of the next record to consume allowing the group to maintain a position on the topic partitions - i.e. what has been consumed and what has not. When a read is committed  the broker moves on the offset so that the next  to read from that partition will read the next record.  When a consumer group is created it needs to know where on the topic to start to consume and this can be configured with the  parameter. In the example application the  value is chosen to instruct the group to start reading from the earliest offset .  The example application deliberately creates a new consumer group with a unique name to avoid collisions between test suite executions which could result in events from previous executions being available to the current execution:   In theory it would be possible for each test to instantiate it's own  which is the sole member of a unique consumer group - this has not been investigated here. Using the  offset strategy above would require each test to filter all events ever published to the topic which may  place a significant overhead on the test.  Let's try to seperate the delivery of events to the test from the consumption of events from the Kafka topic.  Let's assume that a single Kafka  is sufficient for a single test suite execution and that the  is the sole member of a unique consumer group. That  could happily consume from all partitions from the earliest offset and store all records in memory during the execution of the test suite. This is what the  is intended to do:   Note that this class is package private and has been intentionally added to a different package to the test class. This class is not intended for direct use by any test.  The  implements the [Runnable] interface. Thus the  can be executed in its own thread and ensure the  is not accessed by multiple threads.  As the  has no direct relation to the test anymore it is free to poll the broker more frequently - polling has been set to 1 second . Any events retrieved during the polling phase will be added to the  field.  The  field is a HashMap stored in memory. The  is private and cannot be accessed or manipulated by other classes. To ensure thread safety a  is used; multiple reader threads can acquire a read lock except when a thread has acquired an exclusive write lock in which case they will be blocked until the write lock is released. Both read and write locks are acquired and released for the shortest period possible - i.e. when accessing or manipulating the .  The next step is to make the stored events available to the tests.   The  class is intended to expose filtered events to the tests and to hide the implementation details of the Kafka . This class is public and provides a single public method:   In our previous single-threaded example the test waited 10 seconds for consumption of all events during the  operation. In the  this behaviour is replicated by forcing the thread to sleep for 10 seconds on a call to . Once the operation is resumed records can be fetched from the  and filtered by `trace id`.  It is important to note that the  will return all events that it has consumed since the start of the test suite.  The last step is to make the  available to the test. This can easily be done using a [JUnit5 ParameterResolver] extension to pass the  instance to the constructor.   Note that the intention here is to ensure instantiation of the EventSource once and only once. If the  is instantiated multiple times then there will be multiple  instances. This could be a problem if there is collision on consumer group id  or lead to multiple in-memory copies of events.  The  instance can then be passed to the test in the constructor by activating the  extension:   ...and the events are now available to the tests.  Now we are able to consume events in parallel tests yet the tests will still fail. Why? It turns out to be a timing issue...  As noted before the test suite can start before the Spring Boot application is fully initialised. The  extension is used to hold the tests in the  lifecycle phase until the application is ready. In this case we are passing the  in the constructor - before the  lifecycle phase and possibly prior to the application being ready. As the application itself creates the topic the  may be too eager and may not be assigned any partitions. No partitions mean no consumption and all tests fail.  To solve this we will use lazy initialisation in the :   Note the use of  to guarantee that eager threads do not unintentionally instantiate multiple  instances.  Once this change is in place the tests suite starts to consistently return a successful result! By altering the number of threads in the thread pool we can start to see the promised performance improvements. The following numbers are a snapshot taken from my machine when running 100 tests:   From over 15 minute to close to 1 minute is a pretty good improvement and now the test suite is executing at a pace that a developer would find reasonable.  Below is a snapshot of activity with 100 threads case with help from [VisualVM]. It is possible to see the worker threads sleeping for 10 seconds followed by a short intensive burst of activity.  ![]  However there is one problem that lurks - the dreaded .  These example use an event that is not particularly large - as close to the default Kafka message limit of 1Mb as possible. Holding all events created by the test suite in memory is not a problem for a JVM with a max heap size of 512Mb.  Our Kafka  will be consuming events for the lifespan of the test suite and will start consuming from the beginning of the topic. To simulate this growing mass of in-memory events I restricted the JVM to max 256Mb in the Gradle task:   The illustration below shows how the available memory slowly drops off during test execution until there is a final catastrophic failure in the form of a .  ![]  Storing all events in memory seems unreasonable when a test only need access to events for little over the 10 seconds that the test is executing. Why not dispose of events after a reasonable time period? To do this I extended the  to perform a cleanup of messages more than 30 seconds old.  The first step is to define an  class that contains both the value of the Kafka record and a time after which the record is no longer of interest. After every poll the  will check to see if a cleanup is needed:   Once again a write lock will be acquired to ensure that readers will not have access to the  during the clean up. After this no more memory problems occurred, even with 256Mb of available memory.  As I stated at the start of this blog it is essential that tests are reliable and reliability trumps speed. Long running tests will cause frustration, but unreliable tests can bring the most hardened developer to tears.  How can we check reliability in our new improved test suite? In this case I have spun up a Jenkins container and asked it to run the integration test task every 3 minutes. After several hours of continuous running things look stable!  ![]  In [part 1] model which will provide a significant performance improvement without jeopardising reliability.  The [source code for the example application] can be found by following the link.  I hope this blog post has been of interest. Good luck with your testing!
 In my current project we have found that test using a slightly larger unit than single classes makes a lot of sense. Typically there is a service class with some methods that calls a number of entity objects to interact with the database. So our unit under test is often the service class, some entity classes and the database + the spring configuration for wiring.  -[readmore]-  The alternative would be to use mock objects extensively. We have tried that to and in some cases it makes a lot of sense but often the setup of the mock objects need a lot of code and to a certain extent is a copy of the implementation. When the implementation changes the mock setup has to change.  In test where the database is included you need a smooth way to setup data before the test. We have used DbUnit for that. Typically a test method has a String constant containing an xml representation of all the rows we want to be inserted before the test. It is reasonably compact using properties for columns rather than new tags which would have been to verbose. Typically a test case will  - set up data in the database  - get an initialized spring bean from the context, - call a method on that bean - either examines the returned value or the changed state in the database.  There are two problems with this approach when your doing a large system and have a couple of hundred test classes with 20-30 test methods each.  1. To create new test data you need to specify all `NOT NULL` attributes with correct values even if only some of them are important for the specific test your designing. 2. To change all affected test when your domain models evolves over time may create a lot of work. Attribute come and go or move between entities and then you need to change all test that uses that table. This has turned out to be a major refactoring activity for us. It it sometimes tempting to design your changes in a way that minimizes the test refactoring work and then you really are "bicycling on this ice". You may leave out constraint that really should have been in the database, not good.   In the beginning each test method had its own chunk of test data defined in one variable. Often this was copied to the next test method and minor changes was made to create a new scenario. Not only does this increase the refactoring burden, it also makes it very difficult to understand what the difference between two setups are.  By breaking up the data in 5-6 smaller pieces you can replace only one of the pieces with another version and reuse the rest. Choosing good names of the variables helps to understand the intent of the test and its data.  Many times all you want to do is change the value of one or very few columns in one row between two test methods. Using the same data, insert with DbUnit and then write 2-5 lines of java code to read, manipulate some attributes and save the changes makes a lot of sense for the same reasons as above, less data to refactor and easier to understand what a test really does.  Add a method to your test classes superclass that does this and in that method you can analyze table metadata, if a value is not supplied in the xml and the column is `NOT NULL` you may supply a default value based on the type like 0, " ", current time etc. or even get a default value specified in a properties file on a table/column basis.  If you do that you only have to specify the columns that really matter to a specific test and leave the rest out. So the test is not affected by changes in the columns that are not specified.  If there are some readonly data that can never change, populate it with a script that is separated from your tests.   Prepopulated minimal set of data that could be used by all tests, it simply doesn't work. After some time it is impossible to change that data because it is used by so many tests and the new test you want write needs the data setup differently.  Mixing java snippets to include variable data in a dbunit xml string and dbunit definitions, makes the code very hard to understand.   In the method call is the first parameter the id of the order ? Is the third parameter the quantity or the price ? Is there a value for the OPTION attribute specified in the order row or not ? You need to examine the method to find out, potentially having to jump between method definitions and the data setup.  DbUnit makes a lot of sense but you need to try to minimize the refactoring effort when the entity model changes.
 Due to the explosion of smart phones on the market, the need for exposing existing enterprise systems through the mobile channel is growing rapidly. One of the first questions that will come up is how we can establish a secure communication channel with the existing enterprise system.  In this article, I will cover both how to trust a server certificate for secure communication with the server, as well as providing a client certificate to the server for mutual authentication. The client certificate is bundled with the app and of course, the server needs to trust this certificate.  -[readmore]-  Before we can start, keys and certificates need to be in place. My article [Creating self-signed certificates for use on Android] covers how to create keys and certificates as well as importing them into supported key stores. If you don't have any keys or certificates, create the required files by reading that article.  I assume that you already figured out how to create a simple Android app. However, I do not assume that you have put  or  in your Android project’s  directory. Before you continue, make sure the two files are there. These files were created in [Creating self-signed certificates for use on Android].  This section covers how you can implement a custom  by registering a scheme for https communication and load a keystore and a truststore into a .  Android uses Apache Commons HttpClient but since we want communicate securely we must extend the  with some custom behavior . Therefore, we start by creating  and extend the .   Simply put, we need to do two things to get our mutual authentication to work. 1) Create an  where we load our keystores and 2) Register a scheme for https communication that uses our custom . This method will load our keystores.  Extend the  with the two new methods found below. If you do not want to use mutual authentication you can just load the trust store in which the server's certificate is. The resulting  will later be used when creating a scheme for https-communication.   We now have a method that loads our keystores and return an SSLSocketFactory.  Before we can communicate with our server, we must register a scheme for https communication that uses our . The  method and provide the following implementation:   That's about it. We now have the  class that we can use from our Android app to communicate over Https.  It is time to use our  and make a call to a web server. Since this is just a test, we can basically make the call from anywhere in the app and I choose to make it in an Activity's `onCreate.  To make a https call to the server, simply provide an implementation like the one below: 
  Spring Boot is great and in combination with Docker it's really fun to develop production grade applications. Though, in one of my recent projects the target execution platform turned out to be Windows .  Attempting to be a good citizen the Spring Boot App in question preferably should be managed as yet another Windows Service. Hmm... appear to be an old problem with several well-known solutions, but after some googleing I decided to share our solution.   Our requirements was only a few:  1. Open source always is preferred 2. Support for Java 8  3. Simple to install, uninstall and upgrade the Java App 4. Nice to have an option to customize the App icon   I've had some experience .  I will not dig deep into all of the capabilities of WinRun4J, since all you need to know is neatly summarized at their [site]. In our specific project it's not a big deal since the App in question not is subject to be distributed.   Both alternatives above needs a small custom Java wrapper class to handle Windows service events. Main problem to solve is launching/bootstrapping of the Spring Boot App, and Spring already has a solution in place with the  class.  With WinRun4J it's enough to write a custom wrapper class extending an WinRun4J  class. A generic Spring Boot Launcher wrapper class example is demonstrated below. The magic Spring Boot launch line of code is: .  **Implementation of a generic Spring Boot Wrapper**    WinRun4j provides Windows executables for 32 and 64 bit, and a deployment package contains at least the following   - Executable  - Configuration  - WinRun4J Jar library  - Application Spring Boot Jar library  Installing is extremely simple:  1. Make sure a standard Java JRE is installed  2. Unapck the files above into an App folder of your choice 3. Configure App 4. Install App 5. Start App  **Configure **   The actual Spring Boot application logged to disk  and also to a central syslog server, i.e. not to any native Windows log.  **Install**  _Note: the basename of app  must be same as the basename of the ini file_   **Start**  _Note: Windows Services UI might also be used_   **Stop**  _Note: Windows Services UI might also be used_   **Upgrade**  To upgrade your Spring Boot application replace the current application JAR  and then restart  the service.  **Uninstall**  _Note: Reboot after to get rid of everything_    With a tiny written instruction the application has been successfully installed on a bunch of servers by a remote technician , and all seems to run without any hassle.
In this part of the Go microservices [blog series].  __  1. Overview 2. Prometheus 3. Service discovery 4. Exposing metrics in Go services 5. Querying in Prometheus 6. Grafana 7. Summary   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P15  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._  In recent years, [Prometheus] database, optionally with one or more labels.  In this blog post we'll deploy some new services and applications.  The architectural overview of the monitoring solution:  ![overview]  During the course of this blog post, we'll accomplish the following:  * Adding a _/metrics_ endpoint to each microservice served by the prometheus [HttpHandler]. * Instrumenting our Go-code so the latencies and response sizes of our RESTful endpoints are made available at _/metrics_. * Writing and deploying a [Docker Swarm mode]-specific discovery microservice which lets Prometheus know where to find _/metrics_ endpoints to scrape in an ever-changing microservice landscape. * Deploying the Prometheus server in our Docker Swarm mode cluster. * Deployment of Grafana in our Docker Swarm mode cluster. * Querying and Graphing in Grafana.  Prometheus is an open-source toolkit for monitoring and alterting based on an embedded [times-series] database, a query DSL and various mechanics for scraping metrics data off endpoints.  In practice, from our perspective that boils down to:  * A standardized format that services use to expose metrics. * Client libraries for exposing the metrics over HTTP. * Server software for scraping metrics endpoints and storing the data in the time-series database. * A RESTful API for querying the time-series data that can be used by the built-in GUI as well as 3rd-party applications such as Grafana.  The Prometheus server is written in Go.   Prometheus includes four different kinds of metrics:  * **Counter** - numeric values that only may increase such as number of requests served. * **Gauge** - numerical values that can go both up or down. Temperatures, blood pressure, heap size, CPU utilization etc. * **Histogram** - representation of the distribution of numerical data, usually placed into buckets. The most common use in monitoring is for measuring response times and placing each observation into a bucket. * **Summary** - also samples observations like histograms, but uses quantiles instead of buckets.  I strongly recommend this JWorks [blog post] for in-depth information and explanations about Prometheus concepts.  Prometheus client libraries expose data using a really simple format:  # HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use. # TYPE go_memstats_heap_alloc_bytes gauge go_memstats_heap_alloc_bytes 1.259432e+06  Labels and metadata about a metric such as _go_memstats_heap_alloc_bytes_  comes with corresponding _# HELP_ and _# TYPE_ metadata.  * HELP - Just a description of the metric. In the case above, specified by the Go client library. For user-defined metrics, you can of course write whatever you want. * TYPE - Prometheus defines a number of metric [types]: See previous section.  Here's an example _summary_ metric from our lovely _"accountservice"_ exposing the _/accounts/_ endpoint:  # HELP accountservice_GetAccount GET /accounts/ # TYPE accountservice_GetAccount summary accountservice_GetAccount 0.02860325 accountservice_GetAccount 0.083001706 accountservice_GetAccount 0.424586416 accountservice_GetAccount_sum 6.542147227 accountservice_GetAccount_count 129  This [summary] as well as total time spent and number of requests.  We'll use the standard _prom/prometheus_ docker image from docker hub with a custom [configuration] file.  If you've checked out P15 from git, enter the _/support/prometheus_ directory where we have a sample Dockerfile as well as the _prometheus.yaml_ linked above.  Dockerfile:  FROM prom/prometheus ADD ./prometheus.yml /etc/prometheus/prometheus.yml  To build and deploy prometheus with our custom config from the _support/prometheus_ folder:  > docker build -t someprefix/prometheus . > docker service rm prometheus > docker service create -p 9090:9090 --constraint node.role==manager --mount type=volume,source=swarm-endpoints,target=/etc/swarm-endpoints/,volume-driver=local --name=prometheus --replicas=1 --network=my_network someprefix/prometheus  Prometheus should now be up-and-running on port 9090 of your cluster.  ![promethus server 1]  _Please note that this is a non-persistent setup. In a real scenario, you'd want to set it up with requisite persistent storage._  How does Prometheus know which endpoints to scrape for metric data?  A vanilla install of Prometheus will just scrape itself which isn't that useful. Luckily, scrape target discovery is highly [configurable] with built-in support for various container orchestrators, cloud providers and configuration mechanisms.  However, discovery of containers in Docker Swarm mode is not one of the officially supported mechanisms, so we'll use the [file_sd_config] discovery configuration option instead. _file_sd_config_ provides a generic way of letting Prometheus know which endpoints to scrape by reading a JSON file describing endpoints, ports and labels. The path is configured in Prometheus _prometheus.yml_ config file, i.e:  scrape_configs: - job_name: swarm-service-endpoints file_sd_configs: - files: - /etc/swarm-endpoints/swarm-endpoints.json  _/etc/swarm-endpoints_ is a volume mount that Prometheus server will **read** from, while our discovery application described in section 3.2 will **write** the swarm-endpoints.json file to the very same volume mount.  ![discovery overview]  The JSON format is simple, consisting of a list of entries having one or more "targets" and a map of key-value "label" pairs:  [ { "targets": [ "10.0.0.116:6767", "10.0.0.112:6767", ], "labels": { "task": "accountservice" } }, ....... ]  This example shows our "accountservice" running two instances. Remember that we cannot address the accountservice as a Docker Swarm mode "service" in this use-case since we want to scrape each running instance for its _/metrics_. Aggregation can be handled using the Query DSL of Prometheus.  I decided to write a simple discovery application  to accomplish the task described above. It's rather simple and fits into a single source file.  It does the following:  1. Queries the Docker API for running tasks every 15 seconds. 2. Builds a list of scrape targets, grouped buy their "task" label.  3. Writes the result as swarm-endpoints.json to the mounted _/etc/swarm-endpoints/_ volume. 4. Goto 1.  Some key parts of the implementation:   func main { logrus.Println  // Connect to the Docker API endpoint := "unix:///var/run/docker.sock" dockerClient, err := docker.NewClient if err != nil { panic }  // Find the networkID we want to address tasks on. findNetworkId  // Start the task poller, inlined function. go func { for { time.Sleep pollTasks } }  // Block... log.Println ... some code to stop the main method from exiting ... }  Quite straightforward - obtain a docker client, determine ID of Docker network we want to work on  and start the goroutine that will re-write that JSON file every 15 seconds.  Next, the pollTasks function performs the actual work. It's objective is to transform the response of the _ListTasks_ call from the Docker API into JSON structured according to the _file_sd_config_ format we saw earlier in section 3.1. We're using a struct for this purpose:  type ScrapedTask struct { Targets []string  Labels map[string]string  }  The "Targets" and "Labels" are mapped into their expected lower-cased JSON names using json-tags.  Next, the actual code that does most of the work. Follow the comments.  func pollTasks {  // Get running tasks  from the docker client. tasks, _ := client.ListTasks  // Initialize a map that holds one "ScrapedTask" for a given serviceID tasksMap := make  // Iterate over the returned tasks. for _, task := range tasks {  // Lookup service service, _ := client.InspectService  // Skip if service is in ignoredList, e.g. don't scrape prometheus... if isInIgnoredList { continue } portNumber := "-1"  // Find HTTP port of service. for _, port := range service.Endpoint.Ports { if port.Protocol == "tcp" { portNumber = fmt.Sprint } }  // Skip if no exposed tcp port if portNumber == "-1" { continue }  // Iterate network attachments on task for _, netw := range task.NetworksAttachments {  // Only extract IP if on expected network. if netw.Network.ID == networkID { // The process functions extracts IP and stuffs IP+service name into the ScrapedTask instance for the // serviceID. if taskEntry, ok := tasksMap[service.ID]; ok { processExistingTask } else { processNewTask } } } }  // Transform values of map into slice. taskList := make for _, value := range tasksMap { taskList = append }  // Get task list as JSON bytes, err := json.Marshal if err != nil { panic }  // Open and write file file, err := os.Create defer file.Close if err != nil { fmt.Errorf panic } file.Write }  Yes, the function is a bit too long, but it should be relatively easy to make sense of it. A few notes:  * Networks: We will only look up the IP address of a task if it is on the same network as we specified as a command-line argument. Otherwise, we'll risk trying to scrape IP-adresses that doesn't resolve properly. * Port exposed: The service must publish a port, otherwise the scraper can't reach the _/metrics_ endpoint of the service. * Targets: Services having more than one instance gets several entries in the Targets slice of their ScrapedTask.  There's not much more to it than this. Feel free to check out the complete [source].  Note that there [already exists] project on github for this purpose one could try as well.  When packaging our discovery microservice into a Docker image, we use a very simple Dockerfile:  FROM iron/base  ADD swarm-prometheus-discovery-linux-amd64 / ENTRYPOINT ["./swarm-prometheus-discovery-linux-amd64","-network", "my_network", "-ignoredServices", "prometheus,grafana"]  Note that we aren't exposing any ports for inbound traffic since no one needs to ask the service anything. Also note the _-network_ and _-ignoredServices_ arguments:  - -network: Name of the docker network to query - -service: Service names of services we _don't_ want to scrape. The example above specifies _prometheus_ and _grafana_, but could be expanded to more known supporting services that **doesn't** expose Prometheus endpoints at _/metrics_ such as Netflix Zuul, Hystrix, RabbitMQ etc.  To easily build & deploy the discovery service to Docker Swarm, there's a simple [shell script] whose content should be quite familiar by now:  docker service create --constraint node.role==manager\ --mount type=volume,source=swarm-endpoints,target=/etc/swarm-endpoints/\ <-- HERE! --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock\ <-- HERE! --name=swarm-prometheus-discovery --replicas=1 --network=my_network \ someprefix/swarm-prometheus-discovery  The two mounts may use a bit extra explanation:  * _--mount type=volume,source=swarm-endpoints,target=/etc/swarm-endpoints/_ - This argument tells _docker service create_ to mount the _volume_ named "swarm-endpoints" at _/etc/swarm-endpoints/_ in the file system of the running container. As described in the start of this section, we'll configure the prometheus server to load its scrape targets from the same volume mount. * _--mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock_ - This argument creates a _bind mount_ to the docker.sock, allowing the discovery service to directly talk to the Docker API.  Next, we'll add the Go code necessary for making our microservices publish monitoring data in prometheus format on _/metrics_ as well as making sure our RESTful endpoints  produces prometheus monitoring data picked up and published on _/metrics_.  _  The _/metrics_ endpoint Prometheus wants to scrape doesn't appear by itself. We need to add a Route at _/metrics_ that specifies a HTTP handler from the [Prometheus Go client] library:  Route{ "Prometheus", "GET", "/metrics", promhttp.Handler.ServeHTTP, <-- Handler from prometheus false, <-- Flag indicating whether to instrument this endpoint. },  Note the new "false" argument. I've added it so we can control which endpoints of the microservice to apply Prometheus middleware for .  In our "accountservice" we have a number of RESTful HTTP endpoints such as:  - /accounts/ GET - Gets a single account - /graphql POST - GraphQL queries - /accounts POST - Create new account - /health GET - Healthcheck  We should definitely add prometheus monitoring for the first three endpoints, while monitoring the _/health_ endpoint isn't that interesting.  For a typical RESTful endpoint, we probably want to monitor number of requests and latencies for each request. As each data point is placed in a time series that should suffice for producing good metrics for API usage and performance.  To accomplish this, we want a [SummaryVec] for some more info.   Capturing metrics is performed by injecting a Go http.Handler using the middleware pattern . We're using the most simple option where we chain handlers together, i.e:  **[router.go]**  // NewRouter creates a mux.Router and returns a pointer to it. func NewRouter *mux.Router {  initQL  muxRouter := mux.NewRouter  for _, route := range routes {  // create summaryVec for endpoint summaryVec := monitoring.BuildSummaryVec  // Add route to muxRouter, including middleware chaining and passing the summaryVec to the WithMonitoring func. muxRouter.Methods. Path. Name. Handler // <-- CHAINING HERE!!! }  logrus.Infoln return muxRouter }  _monitoring.BuildSummaryVec.  The _monitoring.WithMonitoring_ function is only invoked once, when setting up the middleware chain. It will either return the _next_ handler if the route being processed declares that it doesn't want monitoring, or the inlined http.Handler function declared after the if-statement:  func WithMonitoring http.Handler {  // Just return the next handler if route shouldn't be monitored if !route.Monitor { return next }  return http.HandlerFunc { // impl coming up ... } }  The implementation of our Prometheus monitoring middleware that will be executed on each call:  return http.HandlerFunc { start := time.Now // Start time of the invocation next.ServeHTTP // Invoke the next handler duration := time.Since // Record duration since start after the wrapped handler is done  summary.WithLabelValues // Store duration of request under the "duration" label.  size, err := strconv.Atoi // Get size of response, if possible. if err == nil { summary.WithLabelValues // If response contained Content-Length header, store under the "size" label. } })  To sum things up, we've done the following with the codebase of our "accountservice":  - Added a boolean to our Route struct so we can enable/disable metrics for it. - Added code that creates a SummaryVec instance per endpoint. - Added a new middleware function that measures duration and response size for a HTTP request and stuffs the results into the supplied SummaryVec. - Chained the new middleware func into our existing chain of middlewares.   To speed things up a bit, there's a new shell script [as.sh] one can use to quickly rebuild and redeploy the "accountservice".  After build and redeploy, our "accountservice" should now have a _/metrics_ endpoint. Try curl-ing http://192.168.99.100:6767/metrics  > curl http://192.168.99.100:6767/metrics  # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds 5.6714e-05 go_gc_duration_seconds 0.000197476 ....  Out of the box, the Go Prometheus HTTP handler provides us with a ton of Go runtime statistics - memory usage, GC stats and CPU utilization. Note that we need to call our _/accounts/_ endpoint at least one time to get data for that endpoint:  > curl http://192.168.99.100:6767/accounts/10000 .... response from the endpoint ...  > curl http://192.168.99.100:6767/metrics # HELP accountservice_GetAccount GET /accounts/ # TYPE accountservice_GetAccount summary accountservice_GetAccount 0.014619157 accountservice_GetAccount 0.018249754 accountservice_GetAccount 0.156361284 accountservice_GetAccount_sum 0.8361315079999999 accountservice_GetAccount_count 44 accountservice_GetAccount 293 ...  There they are! One can note the naming convention used, e.g: [namespace]_[route name]_*, we'll get back to how these names and labels are used in the Query DSL later in the Prometheus or Grafana GUI:s.   If everything works out, we should now have an "accountservice" producing metrics which the Prometheus Server knows where to scrape. Let's open up the Prometheus GUI at http://192.168.99.100:9090 again and execute our first query. To get some data, I've run a simple script that calls the _/accounts/_ endpoint with 3 req/s.  We'll do two simple Prometheus queries and use the graphing functionality in Prometheus Server to display the result.  We'll start with just counting the total number of requests. We'll do this by the following query:  accountservice_GetAccount_count  ![graph 1]  This just plots our linearly increasing  count for the GetAccount route.  Let's enter the following into the query field, where we select all quantiles for the "accountservice_GetAccount" having the "duration" label. We multiply the result by 1000 to convert from seconds into milliseconds.  accountservice_GetAccount * 1000  ![graph 2]  I've selected the "stacked" visualization option and it's quite easy to see that our 50th percentile  sits at about 16ms while the 99th percentile duration is approx 80 ms.  The Prometheus GUI can do more, but for more eye-appealing visualizations we'll continue by getting Grafana up and running and configured to use our Prometheus server as datasource.  [Grafana] is a platform for visualization and analytics of time series data. It's used for many purposes, visualization of Prometheus metrics is just one of many and fully describing the capabilties of Grafana is definitely out of the scope of this blog post.  We'll do the following:  - Getting Grafana up-and-running in our cluster - Configure it to use prometheus as data source - Create a dashboard plotting some data from our Accountservice  For the purpose of this blog post, we'll run Grafana without persistence etc which makes it a breeze to set up:  > docker service create -p 3000:3000 --constraint node.role==manager --name=grafana --replicas=1 --network=my_network grafana/grafana  Wait until it's done and fire up your web browser at http://192.168.99.100:3000. Grafana will prompt you to change your password and then take you to its Dashboard:  ![grafana 1]  _Note that we're running Grafana without any persistent storage. In a real setup, you'd set it up properly so your user and reports survives a cluster restart!_  Click the "Add datasource" button and enter _http://192.168.99.100:9090_ as server URL. Note that we'll using "Browser" access which means that Grafana will communicate with the Prometheus server through your browser as proxy. It sort-of works using Server-mode with http://prometheus:9090 as URL , but I keep getting issues with queries just refusing to complete so I'd recommend using browser-mode when just trying things out.  ![grafana 2]  Click the plus button in the upper-left and then select "Graph" as panel type. Next, click the chevron on "Panel title" and select "Edit" in the drop-down menu. You should see something such as:  ![grafana 3]  As you can see, you should select our "Prometheus Go" datasource from the Data Source drop-down. We should now be able to write our first query, using the same query language as we used in section 5.  If you start typing in the Query field, you'll get code-completion to help you get started. In the image below, I've typed "acc" which immediately results in a number of things we could add to our dashboard.  ![grafana 4]  Grafana is very powerful, with an overwhelming amount of options and capabilties for creating graphs, dashboards and analytics. There are people and blogs better suited to digging into exquisite details and graph skills, so I'll settle for describing the queries used for creating a dashboard with two panels. Both show the system running three instances of the "accountservice" under a light load.   ![graph1]  For the average latencies we'll use the following query:  avg_over_time / avg_over_time * 1000  The [avg_over_time function allows us to specify the time window during which we want to aggregate values in the time series, one minute in this case. To get the average, we're dividing the sum of latencies by the count which gets us the average, finally multiplying by 1000 to get the result in milliseconds instead of fractions of a second.  Due to the broken y-axis the results seem to fluctuate a lot, but is actually within approx 16-19 ms.   ![graph2]  Memory utilization is a classic metric in the world of monitoring. The default http.Handler from Prometheus automatically exposes this as a [Gauge] metric we can use in a Grafana dashboard. The query looks like this:  go_memstats_heap_inuse_bytes / 1000000  We see our three instances of the "Accountservice" hovering around the 5 mb mark.  As previously stated, Grafana offers great possibilities for visualizing and analyzing monitoring data exposed by the equally capable Prometheus ecosystem, whose finer details is out of scope for this  blog post.  In this part of the [blog series] and Grafana was our stack of choice. We've accomplished the following:  - Wrote a simple service discovery mechanism so the Prometheus server can find scrape targets on Docker Swarm. - Added Prometheus _/metrics_ endpoint and added middleware for exposing metrics from our RESTful endpoints. - Deployed Prometheus server + Grafana - Showcased a few queries.  In the [next part], we'll do a major refactoring of the codebase to better comply with contemporary and idiomatic Go coding guidelines and patterns.  Please help spread the word! Feel free to share this blog post using your favorite social media platform, there's some icons below to get you started.  Until next time,  // Erik
   The future of [Spring Framework]: Java 8 and Reactive.  -[readmore]-  Jürgen Höller gave a short presentation on the opening evening at [Spring2GX]. A little bit in the shadow of all the rapid innovation and hype with the [Spring Cloud] and associated projects, it's easy to think that the Spring Framework has stagnated and have become almost legacy. Jürgen's presentation proved that to be very wrong indeed. Two major themes are planned for the next major release of Spring Framework: * **Full adoption of Java 8**. Spring Framework 5 will no longer use JDK6 as the minimum Java version, but will require Java 8. It will be interesting to see how much impact that will have on the existing Spring Framework APIs. Just requiring Java 8 as the target runtime will not make much difference. The introduction of [Streams] in Java 8 will be a very welcome additions to many of the Spring APIs, but possibly at the expense of backwards compatibility? * **Fully supporting a reactive programming model**. Exactly how that is going to be achieved was not clear in Jürgen's talk. Judging from the rest of the talks at the conference, [RxJava] seems to have lost momentum.  Interesting news, indeed.
Spring Source recently announced a new member of the Spring family: Spring integration. It is an attempt to provide an abstraction for messaging, in the domain of business integration . It is indeed a very welcome addition to the Spring family!   "Integration" without a namespace, overlaps with the core of Spring itself. Spring is a framework for none-intrusive integration of platform mechanisms into application logic - sometimes by defining a unifying abstraction . When an application architect approaches integration, her concern is to abstract messaging. There are several reasons for doing so:  - Messaging API:s and communication mechanisms has a much shorter life expectancy than a typical enterprise application. Three years back, it was JMS, three years a head it may be JAX WS and so on. - Although JMS as well as JAX WS provides abstraction for message based middleware, a layer of message handling on top of these API:s will have to be repeated for every message consumer or producer, unless abstracted for re-use  - A higher-level API than JMS with support for dependency injection is more or less required for unit testing of business logic that depends on message exchange.  Spring Integration defines such an abstraction.   I'm not quite sure it is.  EIP includes Web Service integration. Web Services have not yet been incorporated, but communicated as a target. Although Web Services are often used for integration  down under the cover of a messaging abstraction. Today, I would say that the opposite is a better approach. The Spring-based service integration framework Apache CXF is a good example. I think Spring Integration should have been named Spring Messaging and solely be focused on the application developers perspective of message-based communication.  Spring is a framework for application architects. When an integration architect approaches integration, applications are considered "frozen commodity". A substantial part of EIP is targeting the needs of an Integration Architect. Aspects like routing, scatter/gather, pub/sub and other types of advanced mediation is better externalized to ESB-type of subsystems than integrated into the application build/release cycle. Mediation sub systems may of cause be based on Spring .   - Inbound JMS processing has no means to configure the type of sophisticated retry / error queue policies often required in real life. On the other hand - as all DI-based frameworks, the user could override any default ChannelAdapter implementation that is not fit for purpose. - Java EE Message Driven Beans are not supported. Inbound JMS processing must be implemented via Spring MessageListener container. This is not an option for enterprise integration at most customers I've been working for. Java EE message driven beans are typically the only remaining EJB-type in use today, due to the value they contribute in terms of monitoring, threading, advanced built-in retry/backout policies, failover awareness etc. - File-based messaging is supported, but with limited functionality. Several critical areas are missing, most of which could be contributed by integration . Spring Batch is not tied to a particular data source, the features it provides are typically needed when the source of the "message" is a legacy system that is not event based. Or better - let an ESB subsystem do the splitting / aggregation...   As the framework is scoped, it may inspire to bad habits rather than common integration best-practice . Even if a application framework supports multi-channel access to other systems , it is typically not the path an application architect should take. Chose a single, robust endpoint technology, like JMS and then delegate semantic and technical adaption to an ESB subsystem that connects to the JMS source.  If you chose a message-based approach, like JMS, Spring Integration is an excellent abstraction that should be used  for point-to-point / multi-channel integration - delegate to your enterprise ESB or to an application-bound light-weight ESB like Mule.
 As almost everybody knows; one of the main benefits with Apache Cassandra is the possibility to create column indexed time series, i.e. use  as comparator and you get a chronologically arranged list of indexed column names and an excellent performance when performing slice queries.  -[readmore]-  > If millisecond precision is good enough for you to create an unique id, then you might skip the rest of this article.  However Apache Cassandra uses UUID of type 1, and  has poor support for this type. Fortunately a utility class [com.eaio.uuid.UUIDGen] created by Johann Burkard can be used to create unique  of type 1, and this utility also is included in the Hector Client API.   Now I wan't to query my columns, and the main question is; how do I create a  of type 1 to position my slice query in the column name index?  n is designed to generate unique times , but the actual algorithm to create a corresponding query  is not provided, i.e.  can only be used to create new unique  column names. There's actually an implementation in , but this method is private, so this class can calculate the original timestamp in millis, but can't help out creating UUID for queries.  To solve the problem, I had to copy the actual unique time generation from  and remove the stuff that makes the generated time unique, and then it was pretty simple to create  that could be used in my queries. Furthermore to retrieve the original timestamp in millis the helper  fix this for you.  1. Create a unique UUID based on an origin timestamp in millis with  and use this as a column name 2. Create a query UUID with your own  described below. 3. Retrieve the actual origin timestamp in millis with   > Pretty heavy stuff to perform such a simple task, and please enlighten me if I've missed something fundamental!  Method to use to cerate query UUIDs:   And now it's just to start browsing the column name index with timestamps that means something like log record timestamps etc.   > Finally  See also this [article] for a nice way to iterate over a lot of columns, at least with some modifications attached.
In this part of the Go microservices [blog series] and how to add this to our Go microservices.  1. Overview 2. Distributed Tracing 3. Zipkin 4. EDGE server - Netflix Zuul 5. Go code - adding distributed tracing 6. Deploy & run 7. Summary   The finished source can be cloned from github:  > git clone https://github.com/callistaenterprise/goblog.git > git checkout P12  _Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding [git branch] branch in github._   State of the microservices landscape when we're finished with this part of the blog series:  ![landscape overview] _Figure 1: Landscape overview for part 12_  Marked with red boxes, we see our two new supporting components - the [Zuul]. Also, we see small boxes with "TA" indicating services where we've added distributed tracing.  Keeping track of the life of a request passing through a system  isn't exactly new. We've been adding request ID's, thread identifiers and user id's to log statements for ages. However, during the transition from monolithic applications to fine-grained microservices, the complexity increases when requests are passed between microservices, to storage backends, with messages spawning new requests - all belonging to the one and same business transaction. How do we identify performance bottlenecks when a requests is served by a large number of services, possibly in part relying on asynchronous operations completing?  While logs are very useful for this purpose, the concept of [distributed tracing] and distributed tracing with Zipkin.  [Zipkin] is an application for visualizing traces between and within services, their supporting components and even messaging. Zipkin originates from Twitter and is currently an open source project on GitHub. Zipkin provides a user-friendly GUI while the backend takes care of collecting tracing data and aggregating those into something we humans can make sense of.  I'm using an external pre-baked container image for Zipkin that exposes port 9411 for the admin GUI. Of course, you can build zipkin from [source], configure different storage backends etc.  A _docker service create_ can look like this:  > docker service create --constraint node.role==manager --replicas 1 \ -p 9411:9411 --name zipkin --network my_network \ --update-delay 10s --with-registry-auth \ --update-parallelism 1 openzipkin/zipkin  The visualization of a distributed trace can look like this :  ![tracing example]  We'll dig a little bit deeper into the possibilities of Zipkin when we've gotten our own tracing up and running.   In order to showcase how we can trace a request from start to finish, we'll introduce an Edge server capable of adding zipkin-compatible tracing information to HTTP requests out of the box. [Netflix Zuul] is the default Edge server of Spring Cloud / Netflix OSS.  If you’re wondering what the difference is between an Edge Server, a reverse-proxy and a load-balancer such as Nginx or HAProxy you’re probably in good company. From my point of view, an Edge Server such as Netflix Zuul can act both as reverse proxy, load-balancer and to some extent a security gateway with capabilities to support your applications with - for example - routing requests to the appropriate internal service, adding correlation id’s to inbound requests or relaying certain HTTP headers such as OAuth tokens.  The "edge" in the name stems from the fact that these servers usually resides where your internal network connects to the public internet or a DMZ net - or even _between_ different applications within your own enterprise, acting as the single way traffic may enter the logical internal network of your application.  I've prepared a container image pre-configured for our sample landscape with Spring Sleuth / Zipkin enabled as well as a simple routing rule that will provide a https endpoint at _/api/accounts/_ that will forward requests to our good ol' underlying "accountservice" at port 6767.  Just a glimpse of some of the Zuul configuration:  # Disable Eureka service discovery, we're on Docker Swarm mode. eureka: client: enabled: false  # Enable zipkin support, sample all requests spring: zipkin: baseUrl: http://zipkin:9411 sleuth: sampler: percentage: 1.0 sample: zipkin: enabled: true  # Zuul routing rules, will create /api/accounts/ mapping to http://accountservice:6767/accounts zuul: ignoredServices: "*" prefix: /api routes: accountservice: path: /accounts/** url: http://accountservice:6767/accounts  You can deploy a pre-baked Zuul image for this blog series using the following _docker service create_:  docker service create --replicas 1 --name edge-server -p 8765:8765 \ --network my_network --update-delay 10s --with-registry-auth \ --update-parallelism 1 eriklupander/edge-server   All of this sounds great - but how do we _actually_ add this cool tracing stuff to our Go-based microservices and how will Zipkin get hold of our traces?  Conveniently enough, there's a ready to use tracing library for us Go-nuts we can use, named [opentracing-go] standard - which is very compatible with Spring Cloud Sleuth used by Zuul and other Spring Cloud-based support services - that we're already using in this blog series.  In all honesty - Go isn't the ideal language to add this kind of stuff since there exists no . Also, the mechanisms offered by interceptors and/or AOP-based programming which is quite suitable for transparently adding functionality such as distributed tracing to a call-stack, isn't natively available in Go.  However - with some careful use of the go [middleware pattern] as the first parameter of each func in the call stack. Google themselves says the following in the official docs:  "At Google, we require that Go programmers pass a Context parameter as the first argument to every function on the call path between incoming and outgoing requests."  This is a somewhat controversial thing within the Go community. I know thread-locals are considered evil too, though very useful at times to keep track of request-scoped information such as security tokens, user principals and of course logging/tracing IDs.  Oh well - enough of this _"I dislike it but I use it anyway"_ stuff. Let's start coding!  Well - I wouldn't necessary call our little [tracing.go] file a library. It basically wraps some functionality of the go-opentracing library and provides a somewhat clean abstraction with a declarative API to start, stop and parse traces.  There's a few typical use cases where we need to concern ourselves with tracing info:  - Incoming HTTP requests: We look for opentracing correlation id's in HTTP headers and if found - starts a trace as well as dumping the required data structure into a Go context. - Outbound HTTP requests: Basically the reverse of the above. We check for tracing data in our Context and add that as a HTTP header in outgoing requests. - Sending a Message with AMQP: More or less the same as above, i.e. if our context contains opentracing ID's we stuff them into headers along with the message. Instead of HTTP headers we're using the header abstraction provided in the AMQP protocol. - Receiving a Message over AMQP: As you've probably figured out already - check if there's tracing data in a message header and if so - extract and start a new trace. - Internal tracing: Talking to an external database? Spawn a child-span to keep track of the amount of time used for that action? Performing a CPU-intensive operation on some data? Track this using a child-span as well. There are many occurrences where it makes sense to keep track of what's going on using tracing even within services.  Each microservice that wants to transmit tracing results to Zipkin needs to be configured to do that. For that purpose, we're going to use [zipkin-go-opentracing]. The code to set this up is very simple:  var tracer opentracing.Tracer  // InitTracing connects the calling service to Zipkin func InitTracing { collector, err := zipkin.NewHTTPCollector if err != nil { panic("Error connecting to zipkin server at " + fmt.Sprintf } tracer, err = zipkin.NewTracer( zipkin.NewRecorder if err != nil { panic } }  Note the initialization of the package-scoped _tracer opentracing.Tracer_ variable, which is the object we'll be doing all our tracing work with. The _zipkinURL_ actually comes from our Yaml-based config files stored on github and served to us over Spring Config and Viper:  zipkin_server_url: http://zipkin:9411  As you might notice, we'll using the http protocol for uploading traces to zipkin. Probably not the most efficient protocol for this purpose. Zipkin also support consumption of AMQP  messages.  As previously stated, we're going to be using the middleware pattern and _context.Context_ to work with tracing data in incoming HTTP requests. In _/accountservice/services/router.go_:  func NewRouter *mux.Router { .... other code above .... router.Methods. Path. Name. Handler // LOOK HERE! .... other code below .... }  func loadTracing http.Handler { return http.HandlerFunc { span := tracing.StartHTTPTrace // Start the span   ctx := tracing.UpdateContext // Add span to context next.ServeHTTP // Note next-based chaining and copy of context!! span.Finish // Finish the span }) }  What's going on above? In the _NewRouter where we do the actual work.  This looks a lot like interceptors and filter chains familiar from other languages and frameworks, where we "wrap" the call to a function into another function, allowing us to do stuff before and after the actual call - in this case starting a span and then closing it once the "next" func is done. We'll probably be adding more chaining of handlers in a later blog post where we'll be adding security and auth checking to our microservices.  The code to start a new HTTPTrace looks like this, e.g. our wrapping of go-opentracing code:  func StartHTTPTrace opentracing.Span { carrier := opentracing.HTTPHeadersCarrier // 1. Get hold of HTTP headers for tracing from request. clientContext, err := tracer.Extract // 2. Extract into a tracing context if err == nil { // 3. If there were a tracing context... return tracer.StartSpan( // 3.1 Start and return child span of the ongoing one opName, ext.RPCServerOption } else { return tracer.StartSpan // 3.2 Otherwise, start a new one from scratch } }  How are we using context to store the "tracing info", i.e. some correlation id's and such?  func UpdateContext context.Context { return context.WithValue }  Since the contexts are immutable, we're using _context.WithValue_ to add the supplied span to our existing context, returning the new context. Note the ugly use of _"opentracing-span"_ as key. I don't particularly like this pattern with hard-coded keys but at least its _only_ the "tracing.go" code that knows about the key we're using to fetch the current tracing span from our thread-local substitute - e.g. the context we're passing around.  So - let's say our "accountservice" got tracing info when Zuul routed a request to _/accounts/_. Now, we want to continue that trace when the "accountservice" performs a HTTP call to the "imageservice". This code is quite intermingled with the Circuit-breaker and retry code from the [last part], but I hope it makes sense anyway:  // Note how we pass context as 1st param and are passing the HTTP req object as a parameter too. func PerformHTTPRequestCircuitBreaker { output := make // hystrix stuff... errors := hystrix.Go error { // hystrix stuff... tracing.AddTracingToReqFromContext // HERE!!! err := callWithRetries return err // For hystrix, forward the err from the retrier. It's nil if OK. }, func error { return err }) ... some more code ...  We see that we're calling _tracing.AddTracingToReqFromContext_:  func AddTracingToReqFromContext { if ctx.Value == nil { // Do nothing if no tracing context available return } carrier := opentracing.HTTPHeadersCarrier // Create HTTP carrier for passing tracing data connected to the passed request. err := tracer.Inject( // Inject passes span data into the HTTP headers of the request ctx.Value, // Note ugly typecast here and use of the hard-coded key... opentracing.HTTPHeaders, carrier) if err != nil { panic // Here be dragons. } }  We'll - I guess the code above isn't my finest hour, but it basically fetches tracing stuff from the passed context  and passes it into the request object as HTTP headers.  We can of course add child traces without dealing with HTTP headers - we could even pass _opentracing.Span_ structs around as parameters instead of using that ugly _context.Context_. A really simple use case is when we're calling our BoltDB to fetch the Account instance. Looks like this:  // Note that we're passing the context as 1st param, just as Google asks us to! func  {  // Tracing code. span := tracing.StartChildSpanFromContext // Start a child span of the current one, // named QueryAccount defer span.Finish // Note use of defer, e.g. the span won't be finished and uploaded to Zipkin until // the ongoing func has finished.  at the very last // line of this func.  account := model.Account err := bc.boltDB.View error { ......... more code ......... }  See comments for details.  A quick peek at this particular trace in Zipkin:  ![queryaccount]  Yes, it's the tiny one using 33 microseconds just below the middle, with "accountservice" as its parent. We'll look more closely at Zipkin very soon.  There's a number of other code changes for this part of the blog series. The key changes being introduction of _context.Context_ as 1st parameter, passing of trace id's  across microservices using HTTP or AMQP headers and each microservice uploading traces to Zipkin using HTTP.   Let's get this show on the road, shall we? We've already covered deployment of Netflix Zuul and Zipkin. Also make sure you've checked out branch [P12], we can continue by rebuilding all our go microservices using the "./copyall.sh" shell script:  > ./copyall.sh built /Users/myuser/goblog/src/github.com/callistaenterprise/goblog/accountservice built /Users/myuser/goblog/src/github.com/callistaenterprise/goblog/vipservice ... and so on ...  This should build all go-based microservices and deploy them to our Swarm. Let's take a look at [dvizz] on http://192.168.99.100:6969:  ![dvizz]  Quite a few services! Time to do a few requests using curl to the Edge server and see if we can get some traces into zipkin!  We'll use curl to request _/api/accounts/10000_ which is the endpoint served by our Zuul Edge server. Internally, the flow of requests should be like this:  ![flow]  1. Our HTTP client only knows about the EDGE server and requests /api/accounts/ over HTTPS 2. Zuul routes this request to the _accountservice_ using the logical service name "accountservice" using HTTP. 3. The _accountservice_ internally loads an accountobject from its BoltDB database and then sends a message to the _VipService_ using AMQP. 4. Next, the _accountservice_ requests a "quote of the day" from the _quotes-service_. 5. Finally, the _accountservice_ requests an imageURL from the _imageservice_.  Run a few calls using url:  curl -k https://192.168.99.100:8765/api/accounts/10000  __  Open the zipkin GUI at http://192.168.99.100:9411 and click the "Find traces" button:  ![zipkin1]  Cool! The traces are there right away. We see that the longest request needed about 45ms from start . The 45ms trace is made up of 9 spans with varying lengths. By clicking on the topmost trace, we can examine it in more detail:  ![zipkin2]  Examine the trace above closely. If we are troubleshooting performance issues, it should be relatively straightforward to spot the most likely culprit for most of the 45ms duration.  Remember, when reading the trace, the topmost spans are usually spending most of their time waiting for sub-services to finish. We should pay special attention to _leaf_ operations taking a lot of time. Let's see:  - getaccount uses 32ms - getquote uses 30 ms - queryaccount using 29μs is the BoltDB query. - getimageurl uses 1.1 ms  so that call is also quite cheap. - vipservice#onmessage uses about 11ms, but remember that we're just sending an asynchronous message so that execution isn't blocking anything else.  Since the _getquote_ span makes up 30 ms of the total 32 ms of the _getaccount_ span, we can probably say for certain that the quotes-service is guilty.  __  Needless to say, a tool such as Zipkin can be invaluable for identifying both _which_ services that's invoked when your microservices are serving a request as well as identifying where time is being spent.  It's also possible to click on the individual spans for even more detail. A cool thing about opentracing and zipkin is that you can attach both arbitrary key-value pairs as well as "log events" to spans that ends up in Zipkin. Here we see Zuul providing some extra info for us:  ![zipkin3]  Of course, we can add this kind of stuff in Go code too.   Let's take a quick peek at resource usage, we've added quite a bit of code in the last parts in regard to circuit breakers, tracing, configuration, logging etc:  CONTAINER CPU % MEM USAGE imageservice.1.fcaax3b2coexljqs82l72sw6q 2.13% 4.496MiB accountservice.3.ma5x5r9wzkkfippr5lg1rucce 0.22% 4.445MiB vipservice.1.ydi9g7qg5fx6841dznzhlynk1 1.93% 3.418MiB  Our go services are still lean.  In a few blog posts, I plan to deploy all of the above using AWS CloudFormation and Docker Stack to an Amazon EC2 cluster made up of t2.micro instances. There, we will really start to notice the impact of resource-friendly services when we start to scale stuff.   In this part of the [blog series] and Zipkin for collecting and viewing traces.  In [part 13] we'll take a look at using Go with CockroachDB and the O/R-mapper GORM.  Please help spread the word! Feel free to share this blog post using your favorite social media platform, there's some icons below to get you started.  Until next time,  // Erik
 I am amazed each time I am working with timestamps and calendars - how hard can it be? It's hard to meet all requirement since each person is used to their own calendar. Latest I been implementing a calendar, to be displayed graphcial, in [Eclipse RCP].  Imaging a calendar that is representing working hours for a factory or shop, the working hours is represented in duties and breaks that is represented in the database in open intervals. A day could look like:  - Monday 08:00-12:00 - Lunch: 12:00-12:45 - Afternoon: 12:45-17:00  Then of course each day of a year is not equal to another - Christmas is a thing that happens each year, I have heard. Another thing that could affect the normal calendar is of course meetings, education etc. So besides the normal duty and open times we need to have some kind of deviation handling.  The easy deviation - close down whole days, for example on Christmas eve is one type of deviation. Another type of deviation is when someone has a birthday for example - cake break! Then you need to add deviations to just close hours and minutes, so you have your original schema with duties and breaks, and then add an extra cake break:  - Monday 08:00-12:00 - Lunch: 12:00-12:45 - Afternoon: 12:45-15:00 - Break: 15:00-15:30 - End day: 15:30-17:00  In the data base the open intervals will be saved in only the open interval:  1. 08:00-12:00 2. 12:45-15:00 3. 15:30-17:00  To show this graphical I am using [JfreeChart]. There are some good graphical samples to download. The samples code is only "free" for those who buy a licence for the development tool.  It was quite easy to implement an easy Gantt renderer to get the first outline.  _Bild saknas_  Code if someone is interested:   There is some default zooming out and in on the calendar that works instant. But the calendar is not that functional only showing the open times in green:  _Bild saknas_  I added tooltip when hovering over the green open times to display more information about each duty. Unfortunately the business wants information about the red part sometimes, when a deviation is added on normal open time. That I haven't solved yet. Since the chart is based on the open times, I am not sure if it is possible to add information about the closed times without changing the setup in the database.  To improve the display and show more information about each duty and break I added tooltip when mouse hovering over the green open times. The tooltip generator was not complete for the default implementation so I implemented my own with ids for each open times:  _Bild saknas_  The implementation of Jfreechart is quite straight forward, but as always when using a new tool it takes times to get the finish of the graphical layout. It was a lot of trial and error the see what happens each time you change a setting or property.  We are not sure if this will be enough for the business to display the calendar - the JfreeChart was probably not the best tool to show a calendar. But we will try it for the first release and then investigate what the business use and not use. JfreeChart have a lot of renderer to display other charts in various ways, look into the samples on the [homepage].
  In this blog post we'll use [Golang]".  -[readmore]-  In another context, I've begun exploring the data ingestion capabilities of [Spring XD]. One of the key features of XD is its broad range of input sources - from HTTP, TCP, JMS, files, various databases etc to Twitter streams and Emails - all built on top of Spring Integration components.  To properly test how XD works under heavy load I needed a tool to provide more than just HTTP while being able to simulate thousands of requests per second on a developer laptop. I've used [JMeter] and Gatling to simulate loads in the past, but neither seems to be the perfect fit for this use case. Gatling is more or less HTTP only and I've sometimes felt the Scala-based DSL not really being my cup of tea. Extremely powerful, but not always easy to grasp or express myself in. It's been a while, but the last time I used JMeter its underlying threading model limited the number of concurrent users, needing load agents to scale. And in all honesty, I'm not too fond of the JMeter GUI. Given that I've been reading up on and experimenting with Googles "Golang" programming language, I decided that brewing my very own load test tool in Go could be a fun exercise!   - A declarative, simple and human-readable way of specifying the simulation, no coding skills should be needed. - Fast simulation startup - a Gatling simulation often takes 5-10 seconds of build time before starting; I'd like close to immediate startup for the quickest possible feedback loop when developing load simulations. - Runnable on a developer laptop, meaning low CPU/memory footprint. JVM-based solutions will seldom use less than 100-200 mb of RAM. - Real-time dashboard. While I guess this may exist for Gatling through some plugin, I'd like a application-native real-time dashboard to monitor ongoing test execution. - Native support for more than pure HTTP. At least TCP/UDP sockets. - Optional: Realtime modification of test parameters - number of users, sleep durations etc. should be possible to change during test execution.   - Control statements. E.g. no conditionals or iterators in the "actions" sequence. - Parallell or chained "simulations" - Advanced feeders  - Test Recording / Proxies - GUI - ...more...  ![Not a powerpoint]  Not very complex - a simulation consists of a single YAML specification, an optional feeder and outputs results to a log and aggregated per second over a Web Socket. The  HTML reports actually aggregates results, graphs etc. on the fly using Javascript on the client side with the raw JSON log file as input.  Why YAML? My primary objective was to have tests specified in a human-readable but structured way. XML, no thanks. JSON - not the right tool in this context IMHO. A new DSL? Too much work.  YAML offers readbility, structure and is also well-known with parsers and writers for most modern programming languages.  The test specification is made up of simulation metadata such as number of users, rampup time and number of iterations to run the _actions_ list. A series of actions makes up the actual load test work performed by each user. Gotling currently supports three actions:  - HTTP requests - TCP packets - Sleep  Let's take a look at a sample test specification, each # is a comment about the feature on that line:  --- iterations: 5 # Number of times to execute the actions per user users: 100 # Number of concurrent users rampup: 10 # Linear ramp-up time. E.g. ~10 per second in this case. feeder: # Similar to Gatling feeders, a .csv file whose data can be fed to the context of each user type: csv # Format. Currently, only .csv is supported. filename: testdata.csv # Goes into the /data folder  # A test specification consists of the metadata above and a number of actions. Currently supported actions are: # sleep, http, tcp actions: - sleep: # Defines a sleep action. Will make the current user iteration sleep for 'duration' seconds duration: 1 # Sleep duration in seconds - http: # Defines a HTTP request action title: Get all courses # Human readable title, used for aggregation in reports method: GET # HTTP method. GET, POST, PUT, DELETE should work. url: http://localhost:9183/courses # URL to execute HTTP request against accept: json # Not in use right now. Only JSON can be handled at the moment) response: # Defines handling of response data . Optional jsonpath: $[*].id+ # Defines a JsonPath expression to use for extracting a single value from the response body variable: courseId # Store the extracted value in the current user's context with this variable name index: random # first, random, last. If more than one match for the JsonPath expression, pick the first, last or a random result. - sleep: duration: 3 - http: title: Get course method: GET url: http://localhost:9183/courses/$ # Note how we use the variable 'courseId' extracted by the http action above. accept: json response: jsonpath: $.author+ variable: author index: first # first, random, last - sleep: duration: 3 # After this sleep, the test will clear its variable context and do another # iteration until  iterations have been executed   HTTP POST/PUT with bodies are also supported, including using variables from previous actions or feeders:  ............ - http: method: POST url: http://localhost:9183/courses/$/poi body: '' accept: json  The body data above needs to be wrapped in single quotes as JSON is a valid subset of YAML and won't be handled as pure string unless quoted. As seen, we've used a number of variables. They can be populated either from previous HTTP responses or a feeder.  There are some samples  in the github repository.  I have shamelessy "borrowed" the concept of [feeders] from Gatling. We can imagine the $ coming from a previous HTTP action, while the other values comes from a .csv file which could look like this:  author,lat,lon,type erilup,58.5435335,12.5453453,OOB grifyu,58.3224255,12.5545345,GRE ... more lines ...  E.g - title of the column in the csv files becomes the variable name.  Enough of the basics, let's dive into Golang and why I think it was a really nice language to build all of this in: [Goroutines]. - A goroutine is a lightweight virtual "thread" starting at [8 kb of RAM]. - A channel is a mechanism used for inter-goroutine communication. It is typed and comes in both blocking- and non-blocking  flavours.  Since Gotling mainly evolves about executing hundreds or thousands of virtual users without totally hogging all system resources, goroutines were an excellent match. The channel abstraction is mainly used for letting each virtual user  report back results to a simulation-wide mechanism.  I've thrown together the sequence diagram below as an overview of the life of a Gotling simulation. It's not 100% accurate and has a number of omitted participants for the sake of clarity. From a Golang perspective, all async invocations are functions called with the '[go]' keyword making the specified function run concurrently in its own goroutine. The main async thresholds in the application are:  - Starting the Web Socket server - Starting each 'User' - Reporting action outcome to the resultshandler  - Assembling per-second aggregation - Transforming to JSON and sending over the Web Socket  ![Sequence diagram]  The snippet of code below showcases many core features of the Go language - the [spawnUsers] function runs on the main thread and launches "users" using the go keyword in a conccurent manner:  // Function declaration. Takes a pointer to a TestDef struct and an array of interface as arguments. // Note that one declares variables in golang with name followed by type. func spawnUsers {  // Here we see the := operator in use - the compiler will later infer the type based on what the right-hand expression returns. // The make declares a channel of type HttpReqResult with a buffer size of 10000. resultsChannel := make  // Note use of 'go' keyword. This invokes the acceptResults function in a new goroutine, passing the resultsChannel // channel as argument. go acceptResults  // WaitGroups is a simple counter-based lock mechanism. The "main" thread will block at wg.Wait until the internal // counter in Wg == 0. This is so all concurrently executing goroutines can finish before the main program exists. wg := sync.WaitGroup  // Calculate the number of seconds to wait until spawning the next user. var waitDuration float32 = float32  // For-statement. Nothing notable except the absence of parenthesises. Note how i := 0 is used instead of var i = 0 for i := 0; i < t.Users; i++ { // Add 1 to the WaitGroup lock wg.Add  // Generate a unique identifier > 10000 UID := strconv.Itoa  // Spawn a goroutine the will execute actions for the current user. // Note that we pass a & reference to the WaitGroup. go launchActions  // Sleep the main thread to implement a linear ramp-up time. time.Sleep } fmt.Println  // Will block here until wg count is 0 wg.Wait }  For further clarification, this is the [launchActions] has finished.  func launchActions { var sessionMap = make  for i := 0; i < t.Iterations; i++ {  // Make sure the sessionMap is cleared before each iteration - except for the UID which stays cleanSessionMapAndResetUID  // If we have feeder data, pop an item and push its key-value pairs into the sessionMap feedSession  // Iterate over the actions. Note the use of the Execute method on the action interface, command-pattern style. for _, action := range actions { if action != nil { action.Execute } } } wg.Done }  I kind of like the duck-typed implicit interfaces of Golang. Here's the [Action] interface declaration:  type Action interface { Execute }  And here is the [HttpAction] method:  // struct. Note the YAML-specific formatters. The struct fields needs capital first letters, controls public/private scope type HttpAction struct { Method string  Url string  Body string  Accept string  Title string  ResponseHandler HttpResponseHandler  }  // This is how we attach a Method to the HttpAction struct. Since its signature matches the Action interface, // it is now an implicit implementor. func  { // Semi-ugly, calls globally scoped method declared elsewhere DoHttpRequest }  It's possible to assert in runtime that a struct implicitly implements a given interface:  var _ Action =   The above will yield an error if the assignment is not possible. Note the use of the underscore identifier - it's used as a "ignore me" variable name in golang. Golang has the nice but sometimes annoying feature that unused variables or imports yields compile errors unless the assignment is to the underscore "blank" identifier. Noticed the multi-value returns often seen in the codebase? Example:  resp, err := client.Do // execute Http request  The first value in this example is a pointer to the Http response and the second is the [error] type. If you do not care about possible errors, the _ identifier can be used to ignore whatever the called function returns. Remember, in golang all declared identifiers _must_ be used or ignored using the _ identifier, otherwise it won't compile. This code won't compile since the declared _err_ isn't used:  func doRequest { resp, err := doSomething fmt.Printf }  Results are HTTP-only this far, TCP/UDP results support will be added eventually. Anyway - we make heavy use of Golang Channels to pass the result of each HTTP invocation back to a global results aggregator. The [code] below is slighly simplified to keep it readble:  func DoHttpRequest { req := buildHttpRequest // factory method for building a request. client := &http.Client start := time.Now // start request timer resp, err := client.Do // Exeute request. if err != null { panic } elapsed := time.Since // capture time spent responseBody, _ := ioutil.ReadAll // get the body  defer resp.Body.Close // defer makes sure the function is executed // after the current method has finished.  httpReqResult := buildHttpResult, resp.StatusCode, elapsed.Nanoseconds  resultsChannel <- httpReqResult // Pass the result to the resultsChannel }  Taken from the code that builds, executes and assembles the request outcome, the snippet above showcases two cool Golang concepts - [defers] of the message passed above looks like this:  func acceptResults { perSecondAggregatorChannel := make go aggregatePerSecondHandler for { select { case msg := <-resChannel: perSecondAggregatorChannel <- &msg writeResult // sync write result to file for later processing. default: time.Sleep } } }  During simulation startup, the acceptResults function was called as a goroutine:  resultsChannel := make // buffered channel go acceptResults  E.g. the resultsChannel was created as a buffered channel with space for 10000 messages. The acceptResults function then declares another channel and launches a goroutine that we will pass results to for per-second aggregation that will eventually be passed in aggregated form to connected Web Sockets. The writeResult to the writeResult function and perSecondAggregator channels to avoid unecessary copying of the entire struct.  The for  statement without conditionals is exactly what it looks like - an eternal loop. The select statement within is similar to a switch statement but used for working with channels. For each 100 microsecond iteration, the for loop will either accept a message if such is waiting in the case msg := <-resChannel statement or if no message exists in the channel at that point in time, the goroutine will sleep for the mentioned 100 microseconds.  Why sleep, why 100 microseconds? I suggest reading up on buffered vs unbuffered channels, but basically this offered a good balance between throughput  or horrible throughput respectively.  Given that you have cloned the Gotling repository, have the Go SDK installed and a HTTP/TCP service to test, a Gotling simulation can be started from the command line:  go run src/github.com/eriklupander/gotling/*.go samples/spring-xd-demo.yml  A binary can be built as well:  go build src/github.com/eriklupander/gotling/*.go  With a compiled binary, it should be sufficient to use :  ./gotling samples/spring-xd-demo.yml  The test will output some basic stats in the console window:  ![Console output]  Note that the rampup time, in this case 30 seconds, evenly distributes user startup.  The "work-in-progress" live dashboard based on Web Sockets and [Highcharts] can be accessed using a HTML5-capable web browser by pointing it to:  http://localhost:8182  ![Dasboard sample]  Technically, the simulation will launch a HTTP server on port 8182 that serves the [static HTML/JS] resources and also provides a Web Socket at http://localhost:8182/start  ws = new WebSocket; ws.onmessage = function { var statFrame = JSON.parse; // from json string to JS object var x = ; // current time  // Add data points to Highcharts series1.addPoint; series2.addPoint; };  In the Golang program, each time a browser connects to the WS server the connection is added to a simple slice containing WS connections. The [code] that assembles the text frame sent over the Web Socket each second looks like this:  func BroadcastStatFrame { for index, wsConn := range connectionRegistry { // Iterate over conns serializedFrame, _ := json.Marshal // struct to bytes err := wsConn.WriteMessage // Write frame if err != nil { // Detected disconnected channel. Need to clean up. fmt.Printf wsConn.Close Remove // Removes the WS connection from the registry } } }  At time of writing this, the reports aren't finished. A WIP sample:  ![REport sample]  I decided to exercise a simple HTTP / JSON / MongoDB server I have previously written in Go using identical Gatling and Gotling load tests. I'd look at throughput, CPU utilization, memory use and stability. The test runs with 8000 concurrent users performing two HTTP GET with 3 seconds between each, e.g. ~2700 requests per second. The first GET returns a JSON listing of available objects and second GET fetches one of those objects using jsonPath variable extraction of a random id performed on the first response.  Both load test tools handles the amount of requests just fine, but going much higher on either makes the HTTP server crash with "out of files" even though I had bumped maxfiles to 'unlimited' on my MacBook Pro.  Env: MacBook Pro OS X 10.11, Core i7@2.5 ghz, 16 GB RAM  - Gatling: 200-280 mb RAM, ~90-110% CPU load. - Gotling: 43 mb RAM, 90-100% CPU load.  This is a rather unscientific comparison using the Activity Monitor on OS X 10.11, especially given my naive Go implementation. CPU usage seems to be quite even while the Go program uses less than 1/4th the memory compared to the JVM-based Gatling tool.  The Gotling tool is something I have written for fun and is essentially my first proper Golang program. It meets most of the basic requirements I set up for myself and I've had lots of fun writing both the program and this blog post. The codebase really needs some more love - use of packages, tests etc. are just two of many things that should be improved upon.  I hope you enjoyed reading about this endeavour. Please do not hesitate to use the comments.
